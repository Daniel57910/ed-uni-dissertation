{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gym stable-baselines3[extra] boto3 scipy python-dotenv awscli --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_149.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_149.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_159.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_159.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_151.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_151.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_153.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_153.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_158.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_158.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_152.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_152.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_154.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_154.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_157.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_157.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_155.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_155.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_156.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_156.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_162.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_162.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_161.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_161.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_164.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_164.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_165.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_165.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_167.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_167.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_163.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_163.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_171.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_171.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_169.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_169.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_150.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_150.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_170.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_170.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_172.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_172.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_166.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_166.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_173.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_173.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_176.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_176.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_177.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_177.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_178.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_178.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_179.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_179.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_174.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_174.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_180.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_180.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_182.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_182.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_181.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_181.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_175.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_175.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_185.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_185.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_187.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_187.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_184.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_184.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_183.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_183.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_188.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_188.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_194.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_194.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_186.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_186.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_189.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_189.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_193.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_193.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_160.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_160.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_191.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_191.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_196.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_196.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_192.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_192.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_199.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_199.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_197.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_197.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_168.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_168.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_198.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_198.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_195.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_195.parquet\n",
      "upload: rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_190.parquet to s3://dissertation-data-dmiller/rl_evaluation/sensitivity_analysis/q2/dqn_pred_cnn/sim_190.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync rl_evaluation s3://dissertation-data-dmiller/rl_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant\n",
    "FEATURE_COLUMNS = [\n",
    "    \n",
    "    \"user_count\",\n",
    "    \"project_count\" ,\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event\",\n",
    "    \"cum_session_time\",\n",
    "    \"expanding_click_average\",\n",
    "   \n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_event\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"delta_last_event\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_count_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"cum_session_event_raw\",\n",
    "    \"date_time\"\n",
    "]\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'session_minutes',\n",
    "    'size_cutoff',\n",
    "    'time_cutoff',\n",
    "    'reward'\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    \"label\",\n",
    "    \"pred\"\n",
    "]\n",
    "\n",
    "LOAD_COLS = list(set(FEATURE_COLUMNS + METADATA + RL_STAT_COLS + PREDICTION_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment\n",
    "# %load environment\n",
    "# %load environment\n",
    "import gym\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "MAX_EVAL_SIZE = 75\n",
    "\n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, dataset, out_features, n_sequences, params=None):\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.unique_sessions = self.dataset[['user_id', 'session_30_count_raw']].drop_duplicates()\n",
    "        self.n_sequences = n_sequences\n",
    "        self.current_session = None\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        self.n_sequences = n_sequences\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        max_session_size = self.dataset['session_size'].max()\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=91, shape=(len(out_features) + 3, n_sequences + 1), dtype=np.float32)\n",
    "        self.episode_bins = []\n",
    "        self.exp_runs = 0\n",
    "        self.params = params\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        random_session = np.random.randint(0, self.unique_sessions.shape[0])\n",
    "        \n",
    "        user_to_run, session_to_run = self.unique_sessions.iloc[random_session][['user_id', 'session_30_count_raw']]\n",
    "        self.current_session = self._get_events(user_to_run, session_to_run)\n",
    "        self.metadata = self._metadata()\n",
    "        self.current_session_index = 1\n",
    "        self.reward = 0\n",
    "        return self._state()\n",
    "    \n",
    "    def _row_to_dict(self, metadata):\n",
    "        \"\"\"\n",
    "        Convert a row of metadata to a dictionary.\n",
    "        \"\"\"\n",
    "        return metadata.to_dict()\n",
    "    \n",
    "    def _reward_exp(self, cum_session_event_raw):\n",
    "        \"\"\"\n",
    "        Reward shaping as\n",
    "            0 if cum_session_event_raw < size_cutoff\n",
    "            (cum_session_event_raw - size_cutoff) * (cum_session_event_raw / size_cutoff) otherwise\n",
    "        \"\"\"\n",
    "        if cum_session_event_raw <= self.metadata['size_cutoff']:\n",
    "            return cum_session_event_raw / self.metadata['size_cutoff']\n",
    "        \n",
    "        return (cum_session_event_raw - self.metadata['size_cutoff']) * (cum_session_event_raw / self.metadata['size_cutoff'])\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        self._take_action(action)\n",
    "            \n",
    "        next_state, done, meta = self._calculate_next_state()\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            current_session_index = self.current_session_index if \\\n",
    "                self.current_session_index != self.current_session.shape[0] else self.current_session.shape[0] - 1\n",
    "            \n",
    "            self.exp_runs += 1\n",
    "            self.metadata['ended_event'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['ended_time'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            self.metadata['exp_runs'] = self.exp_runs\n",
    "            self.episode_bins.append(self._row_to_dict(self.metadata))\n",
    "            \n",
    "            self.metadata['ended_event'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['ended_time'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            self.metadata['exp_runs'] = self.exp_runs\n",
    "            self.episode_bins.append(self._row_to_dict(self.metadata))\n",
    "           \n",
    "            cum_session_event_raw = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            reward_exp = self._reward_exp(cum_session_event_raw)\n",
    "            \n",
    "            return next_state, reward_exp , done, {}\n",
    "        else:\n",
    "            self.reward = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "            cum_session_event_raw = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "            \n",
    "            reward_exp = self._reward_exp(cum_session_event_raw)\n",
    "    \n",
    "            self.current_session_index += 1        \n",
    "            \n",
    "            return next_state, reward_exp, done, meta\n",
    "    \n",
    "    def _metadata(self):\n",
    "        session_metadata = self.current_session.iloc[0][RL_STAT_COLS].copy()\n",
    "        session_metadata['ended'] = 0\n",
    "        for meta_col in ['small', 'medium', 'large']:\n",
    "            session_metadata[f'inc_{meta_col}'] = 0\n",
    "            session_metadata[f'time_{meta_col}'] = 0\n",
    "\n",
    "        return session_metadata\n",
    "    \n",
    "    def flush_episode_bins(self):\n",
    "        episode_bins = self.episode_bins.copy()\n",
    "        self.episode_bins = []\n",
    "        return episode_bins\n",
    "    \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        if (self.current_session_index == self.current_session.shape[0]):\n",
    "            return None, True, {}\n",
    "\n",
    "        if self._continuing_in_session():\n",
    "            return self._state(), False, {}\n",
    "    \n",
    "        return None, True, {}\n",
    "         \n",
    "    def _continuing_in_session(self):\n",
    "        event_cutoff = self.current_session.iloc[self.current_session_index]['size_cutoff']\n",
    "        current_session_event = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "        if current_session_event <= event_cutoff or current_session_event  >= MAX_EVAL_SIZE:\n",
    "            return True\n",
    "        \n",
    "        param_mid = 0.1 if not self.params else self.params['mid']\n",
    "        param_large = 0.2 if not self.params else self.params['large']\n",
    "        param_window = 0.75 if not self.params else self.params['window']\n",
    "    \n",
    "        extending_low = self._probability_extending(current_session_event, self.metadata['inc_small']) - \\\n",
    "            (0.05 + np.random.normal(-0.02, 0.1, 100).mean())\n",
    "\n",
    "            \n",
    "        extending_medium = self._probability_extending(current_session_event, self.metadata['inc_medium']) - \\\n",
    "            (param_mid + np.random.normal(-0.02, 0.1, 100).mean()) \n",
    "            \n",
    "        extending_large = self._probability_extending(current_session_event, self.metadata['inc_large']) + \\\n",
    "            (param_large + np.random.normal(-0.02, 0.1, 100).mean())\n",
    "            \n",
    "        return any([\n",
    "            extending_low > 0.4 and extending_low <= param_window,\n",
    "            extending_medium > 0.4 and extending_medium <= param_window,\n",
    "            extending_large > 0.4 and extending_large <= param_window\n",
    "        ])\n",
    "        \n",
    "           \n",
    "    \n",
    "    def _probability_extending(self, current_session_event, incentive_event):\n",
    "        if incentive_event == 0:\n",
    "            return 0\n",
    "         \n",
    "        continue_session = norm(\n",
    "            loc=max(incentive_event, 1),\n",
    "            scale=max(incentive_event *.75, 1)\n",
    "        ).cdf(max(current_session_event, 1)) \n",
    "        \n",
    "        return continue_session\n",
    "        \n",
    "\n",
    "    def _get_events(self, user_id, session):\n",
    "        subset = self.dataset[\n",
    "            (self.dataset['user_id'] == user_id) &\n",
    "            (self.dataset['session_30_count_raw'] == session).copy()\n",
    "        ]\n",
    "\n",
    "        subset = subset.sort_values(by=['date_time'])\n",
    "        return subset\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        if action == 0:\n",
    "            return 1\n",
    "        \n",
    "        current_session_index = self.current_session_index if \\\n",
    "            self.current_session_index != self.current_session.shape[0] else self.current_session.shape[0] - 1\n",
    "    \n",
    "        if action == 1:\n",
    "            if self.metadata['inc_small'] > 0:\n",
    "                return 1\n",
    "\n",
    "            self.metadata['inc_small'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_small'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "    \n",
    "        elif action == 2:\n",
    "            if self.metadata['inc_medium'] > 0:\n",
    "                return 1\n",
    "            self.metadata['inc_medium'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_medium'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            if self.metadata['inc_large'] > 0:\n",
    "                return 1\n",
    "            self.metadata['inc_large'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_large'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "\n",
    "    def _state(self):\n",
    "\n",
    "        if self.current_session_index > self.n_sequences:\n",
    "            events = self.current_session.iloc[self.current_session_index - (self.n_sequences + 1):self.current_session_index][self.out_features]\n",
    "            events['inc_small'] = self.metadata['inc_small']\n",
    "            events['inc_medium'] = self.metadata['inc_medium']\n",
    "            events['inc_large'] = self.metadata['inc_large']\n",
    "            \n",
    "            events = events.values\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            delta = min((self.n_sequences + 1)- self.current_session_index, self.n_sequences)\n",
    "            zero_cat = np.zeros((delta, len(self.out_features) + 3))\n",
    "            events = self.current_session.iloc[:max(self.current_session_index, 1)][self.out_features]\n",
    "            \n",
    "            events['inc_small'] = self.metadata['inc_small']\n",
    "            events['inc_medium'] = self.metadata['inc_medium']\n",
    "            events['inc_large'] = self.metadata['inc_large']\n",
    "            \n",
    "            \n",
    "            events = np.concatenate((zero_cat, events), axis=0)\n",
    "        \n",
    "        return events.astype(np.float32).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment_q2\n",
    "import gym\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "MAX_EVAL_SIZE = 75\n",
    "from bisect import bisect_left\n",
    "\n",
    "class CitizenScienceEnvQ2(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, dataset, out_features, n_sequences, params=None):\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: \n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnvQ2, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.unique_sessions = self.dataset[['user_id', 'session_30_count_raw']].drop_duplicates()\n",
    "        self.n_sequences = n_sequences\n",
    "        self.current_session = None\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        self.n_sequences = n_sequences\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        max_session_size = self.dataset['session_size'].max()\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=91, shape=(len(out_features) + 3, n_sequences + 1), dtype=np.float32)\n",
    "        self.episode_bins = []\n",
    "        self.exp_runs = 0\n",
    "        self.params = params\n",
    "        self.social_components = {\n",
    "            soc: norm(soc, scale=soc//5)\n",
    "            for soc in self._setup_social_components(params)\n",
    "        }\n",
    "\n",
    "    def _setup_social_components(self, params):\n",
    "        if not params['soc_freq'] or params['soc_freq'] == 7:\n",
    "            return [10, 20, 30, 45, 60, 75, 92]\n",
    "        \n",
    "        if params['soc_freq'] == 3:\n",
    "            return [10, 30, 60, 92]\n",
    "        \n",
    "        return [10, 30, 45, 60, 75, 92]\n",
    "        \n",
    "    def reset(self):\n",
    "        random_session = np.random.randint(0, self.unique_sessions.shape[0])\n",
    "        \n",
    "        user_to_run, session_to_run = self.unique_sessions.iloc[random_session][['user_id', 'session_30_count_raw']]\n",
    "        self.current_session = self._get_events(user_to_run, session_to_run)\n",
    "        self.metadata = self._metadata()\n",
    "        self.current_session_index = 1\n",
    "        self.reward = 0\n",
    "        return self._state()\n",
    "    \n",
    "    def _row_to_dict(self, metadata):\n",
    "        \"\"\"\n",
    "        Convert a row of metadata to a dictionary.\n",
    "        \"\"\"\n",
    "        return metadata.to_dict()\n",
    "    \n",
    "    def _reward_exp(self, cum_session_event_raw):\n",
    "        \"\"\"\n",
    "        Reward shaping as\n",
    "            0 if cum_session_event_raw < size_cutoff\n",
    "            (cum_session_event_raw - size_cutoff) * (cum_session_event_raw / size_cutoff) otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        if all([self.metadata['inc_small'] == 0, self.metadata['inc_medium'] == 0, self.metadata['inc_large'] == 0]):\n",
    "            distance_shaper = cum_session_event_raw / 4\n",
    "        \n",
    "        else:\n",
    "            distance_shaper = np.linalg.norm([\n",
    "                self.metadata['inc_small'], \n",
    "                self.metadata['inc_medium'] - self.metadata['inc_small'], \n",
    "                self.metadata['inc_large'] - self.metadata['inc_medium'] ])\n",
    "        if cum_session_event_raw <= self.metadata['size_cutoff']:\n",
    "            return (cum_session_event_raw / self.metadata['size_cutoff']) * distance_shaper\n",
    "        \n",
    "        return (cum_session_event_raw - self.metadata['size_cutoff']) * (cum_session_event_raw / self.metadata['size_cutoff']) * distance_shaper\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        self._take_action(action)\n",
    "        self._assign_social_components()\n",
    "        next_state, done, meta = self._calculate_next_state()\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            current_session_index = self.current_session_index if \\\n",
    "                self.current_session_index != self.current_session.shape[0] else self.current_session.shape[0] - 1\n",
    "            \n",
    "            self.exp_runs += 1\n",
    "            self.metadata['ended_event'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['ended_time'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            self.metadata['exp_runs'] = self.exp_runs\n",
    "            self.episode_bins.append(self._row_to_dict(self.metadata))\n",
    "           \n",
    "            cum_session_event_raw = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            reward_exp = self._reward_exp(cum_session_event_raw)\n",
    "            \n",
    "            return next_state, reward_exp , done, {}\n",
    "        else:\n",
    "            self.reward = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "            cum_session_event_raw = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "            \n",
    "            reward_exp = self._reward_exp(cum_session_event_raw)\n",
    "    \n",
    "            self.current_session_index += 1        \n",
    "            \n",
    "            return next_state, reward_exp, done, meta\n",
    "    \n",
    "    def _assign_social_components(self):\n",
    "        \n",
    "        current_event = self.current_session_index\n",
    "        social_components_keys = list(self.social_components.keys())\n",
    "        social_likelihood_index = social_components_keys[bisect_left(social_components_keys, current_event)]\n",
    "        \n",
    "        if self.metadata[f'soc_{social_likelihood_index}'] > 0:\n",
    "            return\n",
    "        \n",
    "        social_asssignmnet_fn = self.social_components[social_likelihood_index]\n",
    "        assign_social = social_asssignmnet_fn.cdf(current_event)\n",
    "         \n",
    "        if all([assign_social >= .4, assign_social <= 7]):\n",
    "            self.metadata[f'soc_{social_likelihood_index}'] = current_event\n",
    "\n",
    " \n",
    "    def _metadata(self):\n",
    "        session_metadata = self.current_session.iloc[0][RL_STAT_COLS].copy()\n",
    "        session_metadata['ended'] = 0\n",
    "        for meta_col in ['small', 'medium', 'large']:\n",
    "            session_metadata[f'inc_{meta_col}'] = 0\n",
    "            session_metadata[f'time_{meta_col}'] = 0\n",
    "        \n",
    "        for soc_col in self.social_components:\n",
    "            session_metadata[f'soc_{soc_col}'] = 0\n",
    "\n",
    "        return session_metadata\n",
    "    \n",
    "    def flush_episode_bins(self):\n",
    "        episode_bins = self.episode_bins.copy()\n",
    "        self.episode_bins = []\n",
    "        return episode_bins\n",
    "    \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        if (self.current_session_index == self.current_session.shape[0]):\n",
    "            return None, True, {}\n",
    "\n",
    "        if self._continuing_in_session():\n",
    "            return self._state(), False, {}\n",
    "    \n",
    "        return None, True, {}\n",
    "         \n",
    "    def _continuing_in_session(self):\n",
    "        event_cutoff = self.current_session.iloc[self.current_session_index]['size_cutoff']\n",
    "        current_session_event = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "        if current_session_event <= event_cutoff or current_session_event  >= MAX_EVAL_SIZE:\n",
    "            return True\n",
    "        \n",
    "        param_mid = 0.1 if not self.params else self.params['mid']\n",
    "        param_large = 0.2 if not self.params else self.params['large']\n",
    "        param_window = 0.75 if not self.params else self.params['window']\n",
    "    \n",
    "        extending_low = self._probability_extending(current_session_event, self.metadata['inc_small']) - \\\n",
    "            (0.05 + np.random.normal(-0.02, 0.1, 100).mean())\n",
    "\n",
    "            \n",
    "        extending_medium = self._probability_extending(current_session_event, self.metadata['inc_medium']) - \\\n",
    "            (param_mid + np.random.normal(-0.02, 0.1, 100).mean()) \n",
    "            \n",
    "        extending_large = self._probability_extending(current_session_event, self.metadata['inc_large']) - \\\n",
    "            (param_large + np.random.normal(-0.02, 0.1, 100).mean())\n",
    "        \n",
    "        max_social_component = max([self.metadata[f'soc_{i}'] for i in self.social_components])\n",
    "        \n",
    "        extending_social = self._probability_extending(current_session_event, max_social_component) - \\\n",
    "            (param_large + np.random.normal(-0.02, 0.1, 100).mean())\n",
    "            \n",
    "        return any([\n",
    "            extending_low > 0.4 and extending_low <= param_window,\n",
    "            extending_medium > 0.4 and extending_medium <= param_window,\n",
    "            extending_large > 0.4 and extending_large <= param_window,\n",
    "            extending_social > 0.4 and extending_social <= param_window\n",
    "        ])\n",
    "        \n",
    "           \n",
    "    \n",
    "    def _probability_extending(self, current_session_event, incentive_event):\n",
    "        if incentive_event == 0:\n",
    "            return 0\n",
    "         \n",
    "        continue_session = norm(\n",
    "            loc=max(incentive_event, 1),\n",
    "            scale=max(incentive_event *.75, 1)\n",
    "        ).cdf(max(current_session_event, 1)) \n",
    "        \n",
    "        return continue_session\n",
    "        \n",
    "\n",
    "    def _get_events(self, user_id, session):\n",
    "        subset = self.dataset[\n",
    "            (self.dataset['user_id'] == user_id) &\n",
    "            (self.dataset['session_30_count_raw'] == session).copy()\n",
    "        ]\n",
    "\n",
    "        subset = subset.sort_values(by=['date_time'])\n",
    "        return subset\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        \n",
    "        if action == 0:\n",
    "            return 1\n",
    "        \n",
    "        current_session_index = self.current_session_index if \\\n",
    "            self.current_session_index != self.current_session.shape[0] else self.current_session.shape[0] - 1\n",
    "    \n",
    "        if action == 1:\n",
    "            if self.metadata['inc_small'] > 0:\n",
    "                return 1\n",
    "\n",
    "            self.metadata['inc_small'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_small'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "    \n",
    "        elif action == 2:\n",
    "            if self.metadata['inc_medium'] > 0:\n",
    "                return 1\n",
    "            self.metadata['inc_medium'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_medium'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            if self.metadata['inc_large'] > 0:\n",
    "                return 1\n",
    "            self.metadata['inc_large'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_large'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "\n",
    "    def _state(self):\n",
    "\n",
    "        if self.current_session_index > self.n_sequences:\n",
    "            events = self.current_session.iloc[self.current_session_index - (self.n_sequences + 1):self.current_session_index][self.out_features]\n",
    "            events['inc_small'] = self.metadata['inc_small']\n",
    "            events['inc_medium'] = self.metadata['inc_medium']\n",
    "            events['inc_large'] = self.metadata['inc_large']\n",
    "            \n",
    "            events = events.values\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            delta = min((self.n_sequences + 1)- self.current_session_index, self.n_sequences)\n",
    "            zero_cat = np.zeros((delta, len(self.out_features) + 3))\n",
    "            events = self.current_session.iloc[:max(self.current_session_index, 1)][self.out_features]\n",
    "            \n",
    "            events['inc_small'] = self.metadata['inc_small']\n",
    "            events['inc_medium'] = self.metadata['inc_medium']\n",
    "            events['inc_large'] = self.metadata['inc_large']\n",
    "           \n",
    "            \n",
    "            \n",
    "            events = np.concatenate((zero_cat, events), axis=0)\n",
    "        \n",
    "        return events.astype(np.float32).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2023 06:54:58 PM Found credentials in environment variables.\n",
      "06/29/2023 06:54:59 PM generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "# %load rl_sensitivity_analysis.py\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "import boto3\n",
    "import torch\n",
    "import random\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "global logger, client\n",
    "logger = logging.getLogger(__name__)\n",
    "client = boto3.client('s3')\n",
    "import argparse\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN, A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "\n",
    "from itertools import product\n",
    "from pprint import pprint, pformat\n",
    "\n",
    "MIN_MAX_RANGE = (10, 90)\n",
    "from tqdm import tqdm\n",
    "\n",
    "N_SEQUENCES = 15\n",
    "\n",
    "S3_BASELINE_PATH = 'dissertation-data-dmiller'\n",
    "\n",
    "SENSITIVITY_PARAMS = {\n",
    "    \"window\": (.8,  .6),\n",
    "    \"mid\": {.15, .04},\n",
    "    \"large\": {.3, .09},\n",
    "}\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    'dqn_pred_cnn': DQN,\n",
    "    'a2c_pred_cnn': A2C,\n",
    "}\n",
    "\n",
    "def parse_args():\n",
    "    parse = argparse.ArgumentParser()\n",
    "    parse.add_argument('--algo', type=str, default='dqn_pred_cnn')\n",
    "    parse.add_argument('--run_date', type=str, default='2023-06-22_14-22-36')\n",
    "    parse.add_argument('--write_path', type=str, default='rl_evaluation')\n",
    "    parse.add_argument('--n_files', type=int, default=30)\n",
    "    parse.add_argument('--q2', default=1, type=int)\n",
    "    args = parse.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def find_s3_candidate(algo, run_date):\n",
    "    \n",
    "    folder_prefix = os.path.join(\n",
    "        'experiments',\n",
    "        \"q2\",\n",
    "        algo,\n",
    "        run_date,\n",
    "        'checkpoints'\n",
    "    )\n",
    "\n",
    "    \n",
    "    logger.info(f'Looking for files in {folder_prefix}')\n",
    "    \n",
    "    files = [\n",
    "        {\n",
    "            'key': file['Key'],\n",
    "            'last_modified': file['LastModified'],\n",
    "            'check_index': int(re.sub('[^0-9]', '', file['Key'].split('/')[-1]))\n",
    "        }\n",
    "        for file in client.list_objects_v2(Bucket=S3_BASELINE_PATH, Prefix=folder_prefix)['Contents']\n",
    "    ]\n",
    "    \n",
    "    s3_candidate = sorted(files, key=lambda x: x['check_index'])[-1]['key']\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    logger.info(f'Found candiate: {s3_candidate}')\n",
    "    \n",
    "    return s3_candidate\n",
    "\n",
    "def get_policy(algo, run_date):\n",
    "        \n",
    "    s3_candidate = find_s3_candidate(algo, run_date)\n",
    "    if not os.path.exists(os.path.dirname(s3_candidate)):\n",
    "        logger.info(f'Creating policy path {os.path.dirname(s3_candidate)}')\n",
    "        \n",
    "        os.makedirs(os.path.dirname(s3_candidate), exist_ok=True)\n",
    "       \n",
    "    # return s3_candidate \n",
    "    client.download_file(S3_BASELINE_PATH, s3_candidate, s3_candidate)\n",
    "    return s3_candidate\n",
    "        \n",
    "def simplify_experiment(vectorized_df):\n",
    "    vectorized_df = [\n",
    "        df[(df['session_size'] >= MIN_MAX_RANGE[0]) & (df['session_size'] <= MIN_MAX_RANGE[1])] for df in vectorized_df\n",
    "    ]\n",
    "    \n",
    "    return vectorized_df\n",
    "    \n",
    "\n",
    "\n",
    "def _label_or_pred(algo):\n",
    "    if 'label' in algo:\n",
    "        return 'label'\n",
    "    elif 'pred' in algo:\n",
    "        return 'pred'\n",
    "    else:\n",
    "        return None\n",
    "   \n",
    "\n",
    "def run_sensitivity_analysis(env_datasets, policy_path, feature_cols, param_combos, q2, algo, write_base): \n",
    "    \n",
    "    p_bar = tqdm(param_combos, unit='item')\n",
    "    out_df_container = []\n",
    "    for i, combo in enumerate(p_bar):\n",
    "        if i < 149:\n",
    "            print(f'Skipping combo {i}')\n",
    "            continue\n",
    "        params = {\n",
    "            \"window\": combo[0].round(2),\n",
    "            \"mid\": combo[1].round(2),\n",
    "            \"large\": combo[2].round(2),\n",
    "            \"soc_freq\": int(combo[3])\n",
    "        }\n",
    "        p_bar.set_description(f'Running combo {params}')\n",
    "        \n",
    "        vec_monitor = VecMonitor(DummyVecEnv([lambda: CitizenScienceEnvQ2(dataset, feature_cols, N_SEQUENCES, params) for dataset in env_datasets]))\n",
    "        \n",
    "        model = MODEL_PARAMS[algo].load(\n",
    "            policy_path,\n",
    "            env=vec_monitor,\n",
    "            verbose=0,\n",
    "        )\n",
    "        \n",
    "        evaluate_policy(\n",
    "            model,\n",
    "            model.get_env(),\n",
    "            deterministic=False,\n",
    "            n_eval_episodes=2000\n",
    "        )\n",
    "        \n",
    "        dists = model.get_env().get_attr('episode_bins')\n",
    "        values_to_log = [item for sublist in dists for item in sublist if len(sublist) > 0]\n",
    "        out_df = pd.DataFrame(values_to_log)\n",
    "        out_df['window'] = params['window']\n",
    "        out_df['mid'] = params['mid']\n",
    "        out_df['large'] = params['large']\n",
    "        out_df['soc_freq'] = params['soc_freq']\n",
    "        out_df.to_parquet(os.path.join(write_base, f'sim_{i}.parquet'))\n",
    "        \n",
    "        \n",
    "def rebatch_data(vectorized_df):\n",
    "    df_sublist = []\n",
    "    for i in range(0, len(vectorized_df), 10):\n",
    "        df_sublist.append(pd.concat(vectorized_df[i:i+10], ignore_index=True))\n",
    "    return df_sublist  \n",
    "\n",
    "\n",
    "def main(args):\n",
    "    algo, run_date, write_path, n_files, q2 = args.algo, args.run_date, args.write_path, args.n_files, args.q2\n",
    "\n",
    "    params_window = np.arange(*SENSITIVITY_PARAMS['window'], -.02).tolist()\n",
    "    params_mid = np.arange(*SENSITIVITY_PARAMS['mid'], -.01).tolist()\n",
    "    params_large = np.arange(*SENSITIVITY_PARAMS['large'], -.02).tolist()\n",
    "    social_params = [3, 5, 7]\n",
    "    \n",
    "    logger.info(f'Window params: {params_window}')\n",
    "    logger.info(f'Mid params: {params_mid}')\n",
    "    logger.info(f'Large params: {params_large}')\n",
    "    logger.info(f'Social params: {social_params}')\n",
    "    \n",
    "    param_combos = np.array(list(product(params_window, params_mid, params_large, social_params)))\n",
    "    logger.info(f'Combination parameters obtained: {param_combos.shape}, running monte carlo simulation on 200 random samples')\n",
    "    param_combos = param_combos[np.random.choice(param_combos.shape[0], 200, replace=False), :]\n",
    "   \n",
    "    policy_path = get_policy(algo, run_date)\n",
    "    logger.info(f'Policy path downloaded, evaluating experiment: {policy_path}')\n",
    "    \n",
    "    read_path = os.path.join('rl_ready_data_conv', f'files_used_{n_files}', 'window_1', 'batched_eval')\n",
    "    files_to_read = glob.glob(os.path.join(read_path, '*.parquet'))\n",
    "    logger.info(f'Found {len(files_to_read)} files to read')\n",
    "    env_datasets = [\n",
    "        pd.read_parquet(file) for file in files_to_read\n",
    "    ]\n",
    "\n",
    "    env_datasets = simplify_experiment(env_datasets)\n",
    "    \n",
    "    if 'a2c' in algo:\n",
    "        logger.info(f'Rebatching data for A2C')\n",
    "        env_datasets = rebatch_data(env_datasets)\n",
    "        \n",
    "    feature_cols = FEATURE_COLUMNS + [_label_or_pred(algo)] if _label_or_pred(algo) else FEATURE_COLUMNS\n",
    "    logger.info(f'Length of features: {len(feature_cols)}')\n",
    "    logger.info(f'Running sensitivity analysis per monte carlo simulation')\n",
    "    logger.info(pformat(\n",
    "        {\n",
    "            'algo': algo,\n",
    "            'q2': q2==1,\n",
    "            'n_envs': len(env_datasets),\n",
    "            'n_features': len(feature_cols),\n",
    "            \n",
    "        }\n",
    "    ))\n",
    "\n",
    "    \n",
    "    write_base = os.path.join(write_path, f'sensitivity_analysis', f'{\"q2\" if q2==1 else \"q1\"}', algo)\n",
    "    if not os.path.exists(write_base):\n",
    "        logger.info(f'Creating write path {write_base}')\n",
    "        os.makedirs(write_base, exist_ok=True)\n",
    "   \n",
    "    run_sensitivity_analysis(env_datasets, policy_path, feature_cols, param_combos, q2==1, algo, write_base) \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    algo = 'dqn_pred_cnn'\n",
    "    run_date = '2023-06-22_14-22-36'\n",
    "    write_path = 'rl_evaluation'\n",
    "    n_files = 30\n",
    "    q2 = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/29/2023 06:55:00 PM Window params: [0.8, 0.78, 0.76, 0.74, 0.72, 0.7, 0.6799999999999999, 0.6599999999999999, 0.6399999999999999, 0.6199999999999999, 0.5999999999999999]\n",
      "06/29/2023 06:55:00 PM Mid params: [0.15, 0.13999999999999999, 0.12999999999999998, 0.11999999999999997, 0.10999999999999996, 0.09999999999999995, 0.08999999999999994, 0.07999999999999993, 0.06999999999999992, 0.059999999999999915, 0.049999999999999906]\n",
      "06/29/2023 06:55:00 PM Large params: [0.3, 0.27999999999999997, 0.25999999999999995, 0.23999999999999994, 0.21999999999999992, 0.1999999999999999, 0.17999999999999988, 0.15999999999999986, 0.13999999999999985, 0.11999999999999983, 0.09999999999999981]\n",
      "06/29/2023 06:55:00 PM Social params: [3, 5, 7]\n",
      "06/29/2023 06:55:00 PM Combination parameters obtained: (3993, 4), running monte carlo simulation on 200 random samples\n",
      "06/29/2023 06:55:00 PM Looking for files in experiments/q2/dqn_pred_cnn/2023-06-22_14-22-36/checkpoints\n",
      "06/29/2023 06:55:00 PM Found candiate: experiments/q2/dqn_pred_cnn/2023-06-22_14-22-36/checkpoints/rl_model_6600000_steps.zip\n",
      "06/29/2023 06:55:01 PM Policy path downloaded, evaluating experiment: experiments/q2/dqn_pred_cnn/2023-06-22_14-22-36/checkpoints/rl_model_6600000_steps.zip\n",
      "06/29/2023 06:55:01 PM Found 100 files to read\n",
      "06/29/2023 06:55:07 PM Length of features: 23\n",
      "06/29/2023 06:55:07 PM Running sensitivity analysis per monte carlo simulation\n",
      "06/29/2023 06:55:07 PM {'algo': 'dqn_pred_cnn', 'n_envs': 100, 'n_features': 23, 'q2': True}\n",
      "Running combo {'window': 0.66, 'mid': 0.05, 'large': 0.2, 'soc_freq': 5}:   0%|          | 0/200 [00:00<?, ?item/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping combo 0\n",
      "Skipping combo 1\n",
      "Skipping combo 2\n",
      "Skipping combo 3\n",
      "Skipping combo 4\n",
      "Skipping combo 5\n",
      "Skipping combo 6\n",
      "Skipping combo 7\n",
      "Skipping combo 8\n",
      "Skipping combo 9\n",
      "Skipping combo 10\n",
      "Skipping combo 11\n",
      "Skipping combo 12\n",
      "Skipping combo 13\n",
      "Skipping combo 14\n",
      "Skipping combo 15\n",
      "Skipping combo 16\n",
      "Skipping combo 17\n",
      "Skipping combo 18\n",
      "Skipping combo 19\n",
      "Skipping combo 20\n",
      "Skipping combo 21\n",
      "Skipping combo 22\n",
      "Skipping combo 23\n",
      "Skipping combo 24\n",
      "Skipping combo 25\n",
      "Skipping combo 26\n",
      "Skipping combo 27\n",
      "Skipping combo 28\n",
      "Skipping combo 29\n",
      "Skipping combo 30\n",
      "Skipping combo 31\n",
      "Skipping combo 32\n",
      "Skipping combo 33\n",
      "Skipping combo 34\n",
      "Skipping combo 35\n",
      "Skipping combo 36\n",
      "Skipping combo 37\n",
      "Skipping combo 38\n",
      "Skipping combo 39\n",
      "Skipping combo 40\n",
      "Skipping combo 41\n",
      "Skipping combo 42\n",
      "Skipping combo 43\n",
      "Skipping combo 44\n",
      "Skipping combo 45\n",
      "Skipping combo 46\n",
      "Skipping combo 47\n",
      "Skipping combo 48\n",
      "Skipping combo 49\n",
      "Skipping combo 50\n",
      "Skipping combo 51\n",
      "Skipping combo 52\n",
      "Skipping combo 53\n",
      "Skipping combo 54\n",
      "Skipping combo 55\n",
      "Skipping combo 56\n",
      "Skipping combo 57\n",
      "Skipping combo 58\n",
      "Skipping combo 59\n",
      "Skipping combo 60\n",
      "Skipping combo 61\n",
      "Skipping combo 62\n",
      "Skipping combo 63\n",
      "Skipping combo 64\n",
      "Skipping combo 65\n",
      "Skipping combo 66\n",
      "Skipping combo 67\n",
      "Skipping combo 68\n",
      "Skipping combo 69\n",
      "Skipping combo 70\n",
      "Skipping combo 71\n",
      "Skipping combo 72\n",
      "Skipping combo 73\n",
      "Skipping combo 74\n",
      "Skipping combo 75\n",
      "Skipping combo 76\n",
      "Skipping combo 77\n",
      "Skipping combo 78\n",
      "Skipping combo 79\n",
      "Skipping combo 80\n",
      "Skipping combo 81\n",
      "Skipping combo 82\n",
      "Skipping combo 83\n",
      "Skipping combo 84\n",
      "Skipping combo 85\n",
      "Skipping combo 86\n",
      "Skipping combo 87\n",
      "Skipping combo 88\n",
      "Skipping combo 89\n",
      "Skipping combo 90\n",
      "Skipping combo 91\n",
      "Skipping combo 92\n",
      "Skipping combo 93\n",
      "Skipping combo 94\n",
      "Skipping combo 95\n",
      "Skipping combo 96\n",
      "Skipping combo 97\n",
      "Skipping combo 98\n",
      "Skipping combo 99\n",
      "Skipping combo 100\n",
      "Skipping combo 101\n",
      "Skipping combo 102\n",
      "Skipping combo 103\n",
      "Skipping combo 104\n",
      "Skipping combo 105\n",
      "Skipping combo 106\n",
      "Skipping combo 107\n",
      "Skipping combo 108\n",
      "Skipping combo 109\n",
      "Skipping combo 110\n",
      "Skipping combo 111\n",
      "Skipping combo 112\n",
      "Skipping combo 113\n",
      "Skipping combo 114\n",
      "Skipping combo 115\n",
      "Skipping combo 116\n",
      "Skipping combo 117\n",
      "Skipping combo 118\n",
      "Skipping combo 119\n",
      "Skipping combo 120\n",
      "Skipping combo 121\n",
      "Skipping combo 122\n",
      "Skipping combo 123\n",
      "Skipping combo 124\n",
      "Skipping combo 125\n",
      "Skipping combo 126\n",
      "Skipping combo 127\n",
      "Skipping combo 128\n",
      "Skipping combo 129\n",
      "Skipping combo 130\n",
      "Skipping combo 131\n",
      "Skipping combo 132\n",
      "Skipping combo 133\n",
      "Skipping combo 134\n",
      "Skipping combo 135\n",
      "Skipping combo 136\n",
      "Skipping combo 137\n",
      "Skipping combo 138\n",
      "Skipping combo 139\n",
      "Skipping combo 140\n",
      "Skipping combo 141\n",
      "Skipping combo 142\n",
      "Skipping combo 143\n",
      "Skipping combo 144\n",
      "Skipping combo 145\n",
      "Skipping combo 146\n",
      "Skipping combo 147\n",
      "Skipping combo 148\n"
     ]
    }
   ],
   "source": [
    "main(Args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
