{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113 --quiet\n",
    "!python -m pip install boto3 python-dotenv GPUtil --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Server Not found"
     ]
    }
   ],
   "source": [
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"global_session_time\",\n",
    "    \n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"user_count\",\n",
    "    \"project_count\",\n",
    "    \"country_count\", \n",
    "\n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "LOAD_COLS = LABEL + METADATA + OUT_FEATURE_COLUMNS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf as gpu_pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as cpu_np\n",
    "import cupy as gpu_np\n",
    "import pandas as cpu_pd\n",
    "from pprint import pformat\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import os\n",
    "import numpy \n",
    "import logging\n",
    "import GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionizeData:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, df, max_sequence_index, write_path, load_cols, feature_cols, grouper, save_s3=True):\n",
    "        self.df = df\n",
    "        self.max_sequence_index = max_sequence_index + 1\n",
    "        self.min_sequence_index = self.max_sequence_index - 10\n",
    "        self.device = self._device()\n",
    "        self.sequences = cpu_np.arange(self.min_sequence_index, self.max_sequence_index).tolist()\n",
    "        self.seq_container = []\n",
    "        self.torch_sequences = None\n",
    "        self.output_path = write_path\n",
    "        self.save_s3 = save_s3\n",
    "        self.load_columns = load_cols\n",
    "        self.feature_cols = feature_cols\n",
    "        self.grouper = grouper\n",
    "        \n",
    "\n",
    "    def _device(self):\n",
    "        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def _sequence_lazy(self):\n",
    "         return next(self._lazy_load_shifted_index())\n",
    "\n",
    "    def _shifters(self):\n",
    "        for _ in range(self.min_sequence_index, self.max_sequence_index):\n",
    "            self.logger.info(f'Loading sequence: {_} -> {self.max_sequence_index}')\n",
    "            self.seq_container.append(self._sequence_lazy())\n",
    "        if torch.cuda.is_available():\n",
    "            GPUtil.showUtilization()\n",
    "\n",
    "        sequences = torch.cat(self.seq_container, dim=1).half()\n",
    "        return sequences\n",
    "\n",
    "    def generate_sequence(self):\n",
    "\n",
    "        self.logger.info(f'Generating shifted clickstreams from {self.min_sequence_index} -> {self.max_sequence_index}')\n",
    "        sequence = self._shifters()\n",
    "\n",
    "        self.logger.info(f'Shifter shape: {sequence.shape}')\n",
    "        \n",
    "\n",
    "        self.logger.info(f'Loading intial clickstream to {self.device}')\n",
    "\n",
    "        if self.max_sequence_index == 11:\n",
    "            self.logger.info('Initial clickstream writing to disk')\n",
    "            initial_clickstream = self.df[self.load_columns]\n",
    "            initial_clickstream = self.df[self.load_columns].values.astype(gpu_np.float32)\n",
    "            \n",
    "            self.logger.info(f'Initial clickstream shape: {initial_clickstream.shape}')\n",
    "            self._sequence_to_disk(initial_clickstream, 0)\n",
    "\n",
    "        self.logger.info(f'Writing sequence to disk: {self.max_sequence_index - 1}')\n",
    "        self._sequence_to_disk(sequence.cpu().numpy(), self.max_sequence_index - 1)\n",
    "\n",
    "\n",
    "    def _sequence_to_disk(self, partition, sequence_index):\n",
    "        if self.save_s3:\n",
    "            s3_client = boto3.client(\n",
    "                's3',\n",
    "                aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "                aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "            )\n",
    "\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "\n",
    "        partition_path = os.path.join(self.output_path, f'sequence_index_{sequence_index}.npz')\n",
    "        self.logger.info(f'Saving to disk: {partition_path}')\n",
    "        numpy.savez_compressed(partition_path, partition)\n",
    "\n",
    "        if self.save_s3:\n",
    "            self.logger.info(f'Uploading to s3: dissertation-data-dmiller/{partition_path}')\n",
    "            s3_client.upload_file(partition_path, 'dissertation-data-dmiller', partition_path)\n",
    "\n",
    "    def _lazy_load_shifted_index(self):\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        indx = self.sequences.pop(0)\n",
    "        torch_container = []\n",
    "        for col in self.feature_cols:\n",
    "            sequence = self.df.groupby(self.grouper)[col].shift(indx).fillna(0).values.astype(gpu_np.float16)\n",
    "            sequence_tensor = torch.tensor(sequence).to(self.device).half()\n",
    "            torch_container.append(sequence_tensor.unsqueeze(1))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        yield torch.cat(torch_container, dim=1).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "torch.set_printoptions(sci_mode=False, precision=4, linewidth=400)\n",
    "\n",
    "cpu_np.set_printoptions(suppress=True)\n",
    "cpu_np.set_printoptions(precision=4)\n",
    "\n",
    "def get_logger():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def scale_feature_cols(df, scaler, scaler_columns):\n",
    "    df[scaler_columns] = scaler.fit_transform(df[scaler_columns])\n",
    "    return df\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    logger = get_logger()\n",
    "    logger.info('Starting sessionize_users_cpu.py with arguments')\n",
    "    logger.info(pformat(args.__dict__))\n",
    "    \n",
    "    data_read = os.path.join(args.input_path, f'files_used_{args.data_subset}')\n",
    "\n",
    "    logger.info(f'Reading data from {data_read}')\n",
    "    scaler_cols = [\n",
    "        col for col in OUT_FEATURE_COLUMNS if 'sin' not in col and 'cos' not in col\n",
    "    ]\n",
    "\n",
    "    df = gpu_pd.read_parquet(data_read, columns=LOAD_COLS + ['date_time'])\n",
    "    logger.info(f'Data read: {df.shape}')\n",
    "    logger.info('Casting date time and sorting by date time')\n",
    "    df['date_time'] = gpu_pd.to_datetime(df['date_time'])\n",
    "    df = df.sort_values(by=['date_time'])\n",
    "    logger.info('Data read: scaling scaler columns')\n",
    "    train_partition = int(df.shape[0] * 0.7)\n",
    "    \n",
    "    train_df = df.iloc[:train_partition]\n",
    "    test_df = df.iloc[train_partition:]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    train_df, val_df = train_df.to_pandas(), test_df.to_pandas()\n",
    "    logger.info(f'Fitting on train dataset')\n",
    "    train_df[scaler_cols] = scaler.fit_transform(train_df[scaler_cols])\n",
    "    \n",
    "    logger.info(f'Applying scalar to validation dataset')\n",
    "    val_df[scaler_cols] = scaler.transform(val_df[scaler_cols])\n",
    "    train_df, val_df = gpu_pd.from_pandas(train_df), gpu_pd.from_pandas(val_df)\n",
    "    df = gpu_pd.concat([train_df, val_df]).sort_values(by=['date_time'])\n",
    "    logger.info(f'DF concatenated')\n",
    "    logger.info(f'Shape after features: {df.shape}')\n",
    "\n",
    "    logger.info('Scaling complete: implement sessionize')\n",
    "    \n",
    "    for seq_index in args.seq_list:\n",
    "\n",
    "        sessionize = SessionizeData(\n",
    "            df,\n",
    "            seq_index,\n",
    "            os.path.join(args.output_path, f'files_used_{args.data_subset}'),\n",
    "            LOAD_COLS,\n",
    "            OUT_FEATURE_COLUMNS,\n",
    "            GROUPBY_COLS,\n",
    "            args.save_s3\n",
    "        )\n",
    "    \n",
    "        logger.info(f'Generating sequence for {seq_index}')\n",
    "        sessionize.generate_sequence()\n",
    "    \n",
    "    logger.info(f'Sessionize complete for sequences {args.seq_list}')\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 11:40:16,546 - __main__ - INFO - Starting sessionize_users_cpu.py with arguments\n",
      "2023-06-12 11:40:16,548 - __main__ - INFO - {'data_subset': 30}\n",
      "2023-06-12 11:40:16,549 - __main__ - INFO - Reading data from calculated_features/files_used_30\n",
      "2023-06-12 11:40:18,860 - __main__ - INFO - Data read: (38500990, 36)\n",
      "2023-06-12 11:40:18,861 - __main__ - INFO - Casting date time and sorting by date time\n",
      "2023-06-12 11:40:18,924 - __main__ - INFO - Data read: scaling scaler columns\n",
      "2023-06-12 11:40:30,745 - __main__ - INFO - Fitting on train dataset\n",
      "2023-06-12 11:40:47,147 - __main__ - INFO - Applying scalar to validation dataset\n",
      "2023-06-12 11:40:56,457 - __main__ - INFO - DF concatenated\n",
      "2023-06-12 11:40:56,458 - __main__ - INFO - Shape after features: (38500990, 36)\n",
      "2023-06-12 11:40:56,458 - __main__ - INFO - Scaling complete: implement sessionize\n",
      "2023-06-12 11:40:56,460 - __main__ - INFO - Generating sequence for 10\n",
      "2023-06-12 11:40:56,460 - __main__ - INFO - Generating shifted clickstreams from 1 -> 11\n",
      "2023-06-12 11:40:56,461 - __main__ - INFO - Loading sequence: 1 -> 11\n",
      "2023-06-12 11:41:03,808 - __main__ - INFO - Loading sequence: 2 -> 11\n",
      "2023-06-12 11:41:07,181 - __main__ - INFO - Loading sequence: 3 -> 11\n",
      "2023-06-12 11:41:10,554 - __main__ - INFO - Loading sequence: 4 -> 11\n",
      "2023-06-12 11:41:13,919 - __main__ - INFO - Loading sequence: 5 -> 11\n",
      "2023-06-12 11:41:17,286 - __main__ - INFO - Loading sequence: 6 -> 11\n",
      "2023-06-12 11:41:20,664 - __main__ - INFO - Loading sequence: 7 -> 11\n",
      "2023-06-12 11:41:24,121 - __main__ - INFO - Loading sequence: 8 -> 11\n",
      "2023-06-12 11:41:27,491 - __main__ - INFO - Loading sequence: 9 -> 11\n",
      "2023-06-12 11:41:30,930 - __main__ - INFO - Loading sequence: 10 -> 11\n",
      "2023-06-12 11:41:34,566 - __main__ - INFO - Shifter shape: torch.Size([38500990, 220])\n",
      "2023-06-12 11:41:34,567 - __main__ - INFO - Loading intial clickstream to cuda\n",
      "2023-06-12 11:41:34,568 - __main__ - INFO - Initial clickstream writing to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 55% | 57% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 11:41:34,992 - __main__ - INFO - Initial clickstream shape: (38500990, 35)\n",
      "2023-06-12 11:41:34,993 - __main__ - INFO - Saving to disk: torch_ready_data/files_used_30/sequence_index_0.npz\n",
      "2023-06-12 11:43:45,143 - __main__ - INFO - Writing sequence to disk: 10\n",
      "2023-06-12 11:43:56,464 - __main__ - INFO - Saving to disk: torch_ready_data/files_used_30/sequence_index_10.npz\n",
      "2023-06-12 11:47:56,465 - __main__ - INFO - Generating sequence for 20\n",
      "2023-06-12 11:47:56,466 - __main__ - INFO - Generating shifted clickstreams from 11 -> 21\n",
      "2023-06-12 11:47:56,466 - __main__ - INFO - Loading sequence: 11 -> 21\n",
      "2023-06-12 11:47:59,978 - __main__ - INFO - Loading sequence: 12 -> 21\n",
      "2023-06-12 11:48:03,448 - __main__ - INFO - Loading sequence: 13 -> 21\n",
      "2023-06-12 11:48:06,979 - __main__ - INFO - Loading sequence: 14 -> 21\n",
      "2023-06-12 11:48:10,428 - __main__ - INFO - Loading sequence: 15 -> 21\n",
      "2023-06-12 11:48:13,827 - __main__ - INFO - Loading sequence: 16 -> 21\n",
      "2023-06-12 11:48:17,209 - __main__ - INFO - Loading sequence: 17 -> 21\n",
      "2023-06-12 11:48:20,577 - __main__ - INFO - Loading sequence: 18 -> 21\n",
      "2023-06-12 11:48:23,937 - __main__ - INFO - Loading sequence: 19 -> 21\n",
      "2023-06-12 11:48:27,311 - __main__ - INFO - Loading sequence: 20 -> 21\n",
      "2023-06-12 11:48:30,858 - __main__ - INFO - Shifter shape: torch.Size([38500990, 220])\n",
      "2023-06-12 11:48:30,859 - __main__ - INFO - Loading intial clickstream to cuda\n",
      "2023-06-12 11:48:30,860 - __main__ - INFO - Writing sequence to disk: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 99% | 57% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 11:48:42,094 - __main__ - INFO - Saving to disk: torch_ready_data/files_used_30/sequence_index_20.npz\n",
      "2023-06-12 11:52:36,705 - __main__ - INFO - Generating sequence for 30\n",
      "2023-06-12 11:52:36,706 - __main__ - INFO - Generating shifted clickstreams from 21 -> 31\n",
      "2023-06-12 11:52:36,706 - __main__ - INFO - Loading sequence: 21 -> 31\n",
      "2023-06-12 11:52:40,903 - __main__ - INFO - Loading sequence: 22 -> 31\n",
      "2023-06-12 11:52:44,262 - __main__ - INFO - Loading sequence: 23 -> 31\n",
      "2023-06-12 11:52:47,628 - __main__ - INFO - Loading sequence: 24 -> 31\n",
      "2023-06-12 11:52:50,991 - __main__ - INFO - Loading sequence: 25 -> 31\n",
      "2023-06-12 11:52:54,380 - __main__ - INFO - Loading sequence: 26 -> 31\n",
      "2023-06-12 11:52:57,791 - __main__ - INFO - Loading sequence: 27 -> 31\n",
      "2023-06-12 11:53:01,190 - __main__ - INFO - Loading sequence: 28 -> 31\n",
      "2023-06-12 11:53:04,605 - __main__ - INFO - Loading sequence: 29 -> 31\n",
      "2023-06-12 11:53:07,981 - __main__ - INFO - Loading sequence: 30 -> 31\n",
      "2023-06-12 11:53:11,493 - __main__ - INFO - Shifter shape: torch.Size([38500990, 220])\n",
      "2023-06-12 11:53:11,494 - __main__ - INFO - Loading intial clickstream to cuda\n",
      "2023-06-12 11:53:11,495 - __main__ - INFO - Writing sequence to disk: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 44% | 57% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 11:53:22,668 - __main__ - INFO - Saving to disk: torch_ready_data/files_used_30/sequence_index_30.npz\n",
      "2023-06-12 11:57:13,480 - __main__ - INFO - Generating sequence for 40\n",
      "2023-06-12 11:57:13,480 - __main__ - INFO - Generating shifted clickstreams from 31 -> 41\n",
      "2023-06-12 11:57:13,481 - __main__ - INFO - Loading sequence: 31 -> 41\n",
      "2023-06-12 11:57:16,982 - __main__ - INFO - Loading sequence: 32 -> 41\n",
      "2023-06-12 11:57:20,575 - __main__ - INFO - Loading sequence: 33 -> 41\n",
      "2023-06-12 11:57:24,125 - __main__ - INFO - Loading sequence: 34 -> 41\n",
      "2023-06-12 11:57:27,498 - __main__ - INFO - Loading sequence: 35 -> 41\n",
      "2023-06-12 11:57:30,866 - __main__ - INFO - Loading sequence: 36 -> 41\n",
      "2023-06-12 11:57:34,314 - __main__ - INFO - Loading sequence: 37 -> 41\n",
      "2023-06-12 11:57:37,773 - __main__ - INFO - Loading sequence: 38 -> 41\n",
      "2023-06-12 11:57:41,264 - __main__ - INFO - Loading sequence: 39 -> 41\n",
      "2023-06-12 11:57:44,758 - __main__ - INFO - Loading sequence: 40 -> 41\n",
      "2023-06-12 11:57:48,374 - __main__ - INFO - Shifter shape: torch.Size([38500990, 220])\n",
      "2023-06-12 11:57:48,375 - __main__ - INFO - Loading intial clickstream to cuda\n",
      "2023-06-12 11:57:48,376 - __main__ - INFO - Writing sequence to disk: 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 96% | 57% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 11:57:59,330 - __main__ - INFO - Saving to disk: torch_ready_data/files_used_30/sequence_index_40.npz\n",
      "2023-06-12 12:01:45,594 - __main__ - INFO - Sessionize complete for sequences [10, 20, 30, 40]\n"
     ]
    }
   ],
   "source": [
    "class Arguments:\n",
    "    seq_list = [10, 20, 30, 40]\n",
    "    input_path = 'calculated_features'\n",
    "    output_path = 'torch_ready_data'\n",
    "    data_subset = None\n",
    "    save_s3 = False\n",
    "\n",
    "for data_subset in [30]:\n",
    "    args = Arguments()\n",
    "    args.data_subset = data_subset\n",
    "    df = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://dissertation-data-dmiller/calculated_features/.ipynb_checkpoints/core_dict_30_files-checkpoint.json\n",
      "delete: s3://dissertation-data-dmiller/calculated_features/core_dict_10_files.json\n",
      "delete: s3://dissertation-data-dmiller/calculated_features/core_dict_30_files.json\n",
      "delete: s3://dissertation-data-dmiller/calculated_features/files_used_30\n",
      "delete: s3://dissertation-data-dmiller/calculated_features/files_used_10\n",
      "delete: s3://dissertation-data-dmiller/calculated_features/files_used_10_description.csv\n",
      "delete: s3://dissertation-data-dmiller/calculated_features/files_used_2\n",
      "upload: calculated_features/files_used_2/calculated_features.parquet to s3://dissertation-data-dmiller/calculated_features/files_used_2/calculated_features.parquet\n",
      "upload: calculated_features/files_used_10/calculated_features.parquet to s3://dissertation-data-dmiller/calculated_features/files_used_10/calculated_features.parquet\n",
      "upload: calculated_features/files_used_30/calculated_features.parquet to s3://dissertation-data-dmiller/calculated_features/files_used_30/calculated_features.parquet\n",
      "delete: s3://dissertation-data-dmiller/torch_ready_data/files_used_10/sequence_index_0/arr_0.npy\n",
      "delete: s3://dissertation-data-dmiller/torch_ready_data/files_used_10/sequence_index_20/arr_0.npy\n",
      "delete: s3://dissertation-data-dmiller/torch_ready_data/files_used_10/sequence_index_10/arr_0.npy\n",
      "delete: s3://dissertation-data-dmiller/torch_ready_data/files_used_10/sequence_index_30/arr_0.npy\n",
      "^Cmpleted 497.5 MiB/8.8 GiB (17.0 MiB/s) with 20 file(s) remaining \n",
      "cancelled: ctrl-c received                                         \n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync calculated_features/ s3://dissertation-data-dmiller/calculated_features/ --delete\n",
    "!aws s3 sync torch_ready_data/ s3://dissertation-data-dmiller/torch_ready_data/ --delete\n",
    "!aws s3 sync rl_ready_data_conv s3://dissertation-data-dmiller/rl_ready_data_conv/ --delete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1da7ff4cdcccf44e7e228c52b231f7d5c5854d5618af555ed3871fd5cba609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
