{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install python-dotenv --quiet\n",
    "!python -m pip install awscli --quiet\n",
    "!python -m pip install gym stable-baselines3[extra] boto3 --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync experiments s3://dissertation-data-dmiller/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dissertation-data-dmiller/rl_ready_data_conv/files_used_30/window_1/batched_eval rl_ready_data_conv/files_used_30/window_1/batched_eval\n",
    "!aws s3 sync s3://dissertation-data-dmiller/rl_ready_data_conv/files_used_30/window_1/batched_train rl_ready_data_conv/files_used_30/window_1/batched_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant.py\n",
    "FEATURE_COLUMNS = [\n",
    "    \n",
    "    \"user_count\",\n",
    "    \"project_count\", \n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event\",\n",
    "    \"cum_session_time\",\n",
    "    \"expanding_click_average\",\n",
    "   \n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_event\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"delta_last_event\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_count_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"cum_session_event_raw\",\n",
    "    \"date_time\"\n",
    "]\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'session_minutes',\n",
    "    'size_cutoff',\n",
    "    'time_cutoff',\n",
    "    'reward'\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    \"label\",\n",
    "    \"pred\"\n",
    "]\n",
    "\n",
    "LOAD_COLS = list(set(FEATURE_COLUMNS + METADATA + RL_STAT_COLS + PREDICTION_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load policy_list.py\n",
    "POLICY_LIST = [\n",
    "    {\n",
    "        \"algo\": \"dqn_pred_cnn\",\n",
    "        \"run_date\": \"2023-06-13_16-11-42\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load policies/cnn_policy\n",
    "from typing import Dict, List, Type, Union\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import logging\n",
    "global logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CustomConv1dFeatures(BaseFeaturesExtractor):\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_sequences_features(cls, n_sequences, n_features):\n",
    "        cls.n_sequences = n_sequences\n",
    "        cls.n_features = n_features\n",
    "        \n",
    "    \n",
    "    def __init__(self, observation_space: spaces.Box, features_dim=24):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        self.cnn_1 = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ELU()\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.conv_1_reshape = nn.Conv1d(\n",
    "            self.n_features,\n",
    "            self.n_features*2,\n",
    "            kernel_size=1,\n",
    "            padding=0\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.a_pool_1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.cnn_bottleneck_wide = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features*2, self.n_features*4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*4),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*4, self.n_features*4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*4),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*4, self.n_features*4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*4),\n",
    "            nn.ELU()   \n",
    "        )\n",
    "        \n",
    "        self.conv_2_reshape = nn.Conv1d(\n",
    "            self.n_features*2,\n",
    "            self.n_features*4,\n",
    "            kernel_size=1,\n",
    "            padding=0\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.cnn_bottleneck_narrow = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features*4, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.conv_3_reshape = nn.Conv1d(\n",
    "            self.n_features*4,\n",
    "            self.n_features*2,\n",
    "            kernel_size=1,\n",
    "            padding=0\n",
    "        )\n",
    "        \n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features*2, self.n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features, self.n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features, self.n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.conv_4_reshape = nn.Conv1d(\n",
    "            self.n_features*2,\n",
    "            self.n_features,\n",
    "            kernel_size=1,\n",
    "            padding=0\n",
    "        )\n",
    "                \n",
    "        self.down_max = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features, self.n_features // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features // 2),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features // 2, self.n_features // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features // 2),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features // 2, self.n_features // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features // 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.mpool_flat = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.down_max_reshape = nn.Conv1d(\n",
    "            self.n_features,\n",
    "            self.n_features // 2,\n",
    "            kernel_size=1,\n",
    "            padding=0\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sample_tensor = torch.zeros((1, self.n_features, self.n_sequences))\n",
    "            sample_tensor = self.cnn_1(sample_tensor) + self.conv_1_reshape(sample_tensor)\n",
    "            sample_tensor = self.a_pool_1(sample_tensor)\n",
    "            sample_tensor = self.cnn_bottleneck_wide(sample_tensor) + self.conv_2_reshape(sample_tensor)\n",
    "            sample_tensor = self.cnn_bottleneck_narrow(sample_tensor) + self.conv_3_reshape(sample_tensor)\n",
    "            sample_tensor = self.downsample(sample_tensor) + self.conv_4_reshape(sample_tensor)\n",
    "            sample_tensor = self.down_max(sample_tensor) + self.down_max_reshape(sample_tensor)\n",
    "            mpool_flat_out = self.mpool_flat(sample_tensor)\n",
    "            linear_in = mpool_flat_out.shape[1]\n",
    "            self.final_out_linear = nn.Sequential(\n",
    "\n",
    "                nn.Linear(linear_in, features_dim),\n",
    "                nn.ELU()\n",
    "            )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        obs_cnn_1 = self.cnn_1(obs) + self.conv_1_reshape(obs)\n",
    "        obs_cnn_1 = self.a_pool_1(obs_cnn_1)\n",
    "        \n",
    "        obs_cnn_2 = self.cnn_bottleneck_wide(obs_cnn_1) + self.conv_2_reshape(obs_cnn_1) \n",
    "        obs_cnn_3 = self.cnn_bottleneck_narrow(obs_cnn_2) + self.conv_3_reshape(obs_cnn_2)\n",
    "        obs_cnn_4 = self.downsample(obs_cnn_3) + self.conv_4_reshape(obs_cnn_3)\n",
    "        obs_cnn_5 = self.down_max(obs_cnn_4) + self.down_max_reshape(obs_cnn_4)\n",
    "        \n",
    "        mpool_flat_out = self.mpool_flat(obs_cnn_5)\n",
    "        \n",
    "        return self.final_out_linear(mpool_flat_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment\n",
    "# %load environment\n",
    "# %load environment\n",
    "import gym\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "MAX_EVAL_SIZE = 75\n",
    "\n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, dataset, out_features, n_sequences, evaluation=False):\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.unique_sessions = self.dataset[['user_id', 'session_30_count_raw']].drop_duplicates()\n",
    "        self.n_sequences = n_sequences\n",
    "        self.current_session = None\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        self.n_sequences = n_sequences\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        max_session_size = self.dataset['session_size'].max()\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=91, shape=(len(out_features) + 3, n_sequences + 1), dtype=np.float32)\n",
    "\n",
    "        self.evalution = evaluation\n",
    "        self.episode_bins = []\n",
    "        self.exp_runs = 0\n",
    "\n",
    "    def reset(self):\n",
    "        random_session = np.random.randint(0, self.unique_sessions.shape[0])\n",
    "        \n",
    "        user_to_run, session_to_run = self.unique_sessions.iloc[random_session][['user_id', 'session_30_count_raw']]\n",
    "        self.current_session = self._get_events(user_to_run, session_to_run)\n",
    "        self.metadata = self._metadata()\n",
    "        self.current_session_index = 1\n",
    "        self.reward = 0\n",
    "        return self._state()\n",
    "    \n",
    "    def _row_to_dict(self, metadata):\n",
    "        \"\"\"\n",
    "        Convert a row of metadata to a dictionary.\n",
    "        \"\"\"\n",
    "        return metadata.to_dict()\n",
    "    \n",
    "    def _reward_exp(self, cum_session_event_raw):\n",
    "        \"\"\"\n",
    "        Reward shaping as\n",
    "            0 if cum_session_event_raw < size_cutoff\n",
    "            (cum_session_event_raw - size_cutoff) * (cum_session_event_raw / size_cutoff) otherwise\n",
    "        \"\"\"\n",
    "        if cum_session_event_raw <= self.metadata['size_cutoff']:\n",
    "            return cum_session_event_raw / self.metadata['size_cutoff']\n",
    "        \n",
    "        return (cum_session_event_raw - self.metadata['size_cutoff']) * (cum_session_event_raw / self.metadata['size_cutoff'])\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        self._take_action(action)\n",
    "        # if is_legal < 0:\n",
    "        #     self.exp_runs += 1\n",
    "        #     self.metadata['ended_event'] = -1\n",
    "        #     self.metadata['ended_time'] = -1\n",
    "        #     self.metadata['exp_runs'] = self.exp_runs\n",
    "        #     self.episode_bins.append(self._row_to_dict(self.metadata))\n",
    "            \n",
    "        #     return None, float(-1), True, {}\n",
    "            \n",
    "        next_state, done, meta = self._calculate_next_state()\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            current_session_index = self.current_session_index if \\\n",
    "                self.current_session_index != self.current_session.shape[0] else self.current_session.shape[0] - 1\n",
    "            \n",
    "            self.exp_runs += 1\n",
    "            self.metadata['ended_event'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['ended_time'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            self.metadata['exp_runs'] = self.exp_runs\n",
    "            self.episode_bins.append(self._row_to_dict(self.metadata))\n",
    "            \n",
    "            self.metadata['ended_event'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['ended_time'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            self.metadata['exp_runs'] = self.exp_runs\n",
    "            self.episode_bins.append(self._row_to_dict(self.metadata))\n",
    "           \n",
    "            cum_session_event_raw = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            reward_exp = self._reward_exp(cum_session_event_raw)\n",
    "            \n",
    "            return next_state, reward_exp , done, {}\n",
    "        else:\n",
    "            self.reward = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "            cum_session_event_raw = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "            \n",
    "            reward_exp = self._reward_exp(cum_session_event_raw)\n",
    "    \n",
    "            self.current_session_index += 1        \n",
    "            \n",
    "            return next_state, reward_exp, done, meta\n",
    "    \n",
    "    def _metadata(self):\n",
    "        session_metadata = self.current_session.iloc[0][RL_STAT_COLS].copy()\n",
    "        session_metadata['ended'] = 0\n",
    "        for meta_col in ['small', 'medium', 'large']:\n",
    "            session_metadata[f'inc_{meta_col}'] = 0\n",
    "            session_metadata[f'time_{meta_col}'] = 0\n",
    "\n",
    "        return session_metadata\n",
    "    \n",
    "    def flush_episode_bins(self):\n",
    "        episode_bins = self.episode_bins.copy()\n",
    "        self.episode_bins = []\n",
    "        return episode_bins\n",
    "    \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        if (self.current_session_index == self.current_session.shape[0]):\n",
    "            return None, True, {}\n",
    "\n",
    "        if self._continuing_in_session():\n",
    "            return self._state(), False, {}\n",
    "    \n",
    "        return None, True, {}\n",
    "         \n",
    "    def _continuing_in_session(self):\n",
    "        event_cutoff = self.current_session.iloc[self.current_session_index]['size_cutoff']\n",
    "        current_session_event = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "        if current_session_event <= event_cutoff or current_session_event  >= MAX_EVAL_SIZE:\n",
    "            return True\n",
    "    \n",
    "        extending_low = self._probability_extending(current_session_event, self.metadata['inc_small']) - \\\n",
    "            (0.05 + np.random.normal(-0.02, 0.1, 100).mean())\n",
    "\n",
    "            \n",
    "        extending_medium = self._probability_extending(current_session_event, self.metadata['inc_medium']) - \\\n",
    "            (0.1 + np.random.normal(-0.02, 0.1, 100).mean()) \n",
    "            \n",
    "        extending_large = self._probability_extending(current_session_event, self.metadata['inc_large']) + \\\n",
    "            (0.2 + np.random.normal(-0.02, 0.1, 100).mean())\n",
    "            \n",
    "        return any([\n",
    "            extending_low > 0.4 and extending_low <= 0.75,\n",
    "            extending_medium > 0.4 and extending_medium <= 0.75,\n",
    "            extending_large > 0.4 and extending_large <= 0.75\n",
    "        ])\n",
    "        \n",
    "           \n",
    "    \n",
    "    def _probability_extending(self, current_session_event, incentive_event):\n",
    "        if incentive_event == 0:\n",
    "            return 0\n",
    "         \n",
    "        continue_session = norm(\n",
    "            loc=max(incentive_event, 1),\n",
    "            scale=max(incentive_event *.75, 1)\n",
    "        ).cdf(max(current_session_event, 1)) \n",
    "        \n",
    "        return continue_session\n",
    "        \n",
    "\n",
    "    def _get_events(self, user_id, session):\n",
    "        subset = self.dataset[\n",
    "            (self.dataset['user_id'] == user_id) &\n",
    "            (self.dataset['session_30_count_raw'] == session).copy()\n",
    "        ]\n",
    "\n",
    "        subset = subset.sort_values(by=['date_time'])\n",
    "        return subset\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        if action == 0:\n",
    "            return 1\n",
    "        \n",
    "        current_session_index = self.current_session_index if \\\n",
    "            self.current_session_index != self.current_session.shape[0] else self.current_session.shape[0] - 1\n",
    "    \n",
    "        if action == 1:\n",
    "            if self.metadata['inc_small'] > 0:\n",
    "                return 1\n",
    "                # return -1\n",
    "\n",
    "            self.metadata['inc_small'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_small'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "    \n",
    "        elif action == 2:\n",
    "            if self.metadata['inc_medium'] > 0:\n",
    "                return 1\n",
    "            self.metadata['inc_medium'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_medium'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            if self.metadata['inc_large'] > 0:\n",
    "                return 1\n",
    "            self.metadata['inc_large'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_large'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "\n",
    "    def _state(self):\n",
    "\n",
    "        if self.current_session_index > self.n_sequences:\n",
    "            events = self.current_session.iloc[self.current_session_index - (self.n_sequences + 1):self.current_session_index][self.out_features]\n",
    "            events['inc_small'] = self.metadata['inc_small']\n",
    "            events['inc_medium'] = self.metadata['inc_medium']\n",
    "            events['inc_large'] = self.metadata['inc_large']\n",
    "            \n",
    "            events = events.values\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            delta = min((self.n_sequences + 1)- self.current_session_index, self.n_sequences)\n",
    "            zero_cat = np.zeros((delta, len(self.out_features) + 3))\n",
    "            events = self.current_session.iloc[:max(self.current_session_index, 1)][self.out_features]\n",
    "            \n",
    "            events['inc_small'] = self.metadata['inc_small']\n",
    "            events['inc_medium'] = self.metadata['inc_medium']\n",
    "            events['inc_large'] = self.metadata['inc_large']\n",
    "            \n",
    "            \n",
    "            events = np.concatenate((zero_cat, events), axis=0)\n",
    "        \n",
    "        return events.astype(np.float32).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_results_eval_cpu.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from pprint import pformat\n",
    "from typing import Callable\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.dqn.policies import CnnPolicy\n",
    "import boto3\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, A2C, SAC, TD3\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "torch.set_printoptions(precision=2, linewidth=200, sci_mode=False)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import zipfile\n",
    "import torch.nn as nn\n",
    "\n",
    "S3_BASELINE_PATH = 'dissertation-data-dmiller'\n",
    "N_SEQUENCES = 15\n",
    "CHECKPOINT_FREQ = 100_000\n",
    "TB_LOG = 10_000\n",
    "WINDOW = 2\n",
    "import glob\n",
    "TB_LOG = 10_000\n",
    "WINDOW = 1\n",
    "REWARD_CLIP = 90\n",
    "MIN_MAX_RANGE = (10, 90)\n",
    "\n",
    "global logger\n",
    "\n",
    "logger = logging.getLogger('rl_results_eval')\n",
    "logger.setLevel(logging.INFO)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "import re\n",
    "\n",
    "def parse_args():\n",
    "    parse = argparse.ArgumentParser()\n",
    "    parse.add_argument('--write_path', type=str, default='rl_evaluation')\n",
    "    parse.add_argument('--part', type=str, default='eval')\n",
    "    parse.add_argument('--algo', type=str, default='dqn_pred_cnn'),\n",
    "\n",
    "    parse.add_argument('--run_date', type=str, default='2023-06-13_16-11-42'),\n",
    "    parse.add_argument('--n_files', type=int, default=30)\n",
    "                       \n",
    "    return parse.parse_args()\n",
    "\n",
    "\n",
    "def find_s3_candidate(algo, run_date):\n",
    "    \n",
    "    folder_prefix = os.path.join(\n",
    "        'experiments',\n",
    "        algo,\n",
    "        run_date,\n",
    "        'checkpoints'\n",
    "    )\n",
    "\n",
    "    \n",
    "    logger.info(f'Looking for files in {folder_prefix}')\n",
    "    \n",
    "    files = [\n",
    "        {\n",
    "            'key': file['Key'],\n",
    "            'last_modified': file['LastModified'],\n",
    "            'check_index': int(re.sub('[^0-9]', '', file['Key'].split('/')[-1]))\n",
    "        }\n",
    "        for file in client.list_objects_v2(Bucket=S3_BASELINE_PATH, Prefix=folder_prefix)['Contents']\n",
    "    ]\n",
    "    \n",
    "    s3_candidate = sorted(files, key=lambda x: x['check_index'])[-1]['key']\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    logger.info(f'Found candiate: {s3_candidate}')\n",
    "    \n",
    "    return s3_candidate\n",
    "\n",
    "def get_policy(algo, run_date):\n",
    "    \n",
    "    \n",
    "    s3_candidate = find_s3_candidate(algo, run_date)\n",
    "    if not os.path.exists(os.path.dirname(s3_candidate)):\n",
    "        logger.info(f'Creating policy path {os.path.dirname(s3_candidate)}')\n",
    "        \n",
    "        os.makedirs(os.path.dirname(s3_candidate), exist_ok=True)\n",
    "       \n",
    "    # return s3_candidate \n",
    "    client.download_file(S3_BASELINE_PATH, s3_candidate, s3_candidate)\n",
    "    return s3_candidate\n",
    "        \n",
    "\n",
    "def simplify_experiment(vectorized_df):\n",
    "    vectorized_df = [\n",
    "        df[(df['session_size'] >= MIN_MAX_RANGE[0]) & (df['session_size'] <= MIN_MAX_RANGE[1])] for df in vectorized_df\n",
    "    ]\n",
    "\n",
    "    return vectorized_df\n",
    "\n",
    "      \n",
    "def download_dataset_from_s3(client, base_read_path, full_read_path):\n",
    "    logger.info(f'Downloading data from {base_read_path}')\n",
    "    os.makedirs(base_read_path, exist_ok=True)\n",
    "    \n",
    "    logger.info(f'Downloading data from dissertation-data-dmiller/{full_read_path}')\n",
    "    client.download_file(\n",
    "        'dissertation-data-dmiller',\n",
    "        full_read_path,\n",
    "        full_read_path\n",
    "    )\n",
    "    logger.info(f'Downloaded data from dissertation-data-dmiller/{full_read_path}')\n",
    "    \n",
    "\n",
    "def simplify_experiment(vectorized_df):\n",
    "    vectorized_df = [\n",
    "        df[(df['session_size'] >= MIN_MAX_RANGE[0]) & (df['session_size'] <= MIN_MAX_RANGE[1])] for df in vectorized_df\n",
    "    ]\n",
    "\n",
    "    return vectorized_df\n",
    "\n",
    "def _label_or_pred(algo):\n",
    "    if 'label' in algo:\n",
    "        return 'label'\n",
    "    elif 'pred' in algo:\n",
    "        return 'pred'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def main(args):\n",
    "    \n",
    "    global client\n",
    "    client = boto3.client('s3')\n",
    "\n",
    "\n",
    "    logger.info('Starting offlline evaluation of RL model')\n",
    "    \n",
    "    write_path, part, algo, run_date, n_files, eval_episodes, extra_args = (\n",
    "        args.write_path,\n",
    "        args.part,\n",
    "        args.algo,\n",
    "        args.run_date,\n",
    "        args.n_files,\n",
    "        args.eval_episodes,\n",
    "        args.extra_args\n",
    "    )\n",
    "    \n",
    "    \n",
    "    read_path = os.path.join(\n",
    "        'rl_ready_data_conv',\n",
    "        f'files_used_{n_files}',\n",
    "        'window_1',\n",
    "        f'batched_{part}'\n",
    "    )\n",
    "   \n",
    "    logger.info(f'Reading from {read_path}, writing to {write_path}') \n",
    "    files_to_read = glob.glob(os.path.join(read_path, '*.parquet'))\n",
    "    logger.info(f'Found {len(files_to_read)} files to read')\n",
    "    \n",
    "    \n",
    "    feature_cols = FEATURE_COLUMNS + [_label_or_pred(algo)] if _label_or_pred(algo) else FEATURE_COLUMNS\n",
    "   \n",
    "    logger.info(f'n features: {len(feature_cols)}')\n",
    "\n",
    "    env_files = [\n",
    "        pd.read_parquet(file) for file in files_to_read\n",
    "    ]\n",
    "\n",
    "\n",
    "    logger.info(f'Loaded env files: clipping to {MIN_MAX_RANGE}')\n",
    "    \n",
    "    env_files = simplify_experiment(env_files)\n",
    "    vec_env = DummyVecEnv([lambda: CitizenScienceEnv(df, feature_cols, N_SEQUENCES) for df in env_files])\n",
    "   \n",
    "\n",
    "    \n",
    "    tensorboard_dir = os.path.join(\n",
    "        args.write_path,\n",
    "        f'{args.part}/{algo}_{run_date}'\n",
    "    )\n",
    "       \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    " \n",
    "    logger.info(f'Logging to {tensorboard_dir}, n envs: {len(env_files)}: device: {device}')\n",
    "    \n",
    "    monitor_env = VecMonitor(vec_env)\n",
    "    policy_path = get_policy(algo, run_date)\n",
    "   \n",
    "    logger.info(f'Setting up model')\n",
    "    if 'dqn' in algo:\n",
    "        model = DQN.load(policy_path, env=monitor_env, tensorboard_log=tensorboard_dir, device=device)\n",
    "    \n",
    "    logger.info(f'Running evaluation: n_episodes: {eval_episodes}')\n",
    "    evaluate_policy(\n",
    "        model,\n",
    "        model.get_env(),\n",
    "        deterministic=False,\n",
    "        n_eval_episodes=eval_episodes,\n",
    "       \n",
    "    )\n",
    "    \n",
    "    logger.info(f'Finished evaluation getting attributes')\n",
    "    dist_list = model.get_env().get_attr('episode_bins')\n",
    "    values_to_log = [item for sublist in dist_list for item in sublist if len(sublist) > 0]\n",
    "    out_files = pd.DataFrame(values_to_log)\n",
    "    logger.info(f'Attributes: {out_files.shape}')\n",
    "    write_path = os.path.join(\n",
    "        write_path,\n",
    "        part,\n",
    "        f'{algo}_{eval_episodes}_{args.extra_args}.parquet'\n",
    "    )\n",
    "    if not os.path.exists(os.path.dirname(write_path)):\n",
    "        logger.info(f'Creating write path {os.path.dirname(write_path)}')\n",
    "        os.makedirs(os.path.dirname(write_path), exist_ok=True)\n",
    "        \n",
    "    logger.info(f'Writing to {write_path}')\n",
    "    out_files.to_parquet(write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/21/2023 07:43:03 AM Starting offlline evaluation of RL model\n",
      "06/21/2023 07:43:03 AM Reading from rl_ready_data_conv/files_used_30/window_1/batched_train, writing to rl_evaluation\n",
      "06/21/2023 07:43:03 AM Found 100 files to read\n",
      "06/21/2023 07:43:03 AM n features: 23\n",
      "06/21/2023 07:43:04 AM Loaded env files: clipping to (10, 90)\n",
      "06/21/2023 07:43:05 AM Logging to rl_evaluation/train/dqn_pred_cnn_2023-06-20_10-38-22, n envs: 100: device: cuda\n",
      "06/21/2023 07:43:05 AM Looking for files in experiments/dqn_pred_cnn/2023-06-20_10-38-22/checkpoints\n",
      "06/21/2023 07:43:06 AM Found candiate: experiments/dqn_pred_cnn/2023-06-20_10-38-22/checkpoints/rl_model_6600000_steps.zip\n",
      "06/21/2023 07:43:07 AM Setting up model\n",
      "06/21/2023 07:43:07 AM Running evaluation: n_episodes: 25000\n",
      "06/21/2023 07:59:56 AM Finished evaluation getting attributes\n",
      "06/21/2023 07:59:56 AM Attributes: (54664, 15)\n",
      "06/21/2023 07:59:56 AM Writing to rl_evaluation/train/dqn_pred_cnn_25000_no_pen.parquet\n"
     ]
    }
   ],
   "source": [
    "class TrainArg:\n",
    "    write_path = 'rl_evaluation'\n",
    "    part = 'train'\n",
    "    algo = 'dqn_pred_cnn'\n",
    "    run_date = '2023-06-20_10-38-22'\n",
    "    n_files = 30\n",
    "    eval_episodes = 25_000\n",
    "    extra_args = 'no_pen'\n",
    "    \n",
    "class EvalArg:\n",
    "    write_path = 'rl_evaluation'\n",
    "    part = 'eval'\n",
    "    algo = 'dqn_pred_cnn'\n",
    "    run_date = '2023-06-20_10-38-22'\n",
    "    n_files = 30\n",
    "    eval_episodes = 15_000\n",
    "    extra_args = 'no_pen'\n",
    "    \n",
    "\n",
    "# main(TrainArg)\n",
    "main(EvalArg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/21/2023 07:59:56 AM Starting offlline evaluation of RL model\n",
      "06/21/2023 07:59:56 AM Reading from rl_ready_data_conv/files_used_30/window_1/batched_train, writing to rl_evaluation\n",
      "06/21/2023 07:59:56 AM Found 100 files to read\n",
      "06/21/2023 07:59:56 AM n features: 22\n",
      "06/21/2023 07:59:58 AM Loaded env files: clipping to (10, 90)\n",
      "06/21/2023 07:59:59 AM Logging to rl_evaluation/train/dqn_None_cnn_2023-06-20_15-27-49, n envs: 100: device: cuda\n",
      "06/21/2023 07:59:59 AM Looking for files in experiments/dqn_None_cnn/2023-06-20_15-27-49/checkpoints\n",
      "06/21/2023 07:59:59 AM Found candiate: experiments/dqn_None_cnn/2023-06-20_15-27-49/checkpoints/rl_model_6600000_steps.zip\n",
      "06/21/2023 07:59:59 AM Creating policy path experiments/dqn_None_cnn/2023-06-20_15-27-49/checkpoints\n",
      "06/21/2023 08:00:01 AM Setting up model\n",
      "06/21/2023 08:00:02 AM Running evaluation: n_episodes: 25000\n"
     ]
    }
   ],
   "source": [
    "class TrainArg:\n",
    "    write_path = 'rl_evaluation'\n",
    "    part = 'train'\n",
    "    algo = 'dqn_None_cnn'\n",
    "    run_date = '2023-06-20_15-27-49'\n",
    "    n_files = 30\n",
    "    eval_episodes = 25_000\n",
    "    extra_args = 'no_pen'\n",
    "    \n",
    "class EvalArg:\n",
    "    write_path = 'rl_evaluation'\n",
    "    part = 'eval'\n",
    "    algo = 'dqn_None_cnn'\n",
    "    run_date = '2023-06-20_15-27-49'\n",
    "    n_files = 30\n",
    "    eval_episodes = 15_000\n",
    "    extra_args = 'no_pen'\n",
    "    \n",
    "\n",
    "main(TrainArg)\n",
    "main(EvalArg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
