{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install python-dotenv pqdm torch --quiet\n",
    "!python -m pip install gym stable-baselines3[extra] boto3 --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant.py\n",
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time\",\n",
    "    \"date_time\",\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    'seq_10',\n",
    "    'sq_20'\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'sim_size',\n",
    "    'session_minutes',\n",
    "    'sim_minutes',\n",
    "    'cum_session_event_raw',\n",
    "    'cum_platform_time_raw',\n",
    "    'reward',\n",
    "    'session_30_raw',\n",
    "    'cum_platform_time_raw',\n",
    "    'global_session_time',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_LIST = [\n",
    "    {\n",
    "        \"algo\": \"DQN\",\n",
    "        \"feature_extractor\": \"CNN\",\n",
    "        \"lstm\": \"label\",\n",
    "         \"run_time\": \"2023-05-23-16-18\"\n",
    "    },\n",
    "    {\n",
    "        \"algo\": \"DQN\",\n",
    "        \"feature_extractor\": \"MLP\",\n",
    "        \"lstm\": \"label\",\n",
    "        \"run_time\": \"2023-05-23-15-55\"\n",
    "    }\n",
    "    {\n",
    "        \"algo\": \"DQN\",\n",
    "        \"feature_extractor\": \"CNN\",\n",
    "        \"lstm\": \"seq_40\",\n",
    "        \"run_time\": \"2023-05-23-16-31\"\n",
    "    },\n",
    "    {\n",
    "        \"algo\": \"DQN\",\n",
    "        \"feature_extractor\": \"MLP\",\n",
    "        \"lstm\": \"seq_40\",\n",
    "        \"run_time\": \"2023-05-24-08-18\"\n",
    "    },\n",
    "    {\n",
    "        \"algo\": \"DQN\",\n",
    "        \"feature_extractor\": \"MLP\",\n",
    "        \"lstm\": \"no_pred\",\n",
    "        \"run_time\": \"2023-05-25-20-00\"\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load policies/cnn_policy.py\n",
    "# %load policies/cnn_policy\n",
    "\n",
    "import torch\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CustomConv1dFeatures(BaseFeaturesExtractor):\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_sequences_features(cls, n_sequences, n_features):\n",
    "        cls.n_sequences = n_sequences\n",
    "        cls.n_features = n_features\n",
    "        \n",
    "    \n",
    "    def __init__(self, observation_space: spaces.Box, features_dim=20):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        self.cnn_1 = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            \n",
    "            nn.AvgPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.cnn_2 = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features*2, self.n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features, self.n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.act = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out_shape = self.act(self.cnn_2(self.cnn_1(torch.zeros((1, self.n_features, self.n_sequences))))).shape[1]\n",
    "            self.linear = nn.Linear(out_shape, features_dim)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        out = self.cnn_1(obs)\n",
    "        out = self.cnn_2(out)\n",
    "        out = self.act(out)\n",
    "        return self.linear(out)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment_eval\n",
    "# %load environment\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "import gym\n",
    "\n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, dataset, out_features, n_sequences):\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.n_sequences = n_sequences\n",
    "        self.current_session = None\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        self.n_sequences = n_sequences\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(len(out_features), n_sequences + 1), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        user_to_run, session_to_run = self.dataset.sample(1)[['user_id', 'session_30_raw']].values[0]\n",
    "        self.current_session = self._get_events(user_to_run, session_to_run)\n",
    "        self.metadata = self._metadata()\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        return self._state()\n",
    "    \n",
    "    def _row_to_dict(self, metadata):\n",
    "        \"\"\"\n",
    "        Convert a row of metadata to a dictionary.\n",
    "        \"\"\"\n",
    "        return metadata.to_dict()\n",
    "\n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "\n",
    "        next_state, done, meta = self._calculate_next_state()\n",
    "        \n",
    "        if done:\n",
    "            current_session_index = self.current_session_index if \\\n",
    "                self.current_session_index != self.current_session.shape[0] else self.current_session_index - 1\n",
    "        \n",
    "            self.metadata['ended'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['reward'] = self.reward\n",
    "            meta = self._row_to_dict(self.metadata)\n",
    "            return next_state, float(self.reward), done, meta\n",
    "        else:\n",
    "            self.reward = self.current_session.iloc[self.current_session_index]['reward'] \n",
    "            self.current_session_index += 1        \n",
    "        return next_state, float(self.reward), done, meta\n",
    "    \n",
    "    def _metadata(self):\n",
    "        session_metadata = self.current_session.iloc[0][RL_STAT_COLS]\n",
    "        session_metadata['ended'] = 0\n",
    "        session_metadata['incentive_index'] = 0\n",
    "        return session_metadata\n",
    "    \n",
    "    \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        if (self.current_session_index == self.current_session.shape[0]):\n",
    "            return None, True, {}\n",
    "\n",
    "        if self._continuing_in_session():\n",
    "            return self._state(), False, {}\n",
    "    \n",
    "        return None, True, {}\n",
    "        \n",
    "      \n",
    "  \n",
    "    def _continuing_in_session(self):\n",
    "        sim_counts = self.metadata['sim_size']\n",
    "        current_session_count = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "        if current_session_count <= sim_counts:\n",
    "            return True\n",
    "        \n",
    "        extending_session = self._probability_extending_session(current_session_count)\n",
    "        \n",
    "        return all([extending_session >= .3, extending_session <= .7])\n",
    "        \n",
    "    \n",
    "    def _probability_extending_session(self, current_session_count):\n",
    "        if self.metadata['incentive_index'] == 0:\n",
    "            return 0\n",
    "        \n",
    "        scale = max(5, int(self.metadata['session_size'] / 4))\n",
    "        continue_session = norm(\n",
    "            loc=self.metadata['incentive_index'],\n",
    "            scale=scale\n",
    "        ).cdf(current_session_count)\n",
    "        \n",
    "        return continue_session\n",
    "        \n",
    "\n",
    "    def _get_events(self, user_id, session):\n",
    "        subset = self.dataset[\n",
    "            (self.dataset['user_id'] == user_id) &\n",
    "            (self.dataset['session_30_raw'] == session)\n",
    "        ]\n",
    "   \n",
    "        return subset.sort_values(by=['date_time']).reset_index(drop=True)\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        if action == 0 or self.metadata['incentive_index'] > 0:\n",
    "            return\n",
    "        \n",
    "        current_session_index = min(self.current_session_index, self.current_session.shape[0] - 1)\n",
    "        self.metadata['incentive_index'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "        self.metadata['incentive_time'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "        \n",
    "    def _state(self):\n",
    "\n",
    "        if self.current_session_index > self.n_sequences:\n",
    "            events = self.current_session.iloc[self.current_session_index - (self.n_sequences + 1):self.current_session_index][self.out_features].values\n",
    "            \n",
    "        else:\n",
    "            delta = min((self.n_sequences + 1)- self.current_session_index, self.n_sequences)\n",
    "            zero_cat = np.zeros((delta, len(self.out_features)))\n",
    "            events = self.current_session.iloc[:max(self.current_session_index, 1)][self.out_features].values\n",
    "            events = np.concatenate((zero_cat, events), axis=0)\n",
    "            \n",
    "\n",
    "        return events.astype(np.float32).T\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_results_eval_cpu.py\n",
    "import logging\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from policies.cnn_policy import CustomConv1dFeatures\n",
    "from rl_constant import LABEL, METADATA, OUT_FEATURE_COLUMNS, PREDICTION_COLS, RL_STAT_COLS\n",
    "from stable_baselines3 import DQN, PPO, A2C, SAC, TD3\n",
    "import json\n",
    "from pqdm.processes import pqdm\n",
    "ALL_COLS = LABEL + METADATA + OUT_FEATURE_COLUMNS  + PREDICTION_COLS\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "torch.set_printoptions(precision=2, linewidth=200, sci_mode=False)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "S3_BASELINE_PATH = 'dissertation-data-dmiller'\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "CUM_SESSION_EVENT_RAW = 3\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "N_SEQUENCES = 40\n",
    "EVAL_SPLIT = 0.15\n",
    "\n",
    "global logger\n",
    "\n",
    "logger = logging.getLogger('rl_results_eval')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def find_s3_candidate(client, feature_extractor, lstm, run_time):\n",
    "    \n",
    "    if lstm == 'seq_20':\n",
    "        lstm = 'seq_40'\n",
    "    folder_prefix = os.path.join(\n",
    "        'reinforcement_learning_incentives',\n",
    "        'n_files_30',\n",
    "        f'{feature_extractor}_{lstm}',\n",
    "        'results',\n",
    "        run_time,\n",
    "        'checkpoints',\n",
    "    )\n",
    "    \n",
    "    logger.info(f'Looking for files in {folder_prefix}')\n",
    "    \n",
    "    files = [\n",
    "        {\n",
    "            'key': file['Key'],\n",
    "            'last_modified': file['LastModified'],\n",
    "        }\n",
    "        for file in client.list_objects_v2(Bucket=S3_BASELINE_PATH, Prefix=folder_prefix)['Contents']\n",
    "    ]\n",
    "    \n",
    "    s3_candidate = max(files, key=lambda x: x['last_modified'])['key']\n",
    "    \n",
    "    logger.info(f'Found candiate: {s3_candidate}')\n",
    "    \n",
    "    return s3_candidate\n",
    "\n",
    "def get_policy(client, feature_extractor, lstm, run_time, algo):\n",
    "    \n",
    "    \n",
    "    s3_candidate = find_s3_candidate(client, feature_extractor, lstm, run_time)\n",
    "    \n",
    "    model_base_path, download_path = (\n",
    "        os.path.join('reinforcement_learning_incentives', f'{feature_extractor}_{lstm}'),\n",
    "        os.path.join('reinforcement_learning_incentives', f'{feature_extractor}_{lstm}', f'{algo}.zip') \n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(model_base_path):\n",
    "        logger.info(f'Creating directory {model_base_path}')\n",
    "        os.makedirs(model_base_path)\n",
    "        client.download_file(S3_BASELINE_PATH, s3_candidate, download_path)\n",
    "        logger.info(f'Loading model from {s3_candidate} to {download_path}')\n",
    "\n",
    "    logger.info(f'Checkpoint load path: {download_path}')\n",
    "    return download_path\n",
    "        \n",
    "def _lstm_loader(lstm):\n",
    "    if lstm == 'no_pred':\n",
    "        return []\n",
    "\n",
    "    return LABEL if lstm == 'label' else ['seq_20']\n",
    "\n",
    "def run_session(args):\n",
    "    dataset, feature_meta, model, out_features, n_sequences, info_container = args\n",
    "    _, feature_meta = feature_meta\n",
    "    subset = dataset[\n",
    "        (dataset['user_id'] == feature_meta['user_id']) &\n",
    "        (dataset['session_30_raw'] == feature_meta['session_30_raw'])\n",
    "    ]\n",
    "    \n",
    "    env = CitizenScienceEnv(subset, out_features, n_sequences)\n",
    "    step = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(step, deterministic=True)\n",
    "        step, rewards, done, info = env.step(action)\n",
    "    info_container.append(info)\n",
    "    \n",
    "    return info_container\n",
    "\n",
    "def run_experiment(model, dataset, out_features, n_sequences):\n",
    "    \n",
    "    info_container = []\n",
    "    \n",
    "    dataset = dataset.loc[:,~dataset.columns.duplicated()].copy()\n",
    "    info_container = []\n",
    "    unique_sessions = dataset[['user_id', 'session_30_raw']].drop_duplicates() \n",
    "    logger.info(f'Running experiment with {model}: n_session={len(unique_sessions)}')\n",
    "    \n",
    "    args = [\n",
    "        (dataset, feature_meta, model, out_features, n_sequences, info_container) for _, feature_meta in unique_sessions.iterrows()\n",
    "    ]\n",
    "    \n",
    "    pqdm(args, run_session, n_jobs=8)\n",
    "    \n",
    "    return info_container\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "def get_dataset(conv_path, n_files, window, part='train'):\n",
    "    \n",
    "    \n",
    "    conv_path =  os.path.join(conv_path, f'files_used_{n_files}')\n",
    "\n",
    "\n",
    "    if not os.path.exists(conv_path):\n",
    "        logger.info(f'Creating directory {conv_path}')\n",
    "        os.makedirs(conv_path)\n",
    "        \n",
    "    \n",
    "    conv_path = os.path.join(conv_path, f'window_{window}_{part}.parquet')\n",
    "    \n",
    "    if not os.path.exists(conv_path):\n",
    "        logger.info(f'Convolutional dataset not found at {conv_path}: creating')\n",
    "        logger.info(f'Getting dataset from bucket: {S3_BASELINE_PATH}, key: {conv_path}')\n",
    "        client.download_file(S3_BASELINE_PATH, conv_path, conv_path)\n",
    "        \n",
    "\n",
    "    logger.info(f'Loading convolutional dataset from {conv_path}')\n",
    "    df = pd.read_parquet(conv_path)\n",
    "        \n",
    "    logger.info(f'Dataset loaded: {df.shape}')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def run_exp_wrapper(args, df, write_path):\n",
    "        policy_weights = get_policy(client, args['feature_extractor'].lower(), args['lstm'], args['run_time'], args['algo'])\n",
    "        print(policy_weights)\n",
    "        all_features, out_features = (\n",
    "            METADATA + OUT_FEATURE_COLUMNS + RL_STAT_COLS + _lstm_loader(args['lstm']),\n",
    "            OUT_FEATURE_COLUMNS + _lstm_loader(args['lstm'])\n",
    "        )\n",
    "        df = df[all_features]\n",
    "        env = CitizenScienceEnv(df, out_features, 40)\n",
    "        \n",
    "        if args['feature_extractor'].lower() == 'cnn':\n",
    "            CustomConv1dFeatures.setup_sequences_features(N_SEQUENCES + 1, 21)\n",
    "            logger.info(f'Using custom CNN feature extractor')\n",
    "            policy_kwargs = dict(\n",
    "                features_extractor_class=CustomConv1dFeatures,\n",
    "                net_arch=[10]\n",
    "            )\n",
    "        \n",
    "            model = DQN(policy='CnnPolicy', env=env, policy_kwargs=policy_kwargs)\n",
    "            model.set_parameters(policy_weights)\n",
    "            \n",
    "        experiment = run_experiment(model, df, out_features, N_SEQUENCES)\n",
    "        experiemnt_df = pd.DataFrame(experiment)\n",
    "        \n",
    "        logger.info(f'Finished experiment: {args}')\n",
    "        if not os.path.exists(write_path):\n",
    "            os.makedirs(write_path)\n",
    "        \n",
    "        write_path = os.path.join(\n",
    "            write_path,\n",
    "            f'{args[\"algo\"]}_{args[\"feature_extractor\"]}_{args[\"lstm\"]}_{args[\"run_time\"]}.parquet'   \n",
    "        )\n",
    "        \n",
    "        logger.info(f'Writing experiment to {write_path}')\n",
    "        \n",
    "        experiemnt_df.to_parquet(write_path)\n",
    "        \n",
    "    \n",
    "    \n",
    "     \n",
    "def main(args):\n",
    "    \n",
    "    global client\n",
    "    client = boto3.client('s3')\n",
    "    logger.info('Starting offlline evaluation of RL model')\n",
    "    \n",
    "    conv_path, write_path, n_files, window, data_part = (\n",
    "        args.read_path,\n",
    "        args.write_path,\n",
    "        args.n_files, \n",
    "        args.device, \n",
    "        args.window, \n",
    "        args.data_part\n",
    "    )\n",
    "    \n",
    "    \n",
    "    df = get_dataset(conv_path, n_files, window, data_part)\n",
    "    df = df[:10000]\n",
    "    for r in POLICY_LIST:\n",
    "        logger.info(f'Running evaluation for {r}')\n",
    "        run_exp_wrapper(r, df.copy(), write_path)\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument:\n",
    "    read_path = 'rl_ready_data_conv'\n",
    "    write_path = 'rl_results'\n",
    "    n_files = 2\n",
    "    n_sequences = 40\n",
    "    window = 2\n",
    "    data_part = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
