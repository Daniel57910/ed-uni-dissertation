{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install python-dotenv --quiet\n",
    "!python -m pip install gym stable-baselines3[extra] --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install awscli boto3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync reinforcement_learning_incentives s3://dissertation-data-dmiller/reinforcement_learning_incentives  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! if ![ -d \"rl_ready_data\" ]; then aws s3 cp s3://dissertation-data-dmiller/rl_ready_data ./rl_ready_data --recursive; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant.py\n",
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time\",\n",
    "    \n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    'seq_10',\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'sim_size',\n",
    "    'session_minutes',\n",
    "    'sim_minutes',\n",
    "    'reward',\n",
    "    'session_30_raw',\n",
    "    'cum_platform_time_raw',\n",
    "    'global_session_time',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load callback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat\n",
    "from datetime import datetime\n",
    "\n",
    "class DistributionCallback(BaseCallback):\n",
    "    \n",
    "    @classmethod\n",
    "    def tensorboard_setup(cls, log_dir, log_freq):\n",
    "        cls._log_dir = log_dir\n",
    "        cls._log_freq = log_freq\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        output_formats = self.logger.output_formats\n",
    "        self.tb_formatter = next(f for f in output_formats if isinstance(f, TensorBoardOutputFormat))\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._log_freq == 0:\n",
    "            dist_list = self.training_env.env_method('dists')\n",
    "            values_to_log = np.concatenate([d for d in dist_list if d.shape[0] > 0])\n",
    "\n",
    "            values_df = pd.DataFrame(\n",
    "                values_to_log, \n",
    "                columns=RL_STAT_COLS + ['ended', 'incentive_index', 'n_episodes'] + ['date_time']\n",
    "            )\n",
    "            \n",
    "            dist_session_time = (values_df['session_minutes'] - values_df['reward']).mean()\n",
    "            dist_session_end = (values_df['session_size'] - values_df['ended']).mean()\n",
    "            \n",
    "            dist_sim_time = (values_df['reward'] - values_df['sim_minutes']).mean()\n",
    "            dist_sim_end = (values_df['ended'] - values_df['sim_size']).mean()\n",
    "\n",
    "\n",
    "            dist_inc_session = (values_df['session_size'] - values_df['incentive_index']).mean()\n",
    "            dist_inc_end = (values_df['ended'] - values_df['incentive_index']).mean()\n",
    "            dist_inc_sim_index = (values_df['sim_size'] - values_df['incentive_index']).mean()\n",
    "\n",
    "            n_call = self.n_calls // self._log_freq\n",
    "            \n",
    "            self.tb_formatter.writer.add_scalar('distance/session/max_reward::decrease', dist_session_time, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/session/max_ended::decrease', dist_session_end, n_call)\n",
    "           \n",
    "            self.tb_formatter.writer.add_scalar('distance/session/sim_reward::increase', dist_sim_time, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/session/sim_ended::increase', dist_sim_end, n_call)\n",
    "            \n",
    "            \n",
    "            self.tb_formatter.writer.add_scalar('distance/incentive/max_incentive::decrease', dist_inc_session, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/incentive/ended_incentive', dist_inc_end, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/incentive/sim_inc_placement::decrease', dist_inc_sim_index, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/ended_sim_size::increase', dist_inc_sim_index, n_call)\n",
    "            \n",
    "            self.tb_formatter.writer.flush()\n",
    "            \n",
    "            current_time = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "            values_df.to_csv(f'{self._log_dir}/dist_{n_call}_{current_time}.csv', index=False)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment\n",
    "# %load environment\n",
    "import gym\n",
    "import numpy as np\n",
    "# from rl_constant import RL_STAT_COLS\n",
    "from scipy.stats import norm\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "import gym\n",
    "from datetime import datetime\n",
    "\n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, dataset, out_features, n_sequences):\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.n_episodes = 0\n",
    "        self.n_sequences = n_sequences\n",
    "        self.current_session = None\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        self.metadata_container = []\n",
    "        self.n_sequences = n_sequences\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(len(out_features), n_sequences + 1), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_episodes += 1\n",
    "        user_to_run, session_to_run = self.dataset.sample(1)[['user_id', 'session_30_raw']].values[0]\n",
    "        self.current_session = self._get_events(user_to_run, session_to_run)\n",
    "        self.metadata = self._metadata()\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        return self._state()\n",
    "\n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        next_state, done, meta = self._calculate_next_state()\n",
    "        \n",
    "        if done:\n",
    "            curent_session_index = min(self.current_session_index, self.current_session.shape[0] - 1)\n",
    "            self.metadata['ended'] = self.current_session.iloc[curent_session_index]['cum_session_event_raw']\n",
    "            self.metadata['reward'] = self.reward\n",
    "            self.metadata['date_time'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            self.metadata_container.append(self.metadata.values)\n",
    "            return next_state, float(self.reward), done, meta\n",
    "        else:\n",
    "            self.reward = self.current_session.iloc[self.current_session_index]['reward'] \n",
    "            self.current_session_index += 1        \n",
    "        return next_state, float(self.reward), done, meta\n",
    "    \n",
    "    def _metadata(self):\n",
    "        session_metadata = self.current_session.iloc[0][RL_STAT_COLS]\n",
    "        session_metadata['ended'] = 0\n",
    "        session_metadata['incentive_index'] = 0\n",
    "        session_metadata['n_episodes'] = self.n_episodes\n",
    "        return session_metadata\n",
    "    \n",
    "    \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        if (self.current_session_index == self.current_session.shape[0]):\n",
    "            return None, True, {}\n",
    "\n",
    "        if self._continuing_in_session():\n",
    "            return self._state(), False, {}\n",
    "    \n",
    "        return None, True, {}\n",
    "        \n",
    "      \n",
    "  \n",
    "    def _continuing_in_session(self):\n",
    "        sim_counts = self.metadata['sim_size']\n",
    "        current_session_count = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "        if current_session_count < sim_counts:\n",
    "            return True\n",
    "        \n",
    "        extending_session = self._probability_extending_session(current_session_count)\n",
    "        \n",
    "        return all([extending_session >= .3, extending_session <= .7])\n",
    "        \n",
    "    \n",
    "    def _probability_extending_session(self, current_session_count):\n",
    "        if self.metadata['incentive_index'] == 0:\n",
    "            return 0\n",
    "        \n",
    "        scale = max(5, int(self.metadata['session_size'] / 4))\n",
    "        continue_session = norm(\n",
    "            loc=self.metadata['incentive_index'],\n",
    "            scale=scale\n",
    "        ).cdf(current_session_count)\n",
    "        \n",
    "        return continue_session\n",
    "        \n",
    "\n",
    "    def _get_events(self, user_id, session):\n",
    "        subset = self.dataset[\n",
    "            (self.dataset['user_id'] == user_id) &\n",
    "            (self.dataset['session_30_raw'] == session)\n",
    "        ]\n",
    "   \n",
    "        return subset.sort_values(by=['date_time']).reset_index(drop=True)\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        if action == 0 or self.metadata['incentive_index'] > 0:\n",
    "            return\n",
    "        \n",
    "        current_session_index = min(self.current_session_index, self.current_session.shape[0] - 1)\n",
    "        self.metadata['incentive_index'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "        \n",
    "    def _state(self):\n",
    "\n",
    "        if self.current_session_index > self.n_sequences:\n",
    "            events = self.current_session.iloc[self.current_session_index - (self.n_sequences + 1):self.current_session_index][self.out_features].values\n",
    "            \n",
    "        else:\n",
    "            delta = min((self.n_sequences + 1)- self.current_session_index, self.n_sequences)\n",
    "            zero_cat = np.zeros((delta, len(self.out_features)))\n",
    "            events = self.current_session.iloc[:max(self.current_session_index, 1)][self.out_features].values\n",
    "            events = np.concatenate((zero_cat, events), axis=0)\n",
    "            \n",
    "\n",
    "        return events.astype(np.float32).T\n",
    "  \n",
    "    \n",
    "    def dists(self):\n",
    "        metadata_container = self.metadata_container.copy()\n",
    "        self.metadata_container = []\n",
    "        return np.array(metadata_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load policies/cnn_policy\n",
    "# %load policies/cnn_policy\n",
    "from typing import Dict, List, Type, Union\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CustomConv1dFeatures(BaseFeaturesExtractor):\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_sequences_features(cls, n_sequences, n_features):\n",
    "        cls.n_sequences = n_sequences\n",
    "        cls.n_features = n_features\n",
    "        \n",
    "    \n",
    "    def __init__(self, observation_space: spaces.Box, features_dim=20):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        self.cnn_1 = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            \n",
    "            nn.AvgPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.cnn_2 = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features*2, self.n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features, self.n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.act = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out_shape = self.act(self.cnn_2(self.cnn_1(torch.zeros((1, self.n_features, self.n_sequences))))).shape[1]\n",
    "            self.linear = nn.Linear(out_shape, features_dim)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        out = self.cnn_1(obs)\n",
    "        out = self.cnn_2(out)\n",
    "        out = self.act(out)\n",
    "        return self.linear(out)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load incentive_reinforcement_learning_cpu.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from pprint import pformat\n",
    "from typing import Callable\n",
    "# import boto3\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from callback import DistributionCallback\n",
    "# from environment import CitizenScienceEnv\n",
    "# from policies.cnn_policy import CustomConv1dFeatures\n",
    "# from rl_constant import LABEL, METADATA, OUT_FEATURE_COLUMNS, PREDICTION_COLS\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.common.callbacks import (CallbackList,\n",
    "                                                CheckpointCallback,\n",
    "                                                StopTrainingOnMaxEpisodes)\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "\n",
    "ALL_COLS = METADATA + OUT_FEATURE_COLUMNS \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "torch.set_printoptions(precision=2, linewidth=200, sci_mode=False)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "S3_BASELINE_PATH = 's3://dissertation-data-dmiller'\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "CUM_SESSION_EVENT_RAW = 3\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "\n",
    "\n",
    "def train_eval_split(dataset, logger):\n",
    "    train_split = int(dataset.shape[0] * TRAIN_SPLIT)\n",
    "    eval_split = int(dataset.shape[0] * EVAL_SPLIT)\n",
    "    test_split = dataset.shape[0] - train_split - eval_split\n",
    "    logger.info(f'Train size: 0:{train_split}, eval size: {train_split}:{train_split+eval_split}: test size: {train_split + eval_split}:{dataset.shape[0]}')\n",
    "    train_dataset, eval_dataset, test_split = dataset[:train_split], dataset[train_split:train_split+eval_split], dataset[train_split+eval_split:]\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'eval': eval_dataset,\n",
    "        'test': test_split\n",
    "    }\n",
    "\n",
    "def generate_metadata(dataset):\n",
    "    \n",
    "    session_size = dataset.groupby(['user_id', 'session_30_raw'])['size_of_session'].max().reset_index(name='session_size')\n",
    "    session_minutes = dataset.groupby(['user_id', 'session_30_raw'])['cum_session_time_raw'].max().reset_index(name='session_minutes')\n",
    "    \n",
    "    sim_minutes = dataset.groupby(['user_id', 'session_30_raw'])['cum_session_time_raw'].quantile(.7, interpolation='nearest').reset_index(name='sim_minutes')\n",
    "    sim_size = dataset.groupby(['user_id', 'session_30_raw'])['cum_session_event_raw'].quantile(.7, interpolation='nearest').reset_index(name='sim_size')\n",
    "    \n",
    "    \n",
    "    sessions = [session_size, session_minutes, sim_minutes, sim_size]\n",
    "    sessions = reduce(lambda left, right: pd.merge(left, right, on=['user_id', 'session_30_raw']), sessions)\n",
    "    dataset = pd.merge(dataset, sessions, on=['user_id', 'session_30_raw'])\n",
    "    dataset['reward'] = dataset['cum_session_time_raw']\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parse = argparse.ArgumentParser()\n",
    "    parse.add_argument('--read_path', type=str, default='datasets/rl_ready_data')\n",
    "    parse.add_argument('--n_files', type=int, default=2)\n",
    "    parse.add_argument('--n_episodes', type=int, default=50)\n",
    "    parse.add_argument('--n_sequences', type=int, default=40)\n",
    "    parse.add_argument('--n_envs', type=int, default=100)\n",
    "    parse.add_argument('--lstm', type=str, default='seq_10')\n",
    "    parse.add_argument('--device', type=str, default='cpu')\n",
    "    parse.add_argument('--checkpoint_freq', type=int, default=1000)\n",
    "    parse.add_argument('--tb_log', type=int, default=100)\n",
    "    parse.add_argument('--feature_extractor', type=str, default='cnn') \n",
    "    parse.add_argument('--window', type=int, default=4)\n",
    "    args = parse.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def run_reinforcement_learning_incentives(environment, logger, n_episodes=1):\n",
    "    for epoch in range(n_episodes):\n",
    "        environment_comp = False\n",
    "        state = environment.reset()\n",
    "        i = 0\n",
    "        while not environment_comp:\n",
    "            next_action = (\n",
    "                1 if np.random.uniform(low=0, high=1) > 0.8 else 0\n",
    "            )\n",
    "            state, rewards, environment_comp, meta = environment.step(next_action)\n",
    "            i +=1\n",
    "            if i % 100 == 0:\n",
    "                logger.info(f'Step: {i} - Reward: {rewards}')\n",
    "                \n",
    "        logger.info(f'Epoch: {epoch} - Reward: {rewards}')\n",
    "        print(environment.user_sessions.head(10))\n",
    "\n",
    "\n",
    "def remove_events_in_minute_window(df):\n",
    "    df['second_window'] = df['second'] // 10\n",
    "    df = df.drop_duplicates(\n",
    "        subset=['user_id', 'session_30_raw', 'year', 'month', 'day', 'hour', 'minute'],\n",
    "        keep='last'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def convolve_delta_events(df, window):\n",
    "    df['convolved_delta_event'] = (\n",
    "        df.set_index('date_time').groupby(by=['user_id', 'session_30_raw'], group_keys=False) \\\n",
    "            .rolling(f'{window}T', min_periods=1)['delta_last_event'] \\\n",
    "            .mean()\n",
    "            .reset_index(name='convolved_event_delta')['convolved_event_delta']\n",
    "    )\n",
    "\n",
    "    df['delta_last_event'] = df['convolved_delta_event']\n",
    "\n",
    "    return df\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "    \n",
    "def main(args):\n",
    "    \n",
    "    exec_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    logger.info('Starting Incentive Reinforcement Learning')\n",
    "    \n",
    "    read_path, n_files, n_sequences, n_episodes, device, n_envs, lstm, tb_log, check_freq, feature_ext, window = (\n",
    "        args.read_path, \n",
    "        args.n_files, \n",
    "        args.n_sequences, \n",
    "        args.n_episodes, \n",
    "        args.device,\n",
    "        args.n_envs,\n",
    "        args.lstm,\n",
    "        args.tb_log,\n",
    "        args.checkpoint_freq,\n",
    "        args.feature_extractor,\n",
    "        args.window\n",
    "    )\n",
    "    \n",
    "    read_path = os.path.join(\n",
    "        read_path,\n",
    "        f'files_used_{n_files}',\n",
    "        f'predicted_data.parquet'\n",
    "    )\n",
    "   \n",
    "    # if not os.path.exists(read_path):\n",
    "    #     logger.info(f'Downloading data from {S3_BASELINE_PATH}/{read_path}') \n",
    "    #     s3 = boto3.client('s3')\n",
    "    #     s3.download_file(\n",
    "    #         S3_BASELINE_PATH,     \n",
    "    #         read_path,\n",
    "    #         read_path\n",
    "    #     )\n",
    "             \n",
    "            \n",
    "    logger.info(f'Reading data from {read_path}')\n",
    "    \n",
    "    logger.info(f'Setting up model with prediction {lstm}')\n",
    "    df = pd.read_parquet(read_path, columns=ALL_COLS + [args.lstm] if args.lstm else ALL_COLS)\n",
    "    df['date_time'] = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute', 'second']], errors='coerce')\n",
    "\n",
    "\n",
    "    df = df.sort_values(by=['date_time'])    \n",
    "    \n",
    "    logger.info(f'N events:: {df.shape[0]} creating training partitions')\n",
    "    df = df.head(int(df.shape[0] * .7))\n",
    "    logger.info(f'N events after 70% split: {df.shape[0]}')\n",
    "    size_of_session = df.groupby(['user_id', 'session_30_raw']).size().reset_index(name='size_of_session')\n",
    "    df = pd.merge(df, size_of_session, on=['user_id', 'session_30_raw'])\n",
    "    df['cum_session_event_raw'] = df.groupby(['user_id', 'session_30_raw'])['date_time'].cumcount() + 1\n",
    "    \n",
    "    logger.info(f'Convolution over {window} minute window')\n",
    "    df = convolve_delta_events(df, window)\n",
    "    logger.info(f'Convolving over {window} minute window complete: generating metadata')\n",
    "    df = generate_metadata(df) \n",
    "    logger.info(f'Metadata generated: selecting events only at {window} minute intervals')\n",
    "    df = df[df['minute'] % window == 0]\n",
    "    logger.info(f'Data read: {df.shape[0]} rows, {df.shape[1]} columns, dropping events within 2 minute window')\n",
    "    df = remove_events_in_minute_window(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    logger.info(f'Number of events after dropping events within 2 minute window: {df.shape[0]}')\n",
    "    \n",
    "    unique_episodes = df[['user_id', 'session_30_raw']].drop_duplicates()\n",
    "    unique_sessions = df[['session_30_raw']].drop_duplicates()\n",
    "    df = df.drop(columns=['year', 'month', 'day', 'hour', 'minute', 'second', 'second_window'])\n",
    "    out_features = OUT_FEATURE_COLUMNS + [lstm] if lstm else OUT_FEATURE_COLUMNS\n",
    "    \n",
    "\n",
    "    \n",
    "    citizen_science_vec =DummyVecEnv([lambda: CitizenScienceEnv(df, out_features, n_sequences) for i in range(n_envs)])\n",
    "    logger.info(f'Vectorized environments created')\n",
    "    \n",
    "    base_path = os.path.join(\n",
    "        # S3_BASELINE_PATH,\n",
    "        'reinforcement_learning_incentives_2',\n",
    "        f'n_files_{n_files}',\n",
    "        feature_ext + '_' + 'label' if lstm.startswith('continue') else lstm,\n",
    "        'results',\n",
    "        exec_time,\n",
    "    ) \n",
    "    \n",
    "    \n",
    "    tensorboard_dir, checkpoint_dir = (\n",
    "        os.path.join(base_path, 'training_metrics'),\n",
    "        os.path.join(base_path, 'checkpoints')\n",
    "    )\n",
    "\n",
    "    logger.info(f'Creating callbacks, monitors and loggerss')\n",
    "    callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=n_episodes, verbose=1)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=check_freq // (n_envs // 2), save_path=checkpoint_dir, name_prefix='rl_model')\n",
    "    dist_callback = DistributionCallback()\n",
    "    DistributionCallback.tensorboard_setup(tensorboard_dir, tb_log)\n",
    "    callback_list = CallbackList([callback_max_episodes, dist_callback, checkpoint_callback])\n",
    "    monitor_train = VecMonitor(citizen_science_vec)\n",
    "    \n",
    "    \n",
    "    if feature_ext == 'cnn':\n",
    "        CustomConv1dFeatures.setup_sequences_features(n_sequences + 1, 21)\n",
    "        logger.info('Using custom 1 dimensional CNN feature extractor')\n",
    "        policy_kwargs = dict(\n",
    "            features_extractor_class=CustomConv1dFeatures,\n",
    "            net_arch=[10]\n",
    "        )\n",
    "        model = DQN(policy='CnnPolicy', env=monitor_train, verbose=1, tensorboard_log=tensorboard_dir, policy_kwargs=policy_kwargs, device=device, stats_window_size=1000)\n",
    "    else:\n",
    "        logger.info('Using default MLP feature extractor')\n",
    "        model = DQN(policy='MlpPolicy', env=monitor_train, verbose=1, tensorboard_log=tensorboard_dir, device=device, stats_window_size=1000)\n",
    "    \n",
    "    logger.info(f'Model created: policy')\n",
    "    \n",
    "    logger.info(pformat(model.policy))\n",
    "        \n",
    "    logger.info(f'Beginning training') \n",
    "    \n",
    "    # retutrn\n",
    "            \n",
    "    logger.info(pformat([\n",
    "        'n_episodes: {}'.format(n_episodes),\n",
    "        'read_path: {}'.format(read_path),\n",
    "        'n_files: {}'.format(n_files),\n",
    "        'n_sequences: {}'.format(n_sequences),\n",
    "        'n_envs: {}'.format(n_envs),\n",
    "        'total_timesteps: {}'.format(df.shape),\n",
    "        f'unique_episodes: {unique_episodes.shape[0]}',\n",
    "        'device: {}'.format(device),\n",
    "        'tensorboard_dir: {}'.format(tensorboard_dir),\n",
    "        'checkpoint_dir: {}'.format(checkpoint_dir)\n",
    "    ]))\n",
    "    \n",
    "    model.learn(total_timesteps=15_000_000, progress_bar=True, log_interval=1000, callback=callback_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument:\n",
    "    read_path = 'rl_ready_data'\n",
    "    n_files = 30\n",
    "    n_sequences = 40\n",
    "    n_episodes = 500_000\n",
    "    n_envs = 1000\n",
    "    lstm = 'seq_40'\n",
    "    device = 'cuda'\n",
    "    checkpoint_freq = 250_000\n",
    "    tb_log = 50\n",
    "    feature_extractor = 'cnn'\n",
    "    window = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/28/2023 11:27:35 AM Starting Incentive Reinforcement Learning\n",
      "05/28/2023 11:27:35 AM Reading data from rl_ready_data/files_used_30/predicted_data.parquet\n",
      "05/28/2023 11:27:35 AM Setting up model with prediction seq_40\n",
      "05/28/2023 11:27:51 AM N events:: 38500990 creating training partitions\n",
      "05/28/2023 11:27:51 AM N events after 70% split: 26950693\n",
      "05/28/2023 11:28:08 AM Convolution over 4 minute window\n",
      "05/28/2023 11:28:27 AM Convolving over 4 minute window complete: generating metadata\n",
      "05/28/2023 11:28:51 AM Metadata generated: selecting events only at 4 minute intervals\n",
      "05/28/2023 11:28:57 AM Data read: 6736552 rows, 43 columns, dropping events within 2 minute window\n",
      "05/28/2023 11:28:59 AM Number of events after dropping events within 2 minute window: 1173560\n",
      "05/28/2023 11:28:59 AM Vectorized environments created\n",
      "05/28/2023 11:28:59 AM Creating callbacks, monitors and loggerss\n",
      "05/28/2023 11:28:59 AM Using custom 1 dimensional CNN feature extractor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/28/2023 11:29:01 AM Model created: policy\n",
      "05/28/2023 11:29:01 AM CnnPolicy(\n",
      "  (q_net): QNetwork(\n",
      "    (features_extractor): CustomConv1dFeatures(\n",
      "      (cnn_1): Sequential(\n",
      "        (0): Conv1d(21, 42, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv1d(42, 42, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (4): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "        (6): Conv1d(42, 42, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (7): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (8): Conv1d(42, 42, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (9): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "      )\n",
      "      (cnn_2): Sequential(\n",
      "        (0): Conv1d(42, 21, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv1d(21, 21, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (4): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "      (act): Sequential(\n",
      "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      )\n",
      "      (linear): Linear(in_features=210, out_features=20, bias=True)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=10, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (q_net_target): QNetwork(\n",
      "    (features_extractor): CustomConv1dFeatures(\n",
      "      (cnn_1): Sequential(\n",
      "        (0): Conv1d(21, 42, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv1d(42, 42, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (4): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "        (6): Conv1d(42, 42, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (7): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (8): Conv1d(42, 42, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (9): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "      )\n",
      "      (cnn_2): Sequential(\n",
      "        (0): Conv1d(42, 21, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv1d(21, 21, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (4): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "      (act): Sequential(\n",
      "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      )\n",
      "      (linear): Linear(in_features=210, out_features=20, bias=True)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=10, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "05/28/2023 11:29:01 AM Beginning training\n",
      "05/28/2023 11:29:01 AM ['n_episodes: 500000',\n",
      " 'read_path: rl_ready_data/files_used_30/predicted_data.parquet',\n",
      " 'n_files: 30',\n",
      " 'n_sequences: 40',\n",
      " 'n_envs: 1000',\n",
      " 'total_timesteps: (1173560, 37)',\n",
      " 'unique_episodes: 56022',\n",
      " 'device: cuda',\n",
      " 'tensorboard_dir: '\n",
      " 'reinforcement_learning_incentives_2/n_files_30/seq_40/results/2023-05-28-11-27/training_metrics',\n",
      " 'checkpoint_dir: '\n",
      " 'reinforcement_learning_incentives_2/n_files_30/seq_40/results/2023-05-28-11-27/checkpoints']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to reinforcement_learning_incentives_2/n_files_30/seq_40/results/2023-05-28-11-27/training_metrics/DQN_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ba59b918294c0abe02caa29643f655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "main(Argument)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
