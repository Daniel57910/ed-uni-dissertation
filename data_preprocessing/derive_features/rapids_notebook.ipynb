{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.64 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch boto3 python-dotenv gputil --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "\"\"\"\n",
    "current time yyyy_mm_dd_hh_mm_ss\n",
    "using A6000 or bigger\n",
    "\"\"\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "so = open(f\"logs_{current_time}.log\", 'w', 10)\n",
    "sys.stdout.echo = so\n",
    "sys.stderr.echo = so\n",
    "\n",
    "get_ipython().log.handlers[0].stream = so\n",
    "get_ipython().log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sessionization_pandas.py\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import pdb\n",
    "import pprint as pp\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    import cupy as np\n",
    "    import numpy\n",
    "    import cudf as pd\n",
    "    import dask_cudf as dd\n",
    "    import pandas as cpu_pd\n",
    "    from cuml.preprocessing import MinMaxScaler\n",
    "    import GPUtil\n",
    "else:\n",
    "    import numpy as np\n",
    "    import numpy\n",
    "    import pandas as pd\n",
    "    import dask.dataframe as dd\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "torch.set_printoptions(precision=4)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(linewidth=200)\n",
    "\n",
    "torch.set_printoptions(linewidth=400, precision=4, sci_mode=False)\n",
    "\n",
    "SCALED_COLS =[\n",
    "    'timestamp',\n",
    "    'time_diff_seconds',\n",
    "    '30_minute_session_count',\n",
    "    '5_minute_session_count',\n",
    "    'task_within_session_count',\n",
    "    'user_count',\n",
    "    'project_count',\n",
    "    'country_count',\n",
    "]\n",
    "\n",
    "GENERATED_COLS = [\n",
    "    'cum_events',\n",
    "    'cum_projects',\n",
    "    'cum_time',\n",
    "    'cum_time_within_session',\n",
    "    'av_time_across_clicks',\n",
    "    'av_time_across_clicks_session',\n",
    "    'rolling_average_tasks_within_session',\n",
    "    'rolling_av_time_within_session',\n",
    "    'rolling_time_between_sessions',\n",
    "]\n",
    "\n",
    "ENCODED_COLS = [\n",
    "    'user_id',\n",
    "    'project_id',\n",
    "    'country'\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "TIMESTAMP_INDEX = 1\n",
    "\n",
    "INITIAL_LOAD_COLUMNS = ENCODED_COLS +  ['label', 'date_time'] +  [col for col in SCALED_COLS if 'timestamp' not in col and 'project_count' not in col]\n",
    "\n",
    "TIMESTAMP_INDEX = 1\n",
    "\n",
    "COUNTRY_ENCODING = {\n",
    "    'Finland': 1,\n",
    "    'United States': 2,\n",
    "    'China': 3,\n",
    "    'Singapore': 4,\n",
    "}\n",
    "\n",
    "PARTITION_LIST = [\n",
    "    {\n",
    "        'name': '125k',\n",
    "        'size': 125000,\n",
    "        'indexes': None\n",
    "    },\n",
    "    {\n",
    "        'name': '125m',\n",
    "        'size': 1250000,\n",
    "        'indexes': None\n",
    "    },\n",
    "    {\n",
    "        'name': '5m',\n",
    "        'size': 5000000,\n",
    "    },\n",
    "    {\n",
    "        'name': '10m',\n",
    "        'size': 10000000,\n",
    "    },\n",
    "    {\n",
    "        'name': '20m',\n",
    "        'size': 20000000,\n",
    "    },\n",
    "    {\n",
    "        'name': 'full',\n",
    "        'size': None,\n",
    "    }\n",
    "]\n",
    "\n",
    "class SessionizeData:\n",
    "    def __init__(self, df, max_sequence_index, write_path, partition_list=PARTITION_LIST, save_s3=True):\n",
    "        self.df = df\n",
    "        self.max_sequence_index = max_sequence_index + 1\n",
    "        self.min_sequence_index = self.max_sequence_index - 10\n",
    "        self.device = self._device()\n",
    "        self.sequences = numpy.arange(self.min_sequence_index, self.max_sequence_index).tolist()\n",
    "        self.seq_container = []\n",
    "        self.torch_sequences = None\n",
    "        self.output_path = write_path\n",
    "        self.partition_list = partition_list\n",
    "        self.save_s3 = save_s3\n",
    "\n",
    "    def _device(self):\n",
    "        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def _sequence_lazy(self):\n",
    "         return next(self._lazy_load_shifted_index())\n",
    "\n",
    "    def _shifters(self):\n",
    "        for _ in range(self.min_sequence_index, self.max_sequence_index):\n",
    "            print(f'Loading sequence: {_} -> {self.max_sequence_index}')\n",
    "            self.seq_container.append(self._sequence_lazy())\n",
    "            if torch.cuda.is_available():\n",
    "                GPUtil.showUtilization()\n",
    "\n",
    "        sequences = torch.cat(self.seq_container, dim=1).half()\n",
    "        return sequences\n",
    "\n",
    "    def generate_sequence(self):\n",
    "\n",
    "        print(f'Generating shifted clickstreams from {self.min_sequence_index} -> {self.max_sequence_index}')\n",
    "        sequence = self._shifters()\n",
    "\n",
    "        print(f'Shifters shape: {sequence.shape}')\n",
    "\n",
    "        cols_required =  ['label', 'total_events', 'timestamp_raw'] + ENCODED_COLS + SCALED_COLS + GENERATED_COLS\n",
    "        print(f'Columns required: {cols_required}')\n",
    "        print(f'Loading intial clickstream to {self.device}')\n",
    "\n",
    "        if self.max_sequence_index == 11:\n",
    "            print('Initial clickstream writing to disk')\n",
    "            initial_clickstream = self.df[cols_required].values.astype(np.float32)\n",
    "            self._sequence_to_disk(initial_clickstream, 0)\n",
    "\n",
    "        print(f'Writing sequence to disk: {self.max_sequence_index - 1}') \n",
    "        self._sequence_to_disk(sequence.cpu().numpy(), self.max_sequence_index - 1)\n",
    "\n",
    "\n",
    "    def _sequence_to_disk(self, partition, sequence_index):\n",
    "        if self.save_s3:\n",
    "            s3_client = boto3.client(\n",
    "                's3',\n",
    "                aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "                aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "            )\n",
    "\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "\n",
    "        partition_path = os.path.join(self.output_path, f'sequence_index_{sequence_index}.npz')\n",
    "        print(f'Saving to disk: {partition_path}')\n",
    "        np.savez_compressed(partition_path, partition)\n",
    "\n",
    "        if self.save_s3:\n",
    "            print(f'Uploading to s3: dissertation-data-dmiller/{partition_path}')\n",
    "            s3_client.upload_file(partition_path, 'dissertation-data-dmiller', partition_path)\n",
    "\n",
    "    def _lazy_load_shifted_index(self):\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        indx = self.sequences.pop(0)\n",
    "        torch_container = []\n",
    "        for col in ['project_id'] + SCALED_COLS + GENERATED_COLS:\n",
    "            sequence = self.df.groupby(GROUPBY_COLS)[col].shift(indx).fillna(0).values.astype(np.float16)\n",
    "            sequence_tensor = torch.tensor(sequence).to(self.device).half()\n",
    "            torch_container.append(sequence_tensor.unsqueeze(1))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        yield torch.cat(torch_container, dim=1)\n",
    "\n",
    "def _encode_countries(x):\n",
    "        if x == 'Finland':\n",
    "            return 1\n",
    "        elif x == 'United States':\n",
    "            return 2\n",
    "        elif x == 'China':\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "\n",
    "import pdb\n",
    "\n",
    "def join_for_encodings(df):\n",
    "\n",
    "\n",
    "    project_id_value_counts = df['project_id'].value_counts().reset_index().rename(columns={'index': 'project_id', 'project_id': 'project_count'})\n",
    "    df = df.merge(project_id_value_counts, on='project_id', how='left')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        df = df.compute().to_pandas()\n",
    "    else:\n",
    "        df = df.compute()\n",
    "\n",
    "    df['country'] = df['country'].apply(_encode_countries)\n",
    "\n",
    "    df['date_time'] = cpu_pd.to_datetime(df['date_time'])\n",
    "    total_events = df['user_id'].value_counts().reset_index().rename(columns={'index': 'user_id', 'user_id': 'total_events'})\n",
    "\n",
    "    df = df.merge(total_events, on='user_id', how='left')\n",
    "    df['cum_events'] = df[['user_id', 'task_within_session_count']].groupby('user_id').cumcount() + 1\n",
    "    df['cum_projects'] = (df.groupby('user_id')['project_id'].transform(lambda x: cpu_pd.CategoricalIndex(x).codes) + 1).astype('int32')\n",
    "\n",
    "    df['cum_time'] = df.groupby('user_id')['time_diff_seconds'].cumsum()\n",
    "    df['cum_time_within_session'] = df.groupby(['user_id', '30_minute_session_count'])['time_diff_seconds'].cumsum()\n",
    "    df['av_time_across_clicks'] = df.groupby('user_id')['time_diff_seconds'].rolling(1000, min_periods=1).mean().reset_index()['time_diff_seconds']\n",
    "    df['av_time_across_clicks_session'] = df.groupby(['user_id', '30_minute_session_count'])['time_diff_seconds'].rolling(10, min_periods=1).mean().reset_index()['time_diff_seconds']\n",
    "\n",
    "\n",
    "    av_num_events_within_session = df.groupby(['user_id', '30_minute_session_count'])['task_within_session_count'].max().reset_index()\n",
    "    av_num_events_within_session['rolling_average_tasks_within_session'] = av_num_events_within_session.groupby('user_id')['task_within_session_count'].rolling(10, min_periods=1).mean().reset_index()['task_within_session_count']\n",
    "\n",
    "    av_time_within_session = df.groupby(['user_id', '30_minute_session_count']).apply(lambda x: x['date_time'].max() - x['date_time'].min()).reset_index().rename(columns={0: 'av_time_within_session'})\n",
    "    av_time_within_session['av_time_within_session'] = av_time_within_session['av_time_within_session'].apply(lambda x: x.total_seconds())\n",
    "    av_time_within_session['rolling_av_time_within_session'] = av_time_within_session.groupby('user_id')['av_time_within_session'].rolling(10, min_periods=1).mean().reset_index()['av_time_within_session']\n",
    "\n",
    "\n",
    "    session_start_time = df.groupby(['user_id', '30_minute_session_count'])['date_time'].min().reset_index().rename(columns={'date_time': 'session_start_time'})\n",
    "    session_end_time = df.groupby(['user_id', '30_minute_session_count'])['date_time'].max().reset_index().rename(columns={'date_time': 'session_end_time'})\n",
    "\n",
    "    session_meta = session_start_time.merge(session_end_time, on=['user_id', '30_minute_session_count'], how='left')\n",
    "    session_meta['previous_end_time'] = session_meta.groupby(['user_id'])['session_end_time'].shift()\n",
    "    session_meta['previous_end_time'] = session_meta['previous_end_time'].fillna(0)\n",
    "    session_meta['time_between_sessions'] = session_meta.apply(lambda x: 0 if x['previous_end_time'] == 0 else (x['session_start_time'] - x['previous_end_time']), axis=1)\n",
    "    session_meta['time_between_sessions'] = session_meta['time_between_sessions'].apply(lambda x: 0 if x == 0 else x.seconds)\n",
    "    session_meta['rolling_time_between_sessions'] = session_meta.groupby('user_id')['time_between_sessions'].rolling(5,  min_periods=1).mean().reset_index()['time_between_sessions']\n",
    "\n",
    "    df = df.merge(av_num_events_within_session[['user_id', '30_minute_session_count', 'rolling_average_tasks_within_session']], on=['user_id', '30_minute_session_count'], how='left')\n",
    "    df = df.merge(av_time_within_session[['user_id', '30_minute_session_count', 'rolling_av_time_within_session']], on=['user_id', '30_minute_session_count'], how='left')\n",
    "    df = df.merge(session_meta[['user_id', '30_minute_session_count', 'rolling_time_between_sessions']], on=['user_id', '30_minute_session_count'], how='left')\n",
    "    user_id_hash = cpu_pd.DataFrame(df['user_id'].unique()).reset_index().rename(columns={'index': 'user_id_hash', 0: 'user_id'})\n",
    "    project_id_hash = cpu_pd.DataFrame(df['project_id'].unique()).reset_index().rename(columns={'index': 'project_id_hash', 0: 'project_id'})\n",
    "\n",
    "    user_id_hash['user_id_hash'] = user_id_hash['user_id_hash'] + 1\n",
    "    project_id_hash['project_id_hash'] = project_id_hash['project_id_hash'] + 1\n",
    "\n",
    "    df = df.merge(user_id_hash, on='user_id', how='left')\n",
    "    df = df.merge(project_id_hash, on='project_id', how='left')\n",
    "\n",
    "    df = df.drop(columns=['user_id', 'project_id'])\n",
    "    df = df.rename(columns={'user_id_hash': 'user_id', 'project_id_hash': 'project_id'})\n",
    "    if torch.cuda.is_available():\n",
    "        df = pd.from_pandas(df)\n",
    "    return df\n",
    "\n",
    "def prepare_for_sessionization(data_paths: list, scaler: MinMaxScaler):\n",
    "\n",
    "    df = dd.read_csv(data_paths, usecols=INITIAL_LOAD_COLUMNS)\n",
    "    return df\n",
    "\n",
    "    df = join_for_encodings(df)\n",
    "    df['timestamp_raw'] = df['date_time'].astype('int64') // 10**9\n",
    "    df = df.sort_values(by='date_time')\n",
    "\n",
    "    df['timestamp'] = df['timestamp_raw']\n",
    "    print(f'Loaded data: shape = {df.shape}, min_date, max_date: {df.date_time.min()}, {df.date_time.max()}')\n",
    "    print(f'Data label true: {df.label.value_counts() / len(df)}')\n",
    "    df = df[SCALED_COLS + GENERATED_COLS + ENCODED_COLS + ['timestamp_raw', 'label', 'total_events']]\n",
    "    df[SCALED_COLS + GENERATED_COLS] = scaler.fit_transform(df[SCALED_COLS + GENERATED_COLS].values)\n",
    "    return df.astype('float32')\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--max_sequence_index', type=int, default=10)\n",
    "    parser.add_argument('--input_path', type=str, default='../datasets/frequency_encoded_data')\n",
    "    parser.add_argument('--output_path', type=str, default='torch_ready_data')\n",
    "    parser.add_argument('--data_subset', type=int, default=60, help='Number of files to read from input path')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main(args):\n",
    "    #\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    torch.set_printoptions(precision=4)\n",
    "\n",
    "    np.set_printoptions(suppress=True)\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "    print(f\"Starting {current_time}\\nsubset of data: {args.data_subset}\\nreading from {args.input_path}\\nwrite_path {args.output_path}\\nseq_list {args.seq_list}\")\n",
    "\n",
    "\n",
    "    files = glob.glob(f'{args.input_path}/*.csv')\n",
    "    files = sorted(list(files))\n",
    "    files = files[:args.data_subset]\n",
    "\n",
    "    print(f\"Using {len(files)} files\")\n",
    "\n",
    "\n",
    "    df = prepare_for_sessionization(files, MinMaxScaler())\n",
    "    print('Dataframe columns:')\n",
    "    pp.pprint(list(df.columns))\n",
    "\n",
    "    for seq in args.seq_list:\n",
    "        output_path = os.path.join(\n",
    "            args.output_path,\n",
    "            f'files_used_{args.data_subset}',\n",
    "        )\n",
    "        sessionize = SessionizeData(df, seq, output_path, save_s3=args.save_s3)\n",
    "        sessionize.generate_sequence()\n",
    "        print(f'Finished writing {seq} to disk')\n",
    "    print(f'Exiting application...')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Arguments:\n",
    "    def __init__(self, seq_list):\n",
    "        self.seq_list = seq_list\n",
    "        self.input_path = 'datasets/frequency_encoded_data'\n",
    "        self.output_path = 'datasets/torch_ready_data_2'\n",
    "        self.data_subset = 5\n",
    "        self.save_s3 = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self, seq_list, data_subset):\n",
    "        self.seq_list = seq_list\n",
    "        self.input_path = 'frequency_encoded_data'\n",
    "        self.output_path = 'torch_ready_data_4'\n",
    "        self.data_subset = data_subset\n",
    "        self.save_s3 = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for 5 files: seq_list = [10, 20, 30, 40]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Arguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32/4035680434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_subset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m61\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Generating data for {data_subset} files: seq_list = {[10, 20, 30, 40]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Arguments' is not defined"
     ]
    }
   ],
   "source": [
    "for data_subset in [5, 30, 45, 61]:\n",
    "    print(f'Generating data for {data_subset} files: seq_list = {[10, 20, 30, 40]}')\n",
    "    args = Arguments([10, 20, 30, 40], data_subset)\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import numpy\n",
    "import cudf as pd\n",
    "import dask_cudf as dd\n",
    "import pandas as cpu_pd\n",
    "from cuml.preprocessing import MinMaxScaler\n",
    "import glob\n",
    "\n",
    "SCALED_COLS =[\n",
    "    'timestamp',\n",
    "    'time_diff_seconds',\n",
    "    '30_minute_session_count',\n",
    "    '5_minute_session_count',\n",
    "    'task_within_session_count',\n",
    "    'user_count',\n",
    "    'project_count',\n",
    "    'country_count',\n",
    "]\n",
    "\n",
    "GENERATED_COLS = [\n",
    "    'cum_events',\n",
    "    'cum_projects',\n",
    "    'cum_time',\n",
    "    'cum_time_within_session',\n",
    "    'av_time_across_clicks',\n",
    "    'av_time_across_clicks_session',\n",
    "    'rolling_average_tasks_within_session',\n",
    "    'rolling_av_time_within_session',\n",
    "    'rolling_time_between_sessions',\n",
    "]\n",
    "\n",
    "ENCODED_COLS = [\n",
    "    'user_id',\n",
    "    'project_id',\n",
    "    'country'\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "TIMESTAMP_INDEX = 1\n",
    "\n",
    "INITIAL_LOAD_COLUMNS = ENCODED_COLS +  ['label', 'date_time'] +  [col for col in SCALED_COLS if 'timestamp' not in col and 'project_count' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_sessionization(data_paths: list, scaler: MinMaxScaler):\n",
    "\n",
    "    df = dd.read_csv(data_paths, usecols=INITIAL_LOAD_COLUMNS)\n",
    "    return df\n",
    "\n",
    "    df = join_for_encodings(df)\n",
    "    df['timestamp_raw'] = df['date_time'].astype('int64') // 10**9\n",
    "    df = df.sort_values(by='date_time')\n",
    "\n",
    "    df['timestamp'] = df['timestamp_raw']\n",
    "    print(f'Loaded data: shape = {df.shape}, min_date, max_date: {df.date_time.min()}, {df.date_time.max()}')\n",
    "    print(f'Data label true: {df.label.value_counts() / len(df)}')\n",
    "    df = df[SCALED_COLS + GENERATED_COLS + ENCODED_COLS + ['timestamp_raw', 'label', 'total_events']]\n",
    "    df[SCALED_COLS + GENERATED_COLS] = scaler.fit_transform(df[SCALED_COLS + GENERATED_COLS].values)\n",
    "    return df.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = glob.glob(f'frequency_encoded_data/*.csv')\n",
    "files = sorted(list(files))\n",
    "files = files[:61]\n",
    "df = prepare_for_sessionization(files, MinMaxScaler())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104744"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "846"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df['user_id'].nunique().compute())\n",
    "display(df['project_id'].nunique().compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1da7ff4cdcccf44e7e228c52b231f7d5c5854d5618af555ed3871fd5cba609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
