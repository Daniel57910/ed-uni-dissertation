{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.135 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113 --quiet\n",
    "!python -m pip install boto3 python-dotenv --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time\",\n",
    "    \n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "LOAD_COLS = LABEL + METADATA + OUT_FEATURE_COLUMNS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf as gpu_pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as cpu_np\n",
    "import cupy as gpu_np\n",
    "import pandas as cpu_pd\n",
    "from pprint import pformat\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import os\n",
    "import numpy \n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionizeData:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, df, max_sequence_index, write_path, load_cols, feature_cols, grouper, save_s3=True):\n",
    "        self.df = df\n",
    "        self.max_sequence_index = max_sequence_index + 1\n",
    "        self.min_sequence_index = self.max_sequence_index - 10\n",
    "        self.device = self._device()\n",
    "        self.sequences = cpu_np.arange(self.min_sequence_index, self.max_sequence_index).tolist()\n",
    "        self.seq_container = []\n",
    "        self.torch_sequences = None\n",
    "        self.output_path = write_path\n",
    "        self.save_s3 = save_s3\n",
    "        self.load_columns = load_cols\n",
    "        self.feature_cols = feature_cols\n",
    "        self.grouper = grouper\n",
    "        \n",
    "\n",
    "    def _device(self):\n",
    "        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def _sequence_lazy(self):\n",
    "         return next(self._lazy_load_shifted_index())\n",
    "\n",
    "    def _shifters(self):\n",
    "        for _ in range(self.min_sequence_index, self.max_sequence_index):\n",
    "            self.logger.info(f'Loading sequence: {_} -> {self.max_sequence_index}')\n",
    "            self.seq_container.append(self._sequence_lazy())\n",
    "        if torch.cuda.is_available():\n",
    "            GPUtil.showUtilization()\n",
    "\n",
    "        sequences = torch.cat(self.seq_container, dim=1).half()\n",
    "        return sequences\n",
    "\n",
    "    def generate_sequence(self):\n",
    "\n",
    "        self.logger.info(f'Generating shifted clickstreams from {self.min_sequence_index} -> {self.max_sequence_index}')\n",
    "        sequence = self._shifters()\n",
    "\n",
    "        self.logger.info(f'Shifter shape: {sequence.shape}')\n",
    "        \n",
    "\n",
    "        self.logger.info(f'Loading intial clickstream to {self.device}')\n",
    "\n",
    "        if self.max_sequence_index == 11:\n",
    "            self.logger.info('Initial clickstream writing to disk')\n",
    "            initial_clickstream = self.df[self.load_columns]\n",
    "            initial_clickstream = self.df[self.load_columns].values.astype(gpu_np.float32)\n",
    "            \n",
    "            self.logger.info(f'Initial clickstream shape: {initial_clickstream.shape}')\n",
    "            self._sequence_to_disk(initial_clickstream, 0)\n",
    "\n",
    "        self.logger.info(f'Writing sequence to disk: {self.max_sequence_index - 1}')\n",
    "        self._sequence_to_disk(sequence.cpu().numpy(), self.max_sequence_index - 1)\n",
    "\n",
    "\n",
    "    def _sequence_to_disk(self, partition, sequence_index):\n",
    "        if self.save_s3:\n",
    "            s3_client = boto3.client(\n",
    "                's3',\n",
    "                aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "                aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "            )\n",
    "\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "\n",
    "        partition_path = os.path.join(self.output_path, f'sequence_index_{sequence_index}.npz')\n",
    "        self.logger.info(f'Saving to disk: {partition_path}')\n",
    "        numpy.savez_compressed(partition_path, partition)\n",
    "\n",
    "        if self.save_s3:\n",
    "            self.logger.info(f'Uploading to s3: dissertation-data-dmiller/{partition_path}')\n",
    "            s3_client.upload_file(partition_path, 'dissertation-data-dmiller', partition_path)\n",
    "\n",
    "    def _lazy_load_shifted_index(self):\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        indx = self.sequences.pop(0)\n",
    "        torch_container = []\n",
    "        for col in self.feature_cols:\n",
    "            sequence = self.df.groupby(self.grouper)[col].shift(indx).fillna(0).values.astype(gpu_np.float16)\n",
    "            sequence_tensor = torch.tensor(sequence).to(self.device).half()\n",
    "            torch_container.append(sequence_tensor.unsqueeze(1))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        yield torch.cat(torch_container, dim=1).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "torch.set_printoptions(sci_mode=False, precision=4, linewidth=400)\n",
    "\n",
    "cpu_np.set_printoptions(suppress=True)\n",
    "cpu_np.set_printoptions(precision=4)\n",
    "\n",
    "def get_logger():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def scale_feature_cols(df, scaler, scaler_columns):\n",
    "    df[scaler_columns] = scaler.fit_transform(df[scaler_columns])\n",
    "    return df\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    logger = get_logger()\n",
    "    logger.info('Starting sessionize_users_cpu.py with arguments')\n",
    "    logger.info(pformat(args.__dict__))\n",
    "    \n",
    "    data_read = os.path.join(args.input_path, f'files_used_{args.data_subset}')\n",
    "\n",
    "    logger.info(f'Reading data from {data_read}')\n",
    "    scaler_cols = [\n",
    "        col for col in OUT_FEATURE_COLUMNS if 'sin' not in col and 'cos' not in col\n",
    "    ]\n",
    "\n",
    "    df = gpu_pd.read_parquet(data_read, columns=LOAD_COLS + ['date_time'])\n",
    "    logger.info(f'Data read: {df.shape}')\n",
    "    logger.info('Casting date time and sorting by date time')\n",
    "    df['date_time'] = gpu_pd.to_datetime(df['date_time'])\n",
    "    df = df.sort_values(by=['date_time'])\n",
    "    logger.info('Data read: scaling scaler columns')\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    df = df.to_pandas()\n",
    "    logger.info(f'Shape before features: {df.shape}')\n",
    "    df[scaler_cols] = scaler.fit_transform(df[scaler_cols])\n",
    "    df = gpu_pd.from_pandas(df)\n",
    "    logger.info(f'Shape after features: {df.shape}')\n",
    "\n",
    "    logger.info('Scaling complete: implement sessionize')\n",
    "    \n",
    "    for seq_index in args.seq_list:\n",
    "\n",
    "        sessionize = SessionizeData(\n",
    "            df,\n",
    "            seq_index,\n",
    "            os.path.join(args.output_path, f'files_used_{args.data_subset}'),\n",
    "            LOAD_COLS,\n",
    "            OUT_FEATURE_COLUMNS,\n",
    "            GROUPBY_COLS,\n",
    "            args.save_s3\n",
    "        )\n",
    "    \n",
    "        logger.info(f'Generating sequence for {seq_index}')\n",
    "        sessionize.generate_sequence()\n",
    "    \n",
    "    logger.info(f'Sessionize complete for sequences {args.seq_list}')\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 10:30:47,712 - __main__ - INFO - Starting sessionize_users_cpu.py with arguments\n",
      "2023-05-18 10:30:47,713 - __main__ - INFO - {'data_subset': 30}\n",
      "2023-05-18 10:30:47,713 - __main__ - INFO - Reading data from calculated_features/files_used_30\n"
     ]
    }
   ],
   "source": [
    "class Arguments:\n",
    "    seq_list = [10, 20, 30, 40]\n",
    "    input_path = 'calculated_features'\n",
    "    output_path = 'torch_ready_data'\n",
    "    data_subset = None\n",
    "    save_s3 = True\n",
    "\n",
    "for data_subset in [30]:\n",
    "    args = Arguments()\n",
    "    args.data_subset = data_subset\n",
    "    df = main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1da7ff4cdcccf44e7e228c52b231f7d5c5854d5618af555ed3871fd5cba609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
