{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from pprint import pformat, pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import cudf as pd\n",
    "import pandas as cpu_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv awscli --quiet\n",
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync s3://dissertation-data-dmiller/rl_ready_data_conv/files_used_30/window_1/batched_train rl_ready_data_conv/files_used_30/window_1/batched_train --delete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync s3://dissertation-data-dmiller/labelled_session_count_data labelled_session_count_data --delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(linewidth=200)\n",
    "\n",
    "cpu_pd.set_option('display.max_columns', None)\n",
    "cpu_pd.set_option('display.max_rows', None)\n",
    "cpu_pd.set_option('display.width', 500)\n",
    "cpu_pd.set_option('display.float_format', '{:20,.4f}'.format)\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LOAD_COLUMNS = [\n",
    "    'user_id',\n",
    "    'project_id',\n",
    "    'date_time',\n",
    "    'session_5_count',\n",
    "    'session_count',\n",
    "    'session_terminates',\n",
    "    'country',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_COLS = [\n",
    "    'user_id',\n",
    "    'date_time',\n",
    "    'session_count',\n",
    "    'session_terminates',\n",
    "    'cum_session_event_count',\n",
    "    'cum_session_time_minutes',\n",
    "    'expanding_click_average',\n",
    "    'cum_platform_time_minutes',\n",
    "    'cum_projects',\n",
    "    'rolling_session_time',\n",
    "    'rolling_session_events',\n",
    "    'rolling_session_gap',\n",
    "    'session_event_count',\n",
    "    'session_time_minutes'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "def encode_counts(df, logger):\n",
    "\n",
    "    logger.info('Encoding country counts')\n",
    "    country_count = df['country'].value_counts().reset_index(name='country_count').rename(columns={'index': 'country'})\n",
    " \n",
    "    logger.info('Encoding counts complete: joining users to df')\n",
    "    df = df.merge(country_count, on='country', how='left')\n",
    "    \n",
    "    logger.info(f'Encoding user counts')\n",
    "    user_count = df['user_id'].value_counts().reset_index(name='user_count').rename(columns={'index': 'user_id'})\n",
    "    df = df.merge(user_count, on='user_id', how='left')\n",
    "    \n",
    "    logger.info(f'Encoding project counts')\n",
    "    project_count = df['project_id'].value_counts().reset_index(name='project_count').rename(columns={'index': 'project_id'})\n",
    "    df = df.merge(project_count, on='project_id', how='left')\n",
    "    \n",
    "    return df\n",
    "   \n",
    "def time_encodings(df):\n",
    "    \"\"\"\n",
    "    Timestamp raw encoded in units of seconds\n",
    "    \"\"\"\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    df['date_hour'] = df['date_time'].dt.hour \n",
    "    df['date_minute'] = df['date_time'].dt.minute\n",
    "    \n",
    "    df['date_hour_sin'] = np.sin(2 * np.pi * df['date_hour'] / 24)\n",
    "    df['date_hour_cos'] = np.cos(2 * np.pi * df['date_hour'] / 24)\n",
    "    \n",
    "    df['date_minute_sin'] = np.sin(2 * np.pi * df['date_minute'] / 60)\n",
    "    df['date_minute_cos'] = np.cos(2 * np.pi * df['date_minute'] / 60)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def rolling_time_between_events_session(df, logger):\n",
    "    logger.info('Calculating expanding session time averages')\n",
    "    df = df.reset_index()\n",
    "    df['row_count'] = df.index.values\n",
    "    expanding_window = df.set_index('row_count') \\\n",
    "        .sort_values(by=['date_time']) \\\n",
    "        .groupby(['user_id', 'session_count']) \\\n",
    "        .rolling(10, min_periods=1)['delta_last_event'].mean() \\\n",
    "        .reset_index().rename(columns={'delta_last_event': 'expanding_click_average'}) \\\n",
    "        .sort_values(by='row_count')\n",
    "    \n",
    "    logger.info('Expanding averages calculated: joining to df')\n",
    "    df = df.set_index('row_count').join(expanding_window[['row_count', 'expanding_click_average']].set_index('row_count'))\n",
    "    logger.info('Expanding averages joined to df')\n",
    "    df = df.sort_values(by='date_time')\n",
    "    return df\n",
    "\n",
    "\n",
    "def cum_time_in_session(df):\n",
    "    print('Calculating cum session time')\n",
    "    df['delta_last_event'] = df.groupby(['user_id'])['date_time'].diff()\n",
    "    df = df.to_pandas()\n",
    "    df['delta_last_event'] = df['delta_last_event'].dt.total_seconds()\n",
    "    df['delta_last_event'] = df['delta_last_event'].fillna(0)\n",
    "    print('Cum session time calculated: applying zeroing of first event')\n",
    "    df = pd.from_pandas(df)\n",
    "    df['delta_last_event'] = df[['cum_session_event_count', 'delta_last_event']].apply(lambda x: 0 if x['cum_session_event_count'] == 1 else x['delta_last_event'], axis=1)\n",
    "    df['cum_session_time'] = df.groupby(['user_id', 'session_count'])['delta_last_event'].cumsum()\n",
    "    df['cum_session_time'] = df['cum_session_time'] / 60\n",
    "    print('Zero of first event applied: cum session time calculated')\n",
    "    return df\n",
    "\n",
    "def intra_session_stats(df, logger):\n",
    "    \n",
    "    logger.info('Sorting by date_time and user_id')\n",
    "    df = df.sort_values(by=['date_time', 'user_id'])\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['user_id', 'date_time'], keep='first')\n",
    "    logger.info('Calculating cum_event_count')\n",
    "    df['cum_session_event_count'] = df.groupby(['user_id', 'session_count'])['date_time'].cumcount() + 1\n",
    "    logger.info('Cum_event_count calculated: calculating cum session time (minutes)')\n",
    "    df = cum_time_in_session(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def cum_platform_time(df):\n",
    "    df['cum_platform_time'] = df.groupby(['user_id'])['cum_session_time'].cumsum()\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def running_user_stats(df, logger):\n",
    "    logger.info('Calculating cumulative platform time')\n",
    "    df = df.sort_values(by=['date_time'])\n",
    "    df = cum_platform_time(df)\n",
    "    logger.info('Calculating cumulative platform events')\n",
    "    df['cum_platform_events'] = df.groupby(['user_id']).cumcount() + 1\n",
    "    logger.info('Calculating running unique projects: shifting projects to find unique')\n",
    "    \n",
    "    df['project_id'] = df['project_id'].astype(int)\n",
    "    df['user_id'] = df['user_id'].astype(int)\n",
    "    df['previous_user_project'] = df.groupby('user_id')['project_id'].shift(1)\n",
    "    df['previous_project_exists'] = df['previous_user_project'].notna()\n",
    "    \n",
    "    df['previous_user_project'] = df[['previous_user_project', 'previous_project_exists', 'project_id']].apply(\n",
    "        lambda x: x['previous_user_project'] if x['previous_project_exists'] else x['project_id'], axis=1)\n",
    "    logger.info('Calculating running unique projects: calculating unique projects')\n",
    "    \n",
    "    df['previous_user_project'] = df['previous_user_project'].astype(int)\n",
    "    df['project_change'] = df['project_id'] != df['previous_user_project']\n",
    "    \n",
    "    df['cum_projects'] = df.groupby('user_id')['project_change'].cumsum() + 1\n",
    "    \n",
    "   \n",
    "    df = df.drop(columns=['previous_user_project', 'previous_project_exists', 'project_change'])\n",
    "    logger.info('Calculated running unique projects: calculating average event time delta')\n",
    "    df = df.reset_index()\n",
    "    df['row_count'] = df.index.values\n",
    "    \n",
    "    average_event_time = df.set_index('row_count') \\\n",
    "        .sort_values(by=['date_time']) \\\n",
    "        .groupby('user_id') \\\n",
    "        .rolling(1000, min_periods=1)['delta_last_event'].mean() \\\n",
    "        .reset_index().rename(columns={'delta_last_event': 'average_event_time'}) \\\n",
    "        .sort_values(by='row_count')\n",
    "    df = df.set_index('row_count').join(average_event_time[['row_count', 'average_event_time']].set_index('row_count'))\n",
    "    logger.info('Calculated average event time delta')\n",
    "    return df\n",
    "\n",
    "\n",
    "def time_from_previous_session_minutes(session_inflection_times, logger):\n",
    "    session_inflection_times = session_inflection_times.sort_values(by=['session_count', 'user_id'])\n",
    "    \n",
    "    session_inflection_times['previous_session_end'] = session_inflection_times.groupby(['user_id'])['date_time_max'].shift(1)\n",
    "    session_inflection_times['time_between_session_minutes'] = (session_inflection_times['date_time_min'] - session_inflection_times['previous_session_end']).dt.total_seconds() / 60\n",
    "    session_inflection_times['time_between_session_minutes'] = session_inflection_times['time_between_session_minutes'].fillna(0)\n",
    "    return session_inflection_times[['user_id', 'session_count', 'time_between_session_minutes', 'date_time_min', 'date_time_max']]\n",
    "\n",
    "def rolling_average_session_statistics(df, session_inflection_times, logger):\n",
    " \n",
    "    logger.info('Session inflection times calculated: calculating expanding session time')\n",
    "    average_session_minutes = session_inflection_times.sort_values(by=['session_count', 'user_id']) \\\n",
    "    .set_index(['session_count', 'date_time_min', 'date_time_max']) \\\n",
    "    .groupby(['user_id']) \\\n",
    "    ['session_time_minutes'] \\\n",
    "    .rolling(10, min_periods=1, closed='left') \\\n",
    "    .mean() \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'session_time_minutes': 'rolling_session_time'})\n",
    "   \n",
    "    average_session_minutes['rolling_session_time'] = average_session_minutes['rolling_session_time'].fillna(0)\n",
    "    logger.info('Calculating average events per session')\n",
    "    average_events_session = session_inflection_times.sort_values(by=['session_count', 'user_id']) \\\n",
    "        .set_index(['session_count', 'date_time_min', 'date_time_max']) \\\n",
    "        .groupby(['user_id']) \\\n",
    "        ['session_event_count'] \\\n",
    "        .rolling(10, min_periods=1, closed='left') \\\n",
    "        .mean() \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'session_event_count': 'rolling_session_events'})\n",
    "    \n",
    "    average_events_session['rolling_session_events'] = average_events_session['rolling_session_events'].fillna(0)\n",
    "    \n",
    "    logger.info('Calculating time from previous session')\n",
    "    time_between_session = time_from_previous_session_minutes(session_inflection_times, logger)\n",
    "    \n",
    "    time_between_session = time_between_session.sort_values(by=['session_count', 'user_id']) \\\n",
    "        .set_index(['session_count']) \\\n",
    "        .groupby(['user_id']) \\\n",
    "        ['time_between_session_minutes'] \\\n",
    "        .rolling(10, min_periods=1) \\\n",
    "        .mean() \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'time_between_session_minutes': 'rolling_session_gap'})\n",
    "\n",
    "    logger.info('Joining dataframes')\n",
    "   \n",
    "    session_stats = cpu_pd.merge(average_events_session, average_session_minutes, on=['user_id', 'session_count']) \n",
    "    session_stats = cpu_pd.merge(session_stats, session_inflection_times, on=['user_id', 'session_count'])\n",
    "    session_stats = cpu_pd.merge(session_stats, time_between_session, on=['user_id', 'session_count'])\n",
    "    session_stats = pd.from_pandas(session_stats)\n",
    "    \n",
    "    df = pd.merge(df, session_stats[['user_id', 'session_count', 'rolling_session_time', 'rolling_session_events', 'rolling_session_gap', 'session_event_count', 'session_time_minutes', 'rolling_session_gap']], on=['user_id', 'session_count'])\n",
    "    logger.info('Dataframes joined::returning')\n",
    "    return df\n",
    "\n",
    "\n",
    "def assign_metadata(df, logger):\n",
    "    logger.info(f'Obtaining global session time and user events')\n",
    "    global_session_time = df.groupby('user_id')['cum_platform_time'].max().reset_index().rename(columns={'cum_platform_time': 'global_session_time'})\n",
    "    \n",
    "    logger.info('Joining session_time to df')\n",
    "    df = pd.merge(df, global_session_time, on='user_id', how='left')\n",
    "\n",
    "    \n",
    "    logger.info('Calculating time meta [year, month, day, hour, minute, second]')\n",
    "\n",
    "    df['year'] = df['date_time'].dt.year\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['minute'] = df['date_time'].dt.minute\n",
    "    df['second'] = df['date_time'].dt.second\n",
    "    \n",
    "    logger.info('Assigning raw session time and counts, platform time and counts and session count')\n",
    "    df['cum_session_time_raw'] = df['cum_session_time']\n",
    "    df['cum_platform_time_raw'] = df['cum_platform_time']\n",
    "    df['cum_session_event_raw'] = df['cum_session_event_count']\n",
    "    df['cum_platform_event_raw'] = df['cum_platform_events']\n",
    "    df['session_count_raw'] = df['session_count']\n",
    "    df['user_count_raw'] = df['user_count']\n",
    "    df['project_count_raw'] = df['project_count']\n",
    "    \n",
    "    return df\n",
    "    \n",
    "   \n",
    "def hash_user_id(df):\n",
    "    user_id = df[['user_id']].drop_duplicates().reset_index().rename(columns={'index': 'user_id_hash'})\n",
    "    df = pd.merge(df, user_id, on='user_id')\n",
    "    df = df.drop(columns=['user_id'])\n",
    "    df = df.rename(columns={'user_id_hash': 'user_id'})\n",
    "    return df\n",
    "\n",
    "def hash_project_id(df):\n",
    "    project_id = df[['project_id']].drop_duplicates().reset_index().rename(columns={'index': 'project_id_hash'})\n",
    "    df = pd.merge(df, project_id, on='project_id')\n",
    "    df = df.drop(columns=['project_id'])\n",
    "    df = df.rename(columns={'project_id_hash': 'project_id'})\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_summary_session_stats(df, logger):\n",
    "    logger.info('Generating session statistics')\n",
    "    session_inflection_statistics = df.groupby(['user_id', 'session_count']).agg({'date_time': ['min', 'max', 'count']}).reset_index()\n",
    "    session_inflection_statistics.columns = ['user_id', 'session_count', 'date_time_min', 'date_time_max', 'session_event_count']\n",
    "    session_inflection_statistics = session_inflection_statistics.to_pandas()\n",
    "    session_inflection_statistics['session_time_minutes'] = (session_inflection_statistics['date_time_max'] - session_inflection_statistics['date_time_min']).dt.total_seconds() / 60\n",
    "    return session_inflection_statistics\n",
    "\n",
    "def calculate_immediate_session_stats(df, session_inflections, logger):\n",
    "    \n",
    "    session_inflections = pd.from_pandas(session_inflections)\n",
    "    session_inflections = session_inflections.sort_values(by=['session_count', 'user_id'])\n",
    "    logger.info(f'Calculating immediate previous session time')\n",
    "    session_inflections['previous_session_time'] = session_inflections.groupby(['user_id'])['session_time_minutes'].shift(1)\n",
    "    \n",
    "    logger.info(f'Calculate immmediate previous session events')\n",
    "    session_inflections['previous_session_events'] = session_inflections.groupby(['user_id'])['session_event_count'].shift(1)\n",
    "    session_inflections = session_inflections.fillna(0)    \n",
    "    logger.info(f'Joining immediate session stats to df')\n",
    "    df = pd.merge(df, session_inflections[['user_id', 'session_count', 'previous_session_time', 'previous_session_events']], on=['user_id', 'session_count'])\n",
    "    \n",
    "    return df\n",
    "   \n",
    "def _pretty_print_columns(df):\n",
    "    for col in df.columns:\n",
    "        print(f'    \"{col}\"')\n",
    "def main(args):\n",
    "    #\n",
    "\n",
    "    np.set_printoptions(suppress=True)\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    logger =  get_logger()\n",
    "    logger.info(f'Running feature calculation with args')\n",
    "    logger.info(pformat(args.__dict__))\n",
    "    \n",
    "    input_path = os.path.join(\n",
    "        args.input_path,\n",
    "        f'files_used_{args.n_files}_window_{args.data_window}.parquet'\n",
    "    )\n",
    "\n",
    "\n",
    "    output_path = os.path.join(\n",
    "        args.output_path,\n",
    "        f'files_used_{args.n_files}',\n",
    "        f'calculated_features_window_{args.data_window}.parquet')\n",
    "    \n",
    "    logger.info(f'Input path: {input_path}')\n",
    "    logger.info(f'Output path: {output_path}')\n",
    "        \n",
    "    \n",
    "    logger.info(f'Loading data from input_path')\n",
    "    df = pd.read_parquet(input_path, columns=INITIAL_LOAD_COLUMNS)\n",
    "    df = hash_user_id(df)\n",
    "    logger.info(f'User id hashed: {df.shape}')\n",
    "    df = hash_project_id(df)\n",
    "    logger.info(f'Project id hashed: {df.shape}')\n",
    "    \n",
    "    logger.info(f'Loaded data: shape = {df.shape}, min_date, max_date: {df.date_time.min()}, {df.date_time.max()}')\n",
    "    label_count = df[df['session_terminates'] == True].shape[0] / df.shape[0]\n",
    "    logger.info(f'Perc ending in {args.data_window}: {label_count}')\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    logger.info(f'Sorting data by date_time')\n",
    "    df = df.sort_values(by='date_time')\n",
    "    logger.info('Finished sorting data: encoding value counts for user project and country')\n",
    "    df = encode_counts(df, logger)\n",
    "    \n",
    "    logger.info(f'Finished encoding value counts: encoding time features: {df.shape}')\n",
    "    df = time_encodings(df) \n",
    "   \n",
    "    logger.info('Time encodings complete: encoding categorical features')\n",
    "    \n",
    "    logger.info('Categorical features encoded: calculating intra-session stats')\n",
    "    df = intra_session_stats(df, logger)\n",
    "    \n",
    "    logger.info(f'Intra session stats calculated: calculating running user stats')\n",
    "    df = running_user_stats(df, logger)\n",
    "    logger.info('Beginning rolling window 10 calculation')\n",
    "    \n",
    "    df = rolling_time_between_events_session(df, logger)\n",
    "    logger.info('Rolling window 10 calculation complete: beginning expanding window calculation')\n",
    "    \n",
    "   \n",
    "    session_inflection_times = generate_summary_session_stats(df, logger)\n",
    "    logger.info('Session inflection times calculated: columns')\n",
    "    logger.info(pformat(session_inflection_times.columns))\n",
    "    df = rolling_average_session_statistics(df, session_inflection_times, logger)\n",
    "    df = calculate_immediate_session_stats(df, session_inflection_times, logger)\n",
    "    logger.info('Time within session and average session clicks calculated:: calculating time between session')\n",
    "    df['session_raw'] = df['session_count']\n",
    "\n",
    "    logger.info('Assigning metadata')\n",
    "    df = assign_metadata(df, logger)\n",
    "    logger.info('Metadata assigned: dropping columns')\n",
    "    \n",
    "    logger.info('Returning df to dask for writing to disk')\n",
    "       \n",
    "    if not os.path.exists(os.path.dirname(output_path)):\n",
    "        os.makedirs(os.path.dirname(output_path))\n",
    "    \n",
    "    \n",
    "    logger.info(f'Final out columns:')\n",
    "    _pretty_print_columns(df)\n",
    "\n",
    "    df = df.sort_values(by='date_time').reset_index(drop=True)\n",
    "    df['label'] = df['session_terminates'].apply(lambda x: x == False)\n",
    "    df = df.to_pandas()\n",
    "    logger.info(f'Brought back to CPU for writing to disk')\n",
    "    logger.info(f'Writing to: {output_path}')\n",
    "    df.to_parquet(output_path)\n",
    "\n",
    "    logger.info('Finished writing to disk')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_is_monotonic_increasing_glob(df):\n",
    "    df = df.to_pandas()\n",
    "    df = df.sort_values(by=['user_id', 'date_time'])\n",
    "    monotonic_date_time = df.groupby(['user_id'])['date_time'].is_monotonic_increasing.reset_index(name='is_monotonic_increasing')\n",
    "    if monotonic_date_time['is_monotonic_increasing'].all() == True:\n",
    "        print('monotonic date time')\n",
    "    else:\n",
    "        print(monotonic_date_time[monotonic_date_time['is_monotonic_increasing'] == False])\n",
    "        return\n",
    "    monotonic_plat_event = df.groupby(['user_id'])['cum_platform_event_raw'].is_monotonic_increasing.reset_index(name='is_monotonic_increasing')\n",
    "    if monotonic_plat_event['is_monotonic_increasing'].all() == True:\n",
    "        print('monotonic plat event')\n",
    "    else:\n",
    "        print(monotonic_plat_event[monotonic_plat_event['is_monotonic_increasing'] == False])\n",
    "        return\n",
    "\n",
    "\n",
    "    df['cum_platform_time_raw'] = df['cum_platform_time_raw'].round(2)\n",
    "\n",
    "    monotonic_plat_time = df.groupby(['user_id'])['cum_platform_time_raw'].is_monotonic_increasing.reset_index(name='is_monotonic_increasing')\n",
    "    if monotonic_plat_time['is_monotonic_increasing'].all() == True:\n",
    "        print('monotonic plat time')\n",
    "    else:\n",
    "        print(monotonic_plat_time[monotonic_plat_time['is_monotonic_increasing'] == False])\n",
    "        return\n",
    "\n",
    "\n",
    "    \n",
    "    monotonic_cum_projects = df.groupby(['user_id'])['cum_projects'].is_monotonic_increasing.reset_index(name='is_monotonic_increasing')\n",
    "    \n",
    "    if monotonic_cum_projects['is_monotonic_increasing'].all() == True:\n",
    "        print('monotonic plat proj')\n",
    "    else:\n",
    "        print(monotonic_cum_projects[monotonic_cum_projects['is_monotonic_increasing'] == False])\n",
    "        return\n",
    "\n",
    "def test_is_monotonic_increasing_session(df):\n",
    "    df = df.to_pandas()\n",
    "    df = df.sort_values(by=['user_id', 'date_time'])\n",
    "    delta_last_check = df.groupby(['user_id', 'session_30_count_raw'])['delta_last_event'].max().reset_index(name='delta_last_event_max')\n",
    "\n",
    "\n",
    "    monotonic_session_event = df.groupby(['user_id', 'session_30_count_raw'])['cum_session_event_raw'].is_monotonic_increasing.reset_index(name='is_monotonic_increasing')\n",
    "    monotonic_session_time = df.groupby(['user_id', 'session_30_count_raw'])['cum_session_time_raw'].is_monotonic_increasing.reset_index(name='is_monotonic_increasing')\n",
    "    \n",
    "    for df_grouper, msg in zip([monotonic_session_event, monotonic_session_time], ['event', 'time']):\n",
    "        if df_grouper['is_monotonic_increasing'].all() == True:\n",
    "            print(f'monotonic session {msg}')\n",
    "        else:\n",
    "            print(df_grouper[df_grouper['is_monotonic_increasing'] == False])\n",
    "            return\n",
    "    \n",
    "    if delta_last_check['delta_last_event_max'].max() <= 30:\n",
    "        print('session time diff max less than 30')\n",
    "    else:\n",
    "        print(delta_last_check[delta_last_check['delta_last_event_max'] > 30])\n",
    "    \n",
    "\n",
    "def test_df(df):\n",
    "    print(f'Testing monotonic increasing')\n",
    "    # test_is_monotonic_increasing_glob(df)\n",
    "    print('Monotonic increasing global passed')\n",
    "    test_is_monotonic_increasing_session(df)\n",
    "    print('Monotonic increasing local passed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    input_path = 'labelled_session_count_data_2'\n",
    "    output_path = 'calculated_features'\n",
    "    n_files = 30\n",
    "    data_window = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 10:26:20,173 - __main__ - INFO - Running feature calculation with args\n",
      "2023-07-10 10:26:20,174 - __main__ - INFO - mappingproxy({'__dict__': <attribute '__dict__' of 'Args' objects>,\n",
      "              '__doc__': None,\n",
      "              '__module__': '__main__',\n",
      "              '__weakref__': <attribute '__weakref__' of 'Args' objects>,\n",
      "              'data_window': 10,\n",
      "              'input_path': 'labelled_session_count_data_2',\n",
      "              'n_files': 30,\n",
      "              'output_path': 'calculated_features'})\n",
      "2023-07-10 10:26:20,175 - __main__ - INFO - Input path: labelled_session_count_data_2/files_used_30_window_10.parquet\n",
      "2023-07-10 10:26:20,175 - __main__ - INFO - Output path: calculated_features/files_used_30/calculated_features_window_10.parquet\n",
      "2023-07-10 10:26:20,176 - __main__ - INFO - Loading data from input_path\n",
      "2023-07-10 10:26:21,452 - __main__ - INFO - User id hashed: (38500990, 7)\n",
      "2023-07-10 10:26:21,616 - __main__ - INFO - Project id hashed: (38500990, 7)\n",
      "2023-07-10 10:26:21,618 - __main__ - INFO - Loaded data: shape = (38500990, 7), min_date, max_date: 2021-10-19T08:40:37.000000000, 2022-08-14T05:13:27.000000000\n",
      "2023-07-10 10:26:21,649 - __main__ - INFO - Perc ending in 10: 0.4040091176876231\n",
      "2023-07-10 10:26:21,650 - __main__ - INFO - Sorting data by date_time\n",
      "2023-07-10 10:26:21,774 - __main__ - INFO - Finished sorting data: encoding value counts for user project and country\n",
      "2023-07-10 10:26:21,775 - __main__ - INFO - Encoding country counts\n",
      "2023-07-10 10:26:21,803 - __main__ - INFO - Encoding counts complete: joining users to df\n",
      "2023-07-10 10:26:21,852 - __main__ - INFO - Encoding user counts\n",
      "2023-07-10 10:26:21,921 - __main__ - INFO - Encoding project counts\n",
      "2023-07-10 10:26:22,020 - __main__ - INFO - Finished encoding value counts: encoding time features: (38500990, 10)\n",
      "2023-07-10 10:26:22,074 - __main__ - INFO - Time encodings complete: encoding categorical features\n",
      "2023-07-10 10:26:22,075 - __main__ - INFO - Categorical features encoded: calculating intra-session stats\n",
      "2023-07-10 10:26:22,075 - __main__ - INFO - Sorting by date_time and user_id\n",
      "2023-07-10 10:26:22,323 - __main__ - INFO - Calculating cum_event_count\n",
      "2023-07-10 10:26:22,597 - __main__ - INFO - Cum_event_count calculated: calculating cum session time (minutes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating cum session time\n",
      "Cum session time calculated: applying zeroing of first event\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 10:26:30,331 - __main__ - INFO - Intra session stats calculated: calculating running user stats\n",
      "2023-07-10 10:26:30,332 - __main__ - INFO - Calculating cumulative platform time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero of first event applied: cum session time calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 10:26:30,558 - __main__ - INFO - Calculating cumulative platform events\n",
      "2023-07-10 10:26:30,650 - __main__ - INFO - Calculating running unique projects: shifting projects to find unique\n",
      "2023-07-10 10:26:30,761 - __main__ - INFO - Calculating running unique projects: calculating unique projects\n",
      "2023-07-10 10:26:30,936 - __main__ - INFO - Calculated running unique projects: calculating average event time delta\n",
      "2023-07-10 10:26:31,837 - __main__ - INFO - Calculated average event time delta\n",
      "2023-07-10 10:26:31,879 - __main__ - INFO - Beginning rolling window 10 calculation\n",
      "2023-07-10 10:26:31,880 - __main__ - INFO - Calculating expanding session time averages\n",
      "2023-07-10 10:26:32,601 - __main__ - INFO - Expanding averages calculated: joining to df\n",
      "2023-07-10 10:26:32,857 - __main__ - INFO - Expanding averages joined to df\n",
      "2023-07-10 10:26:33,093 - __main__ - INFO - Rolling window 10 calculation complete: beginning expanding window calculation\n",
      "2023-07-10 10:26:33,094 - __main__ - INFO - Generating session statistics\n",
      "2023-07-10 10:26:33,184 - __main__ - INFO - Session inflection times calculated: columns\n",
      "2023-07-10 10:26:33,185 - __main__ - INFO - Index(['user_id', 'session_count', 'date_time_min', 'date_time_max', 'session_event_count', 'session_time_minutes'], dtype='object')\n",
      "2023-07-10 10:26:33,185 - __main__ - INFO - Session inflection times calculated: calculating expanding session time\n",
      "2023-07-10 10:26:37,477 - __main__ - INFO - Calculating average events per session\n",
      "2023-07-10 10:26:41,641 - __main__ - INFO - Calculating time from previous session\n",
      "2023-07-10 10:26:45,136 - __main__ - INFO - Joining dataframes\n",
      "2023-07-10 10:26:46,046 - __main__ - INFO - Dataframes joined::returning\n",
      "2023-07-10 10:26:46,152 - __main__ - INFO - Calculating immediate previous session time\n",
      "2023-07-10 10:26:46,190 - __main__ - INFO - Calculate immmediate previous session events\n",
      "2023-07-10 10:26:46,219 - __main__ - INFO - Joining immediate session stats to df\n",
      "2023-07-10 10:26:46,359 - __main__ - INFO - Time within session and average session clicks calculated:: calculating time between session\n",
      "2023-07-10 10:26:46,360 - __main__ - INFO - Assigning metadata\n",
      "2023-07-10 10:26:46,360 - __main__ - INFO - Obtaining global session time and user events\n",
      "2023-07-10 10:26:46,387 - __main__ - INFO - Joining session_time to df\n",
      "2023-07-10 10:26:46,521 - __main__ - INFO - Calculating time meta [year, month, day, hour, minute, second]\n",
      "2023-07-10 10:26:46,534 - __main__ - INFO - Assigning raw session time and counts, platform time and counts and session count\n",
      "2023-07-10 10:26:46,579 - __main__ - INFO - Metadata assigned: dropping columns\n",
      "2023-07-10 10:26:46,580 - __main__ - INFO - Returning df to dask for writing to disk\n",
      "2023-07-10 10:26:46,581 - __main__ - INFO - Final out columns:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \"index\"\n",
      "    \"date_time\"\n",
      "    \"session_5_count\"\n",
      "    \"session_count\"\n",
      "    \"session_terminates\"\n",
      "    \"country\"\n",
      "    \"user_id\"\n",
      "    \"project_id\"\n",
      "    \"country_count\"\n",
      "    \"user_count\"\n",
      "    \"project_count\"\n",
      "    \"date_hour\"\n",
      "    \"date_minute\"\n",
      "    \"date_hour_sin\"\n",
      "    \"date_hour_cos\"\n",
      "    \"date_minute_sin\"\n",
      "    \"date_minute_cos\"\n",
      "    \"cum_session_event_count\"\n",
      "    \"delta_last_event\"\n",
      "    \"cum_session_time\"\n",
      "    \"cum_platform_time\"\n",
      "    \"cum_platform_events\"\n",
      "    \"cum_projects\"\n",
      "    \"average_event_time\"\n",
      "    \"expanding_click_average\"\n",
      "    \"rolling_session_time\"\n",
      "    \"rolling_session_events\"\n",
      "    \"rolling_session_gap\"\n",
      "    \"session_event_count\"\n",
      "    \"session_time_minutes\"\n",
      "    \"previous_session_time\"\n",
      "    \"previous_session_events\"\n",
      "    \"session_raw\"\n",
      "    \"global_session_time\"\n",
      "    \"year\"\n",
      "    \"month\"\n",
      "    \"day\"\n",
      "    \"hour\"\n",
      "    \"minute\"\n",
      "    \"second\"\n",
      "    \"cum_session_time_raw\"\n",
      "    \"cum_platform_time_raw\"\n",
      "    \"cum_session_event_raw\"\n",
      "    \"cum_platform_event_raw\"\n",
      "    \"session_count_raw\"\n",
      "    \"user_count_raw\"\n",
      "    \"project_count_raw\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 10:27:09,144 - __main__ - INFO - Brought back to CPU for writing to disk\n",
      "2023-07-10 10:27:09,145 - __main__ - INFO - Writing to: calculated_features/files_used_30/calculated_features_window_10.parquet\n",
      "2023-07-10 10:27:40,676 - __main__ - INFO - Finished writing to disk\n"
     ]
    }
   ],
   "source": [
    "main(Args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1da7ff4cdcccf44e7e228c52b231f7d5c5854d5618af555ed3871fd5cba609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
