{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly as px\n",
    "import plotly.express as px\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_AUC_PATH = 'auroc_eval_data'\n",
    "CORE_CSV_PATH = 'result_csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_glob_auc(read_path, experiment):\n",
    "    print(f'Loading data')\n",
    "    df = pd.read_parquet(read_path)\n",
    "    df['row_count'] = df.index.values\n",
    "    eval_cutoff = df.shape[0] * .5\n",
    "    df['subset'] = df['row_count'].apply(lambda x: 'eval' if x < eval_cutoff else 'test')\n",
    "    df = df.drop(columns=['row_count'])\n",
    "\n",
    "    eval_df = df[df['subset'] == 'eval']\n",
    "    test_df = df[df['subset'] == 'test']\n",
    "    auc_container = []\n",
    "    for col in [col for col in eval_df.columns if 'LSTM' in col]:\n",
    "        print(f'Calculating AUROC for {col}')\n",
    "        auc_container.append(\n",
    "            {\n",
    "                'Model': col,\n",
    "                'AUROC': roc_auc_score(eval_df['label'], eval_df[col]),\n",
    "                'Acc': accuracy_score(eval_df['label'], eval_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Prec': precision_score(eval_df['label'], eval_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Rec': recall_score(eval_df['label'], eval_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Subset': 'eval'\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        auc_container.append(\n",
    "            {\n",
    "                'Model': col,\n",
    "                'AUROC': roc_auc_score(test_df['label'], test_df[col]),\n",
    "                'Acc': accuracy_score(test_df['label'], test_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Prec': precision_score(test_df['label'], test_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Rec': recall_score(test_df['label'], test_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Subset': 'test'\n",
    "            }\n",
    "        )\n",
    "\n",
    "    out_df = pd.DataFrame(auc_container)\n",
    "    out_df['Experiment'] = experiment\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def plot_subset(df, target_var):\n",
    "    eval_exp, test_exp = df[df['Subset'] == 'eval'], df[df['Subset'] == 'test']\n",
    "    \n",
    "    eval_fig = px.line(eval_exp, x='Window', y=target_var, color='Experiment', markers=True)\n",
    "    test_fig = px.line(test_exp, x='Window', y=target_var, color='Experiment', markers=True)\n",
    "\n",
    "    eval_fig.update_layout(\n",
    "        xaxis_title='Data Window Size',\n",
    "        yaxis_title=target_var,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "   \n",
    "    test_fig.update_layout(\n",
    "        xaxis_title='Data Window Size',\n",
    "        yaxis_title=target_var,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "     \n",
    "    eval_fig.write_image(f'lstm_plots/metric_graphs/{target_var}_eval.png')\n",
    "    test_fig.write_image(f'lstm_plots/metric_graphs/{target_var}_test.png')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('result_csv/result_summary_eval_test.csv')\n",
    "df['Window'] = df['Model'].apply(lambda x: 30 if 'H' in x else int(x.split(' ')[-1]))\n",
    "df = df[df['Model'] != 'LSTM SEQ 30 H']\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'Acc': 'Accuracy',\n",
    "    'Prec': 'Precision',\n",
    "    'Rec': 'Recall'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric Value</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.669</td>\n",
       "      <td>10</td>\n",
       "      <td>ACC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.861</td>\n",
       "      <td>10</td>\n",
       "      <td>PREC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.683</td>\n",
       "      <td>10</td>\n",
       "      <td>REC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.653</td>\n",
       "      <td>10</td>\n",
       "      <td>AUC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.676</td>\n",
       "      <td>10</td>\n",
       "      <td>ACC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.876</td>\n",
       "      <td>10</td>\n",
       "      <td>PREC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.690</td>\n",
       "      <td>10</td>\n",
       "      <td>REC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.656</td>\n",
       "      <td>10</td>\n",
       "      <td>AUC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.669</td>\n",
       "      <td>20</td>\n",
       "      <td>ACC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.641</td>\n",
       "      <td>20</td>\n",
       "      <td>PREC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.689</td>\n",
       "      <td>20</td>\n",
       "      <td>REC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.670</td>\n",
       "      <td>20</td>\n",
       "      <td>AUC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.672</td>\n",
       "      <td>20</td>\n",
       "      <td>ACC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.670</td>\n",
       "      <td>20</td>\n",
       "      <td>PREC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.699</td>\n",
       "      <td>20</td>\n",
       "      <td>REC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.672</td>\n",
       "      <td>20</td>\n",
       "      <td>AUC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.698</td>\n",
       "      <td>30</td>\n",
       "      <td>ACC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.802</td>\n",
       "      <td>30</td>\n",
       "      <td>PREC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.698</td>\n",
       "      <td>30</td>\n",
       "      <td>REC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.698</td>\n",
       "      <td>30</td>\n",
       "      <td>AUC</td>\n",
       "      <td>eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.699</td>\n",
       "      <td>30</td>\n",
       "      <td>ACC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.792</td>\n",
       "      <td>30</td>\n",
       "      <td>PREC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.693</td>\n",
       "      <td>30</td>\n",
       "      <td>REC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.702</td>\n",
       "      <td>30</td>\n",
       "      <td>AUC</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Metric Value  Experiment Metric Subset\n",
       "0          0.669          10    ACC   eval\n",
       "1          0.861          10   PREC   eval\n",
       "2          0.683          10    REC   eval\n",
       "3          0.653          10    AUC   eval\n",
       "4          0.676          10    ACC   test\n",
       "5          0.876          10   PREC   test\n",
       "6          0.690          10    REC   test\n",
       "7          0.656          10    AUC   test\n",
       "8          0.669          20    ACC   eval\n",
       "9          0.641          20   PREC   eval\n",
       "10         0.689          20    REC   eval\n",
       "11         0.670          20    AUC   eval\n",
       "12         0.672          20    ACC   test\n",
       "13         0.670          20   PREC   test\n",
       "14         0.699          20    REC   test\n",
       "15         0.672          20    AUC   test\n",
       "16         0.698          30    ACC   eval\n",
       "17         0.802          30   PREC   eval\n",
       "18         0.698          30    REC   eval\n",
       "19         0.698          30    AUC   eval\n",
       "20         0.699          30    ACC   test\n",
       "21         0.792          30   PREC   test\n",
       "22         0.693          30    REC   test\n",
       "23         0.702          30    AUC   test"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_df = pd.read_csv('result_csv/baseline_results_rand_f.csv')\n",
    "baseline_df['Experiment'] = baseline_df['Metric Name'].apply(lambda x: int(re.sub('[^0-9]', '', x)))\n",
    "baseline_df['Metric'] = baseline_df['Metric Name'].apply(lambda x: x.split(' ')[0])\n",
    "# baseline_df['Window'] = 1\n",
    "baseline_df['Subset'] = baseline_df['Metric Name'].apply(lambda x: 'eval' if 'EVAL' in x else 'test')\n",
    "# baseline_df = baseline_df.pivot(index='Experiment', columns='Subset', values='Metric Value').reset_index()\n",
    "baseline_df = baseline_df.drop(columns=['Metric Name'])\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df_pivoted = baseline_df.pivot(index=['Experiment', 'Subset'], columns='Metric', values='Metric Value').reset_index()\n",
    "baseline_df_pivoted = baseline_df_pivoted.rename(columns={\n",
    "    'ACC': 'Accuracy',\n",
    "    'PREC': 'Precision',\n",
    "    'REC': 'Recall'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df_pivoted['Model'] = 'Random Forest'\n",
    "baseline_df_pivoted['Window'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Experiment', 'Subset', 'Accuracy', 'AUC', 'Precision', 'Recall',\n",
       "       'Model', 'Window'],\n",
       "      dtype='object', name='Metric')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_appender = df.rename(columns={\n",
    "    'AUROC': 'AUC'\n",
    "})\n",
    "\n",
    "baseline_df_pivoted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Model', 'AUC', 'Accuracy', 'Precision', 'Recall', 'Subset',\n",
       "       'Experiment', 'Window'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_appender.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrlr}\n",
      "\\toprule\n",
      "Model & AUC & Accuracy & Precision & Recall & Subset & Experiment \\\\\n",
      "\\midrule\n",
      "LSTM SEQ 1 & 0.699 & 0.670 & 0.691 & 0.856 & test & 10 \\\\\n",
      "LSTM SEQ 10 & 0.670 & 0.526 & 0.823 & 0.309 & test & 10 \\\\\n",
      "LSTM SEQ 20 & 0.694 & 0.669 & 0.689 & 0.858 & test & 10 \\\\\n",
      "LSTM SEQ 30 & 0.696 & 0.668 & 0.691 & 0.852 & test & 10 \\\\\n",
      "LSTM SEQ 40 & 0.674 & 0.576 & 0.780 & 0.450 & test & 10 \\\\\n",
      "LSTM SEQ 1 & 0.683 & 0.625 & 0.749 & 0.605 & test & 20 \\\\\n",
      "LSTM SEQ 10 & 0.686 & 0.642 & 0.736 & 0.669 & test & 20 \\\\\n",
      "LSTM SEQ 20 & 0.694 & 0.669 & 0.689 & 0.858 & test & 20 \\\\\n",
      "LSTM SEQ 30 & 0.690 & 0.630 & 0.752 & 0.612 & test & 20 \\\\\n",
      "LSTM SEQ 40 & 0.690 & 0.632 & 0.749 & 0.622 & test & 20 \\\\\n",
      "LSTM SEQ 1 & 0.749 & 0.690 & 0.707 & 0.563 & test & 30 \\\\\n",
      "LSTM SEQ 10 & 0.744 & 0.688 & 0.717 & 0.539 & test & 30 \\\\\n",
      "LSTM SEQ 20 & 0.749 & 0.690 & 0.719 & 0.541 & test & 30 \\\\\n",
      "LSTM SEQ 30 & 0.748 & 0.690 & 0.702 & 0.571 & test & 30 \\\\\n",
      "LSTM SEQ 40 & 0.748 & 0.691 & 0.705 & 0.573 & test & 30 \\\\\n",
      "Random Forest & 0.656 & 0.676 & 0.876 & 0.690 & test & 10 \\\\\n",
      "Random Forest & 0.672 & 0.672 & 0.670 & 0.699 & test & 20 \\\\\n",
      "Random Forest & 0.702 & 0.699 & 0.792 & 0.693 & test & 30 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results_appendix_df[results_appendix_df['Subset'] == 'test'].drop(columns=['Window']).to_latex(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(df, 'AUROC')\n",
    "plot_subset(df, 'Accuracy')\n",
    "plot_subset(df, 'Precision')\n",
    "plot_subset(df, 'Recall')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
