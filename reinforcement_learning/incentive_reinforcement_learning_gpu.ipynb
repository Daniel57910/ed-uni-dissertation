{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 22.10.1+2.gca9a422da9 requires cupy-cuda115<12.0.0a0,>=9.5.0, which is not installed.\n",
      "cudf 22.10.1+2.gca9a422da9 requires cupy-cuda115<12.0.0a0,>=9.5.0, which is not installed.\n",
      "cudf 22.10.1+2.gca9a422da9 requires cuda-python<11.7.1,>=11.5, but you have cuda-python 11.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch --quiet\n",
    "!python -m pip install gym stable-baselines3[extra] python-dotenv fsspec[\"s3\"] boto3 s3fs==2022.11.0 tensorboard --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load npz_extractor.py\n",
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import logging \n",
    "\n",
    "class NPZExtractor:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, input_path, n_files, n_sequences, s3_client, data_partition) -> None:\n",
    "        self.input_path = input_path\n",
    "        self.n_files = n_files\n",
    "        self.n_sequences = n_sequences\n",
    "        self.s3_client = s3_client\n",
    "        self.data_partition = data_partition\n",
    "\n",
    "\n",
    "    def get_dataset_pointer(self):\n",
    "\n",
    "        read_path = os.path.join(self.input_path, f'files_used_{self.n_files}')\n",
    "        if not os.path.exists(read_path):\n",
    "            print(f'Creating directory: {read_path}')\n",
    "            os.makedirs(read_path)\n",
    "\n",
    "\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            key_zip, key_npy = (\n",
    "                os.path.join(read_path, f'sequence_index_{_}.npz'),\n",
    "                os.path.join(read_path, f'sequence_index_{_}')\n",
    "            )\n",
    "\n",
    "            self.logger.info(f'Loading pointer to dataset: {key_npy}: derived from {key_zip}')\n",
    "\n",
    "            if not os.path.exists(key_npy):\n",
    "                self.logger.info(f'Zip file to extract: {key_zip}: npy file to load: {key_npy}')\n",
    "                # self.s3_client.download_file(\n",
    "                #     'dissertation-data-dmiller',\n",
    "                #     key_zip,\n",
    "                #     key_zip\n",
    "                # )\n",
    "                self.logger.info(f'Zip file downloaded: {key_zip}')\n",
    "                self._zip_extract(key_zip, key_npy)\n",
    "\n",
    "        lz_concatenated_results = self._lazy_concatenate()\n",
    "\n",
    "        if self.data_partition:\n",
    "            return [p[:self.data_partition] for p in lz_concatenated_results]\n",
    "        else:\n",
    "            return lz_concatenated_results\n",
    "\n",
    "\n",
    "    def _zip_extract(self, key_zip, key_npy):\n",
    "        self.logger.info(f'Extracting file: {key_zip} -> {key_npy}')\n",
    "\n",
    "        with zipfile.ZipFile(key_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=key_npy, members=['arr_0.npy'])\n",
    "\n",
    "\n",
    "        self.logger.info(f'Zip file exracted: {key_zip} -> {key_npy}/arr_0.npy')\n",
    "\n",
    "    def _lazy_concatenate(self):\n",
    "        lz_concat = []\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            path_to_load = os.path.join(self.input_path, f'files_used_{self.n_files}', f'sequence_index_{_}', f'arr_0.npy')\n",
    "            self.logger.info(f'Loading: {path_to_load}')\n",
    "            lz_concat.append(np.load(path_to_load))\n",
    "        return lz_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load callback\n",
    "from stable_baselines3.common.callbacks import EveryNTimesteps, BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat\n",
    "from stable_baselines3.common.logger import Figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "class DistributionCallback(BaseCallback):\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self._dist_log_freq = 1000\n",
    "        self._reward_log_freq = 100\n",
    "        output_formats = self.logger.output_formats\n",
    "        self.tb_formatter = next(f for f in output_formats if isinstance(f, TensorBoardOutputFormat))\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._dist_log_freq == 0:\n",
    "            dist_list = self.training_env.env_method('dists')\n",
    "            episode_list = self.training_env.get_attr('current_episode')\n",
    "            try:\n",
    "                for episode, dist in zip(episode_list, dist_list):\n",
    "                    self.tb_formatter.writer.add_histogram('incentive_index', dist[:, 0], episode)\n",
    "                    self.tb_formatter.writer.add_histogram('distance_session_end', dist[:, 1], episode)\n",
    "                    self.tb_formatter.writer.add_histogram('distance_incentive_allocated', dist[:, 2], episode)\n",
    "                self.tb_formatter.writer.flush()\n",
    "            except Exception as e:\n",
    "                raise Exception('Unable to log distributions: {}'.format(e))\n",
    "            \n",
    "        \n",
    "        if self.n_calls % self._reward_log_freq == 0:\n",
    "            episode_list = self.training_env.get_attr('current_episode')\n",
    "            rewards = self.training_env.get_attr('user_sessions')\n",
    "            reward_dict = {}\n",
    "            for episode, reward in zip(episode_list, rewards):\n",
    "                reward_dict[f'episode_{episode}'] = reward['total_reward'].sum()\n",
    "           \n",
    "            self.tb_formatter.writer.add_scalars(\n",
    "                'cum_reward', reward_dict, (self.n_calls / np.max(episode_list)) // self._reward_log_freq\n",
    "            )\n",
    "            \n",
    "            self.tb_formatter.writer.flush()\n",
    "                \n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment\n",
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import pdb\n",
    "\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "TASK_INDEX = 3\n",
    "\n",
    "N_EVENT_INDEX = -1\n",
    "\n",
    "USER_IN_SESSION_INDEX = 0\n",
    "SESSION_COUNT_INDEX = 1\n",
    "TASK_IN_SESSION_INDEX = 2\n",
    "REWARD_ALLOCATED_INDEX = 3\n",
    "\n",
    "SESSION_FINISHED_INDEX = -1\n",
    "\n",
    "CUM_PLATFORM_TIME_INDEX = 4\n",
    "METADATA_INDEX = 12\n",
    "\n",
    "import logging\n",
    "from scipy.stats import norm \n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat\n",
    "\n",
    "\n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    logger = logging.getLogger(__name__) \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, user_sessions, experience_dataset, n_sequences, n_features) -> None:\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.user_sessions = user_sessions\n",
    "        self.experience_dataset = experience_dataset\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(n_sequences + 1, n_features), dtype=np.float32)\n",
    "        self.n_sequences = n_sequences\n",
    "        self.n_features = n_features\n",
    "        self.current_session = None\n",
    "        self.current_episode = 0\n",
    "        \n",
    "    def _extract_features(self, feature_array):\n",
    "        \n",
    "        metadata, features = feature_array[:, :METADATA_INDEX], feature_array[:, METADATA_INDEX:]\n",
    "        features = features.reshape((features.shape[0], self.n_sequences + 1, self.n_features))\n",
    "        features = np.flip(features, axis=1).squeeze(0)\n",
    "        return metadata.squeeze(0), features\n",
    "\n",
    "    def _state(self, user, session, task_count):\n",
    "        \n",
    "        \"\"\"\n",
    "        get index of current state\n",
    "        \"\"\" \n",
    "        current_state = self.experience_dataset[\n",
    "            (self.experience_dataset[:, USER_INDEX] == user) &\n",
    "            (self.experience_dataset[:, SESSION_INDEX] == session) &\n",
    "            (self.experience_dataset[:, TASK_INDEX] == task_count)\n",
    "        ]\n",
    "\n",
    "        metadata, features = self._extract_features(current_state)\n",
    "        cum_platform_time = metadata[CUM_PLATFORM_TIME_INDEX]\n",
    "        return features, cum_platform_time\n",
    "\n",
    "    \n",
    "    def _seed_user_session(self):\n",
    "        \"\"\"\n",
    "        find all users sessions that have not been completed\n",
    "        select random user session from list\n",
    "        \"\"\"\n",
    "        current_session = self.user_sessions[self.user_sessions['ended'] == 0].sample(1)\n",
    "        current_session['task_index'] = 1\n",
    "        self.current_session = current_session\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        self._take_action(action)\n",
    "            \n",
    "        state, rewards, done, meta = self._calculate_next_state() \n",
    "        if not done:\n",
    "            self._update_session_metadata(self.current_session)\n",
    "        \n",
    "        return state, rewards, done, meta\n",
    "\n",
    "    def _update_session_metadata(self, current_session):\n",
    "        self.user_sessions.loc[current_session.index] = current_session \n",
    "        \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        next_state = self.current_session['task_index'] + 1\n",
    "        extending = self._extending()\n",
    "        if not extending:\n",
    "            self.logger.debug(f'User: {self.current_session} has completed their session')\n",
    "            self._user_session_terminate()\n",
    "            if self.user_sessions['ended'].all():\n",
    "                self.logger.debug('All users have completed their sessions')\n",
    "                return None, self.user_sessions['total_reward'].sum().astype(float), True, {}\n",
    "            \n",
    "            self._seed_user_session()\n",
    "            user, session, count = self.current_session[['user_id', 'session_id', 'task_index']].values[0]\n",
    "            return (\n",
    "                self._state(user, session, count)[0], \n",
    "                self.user_sessions['total_reward'].sum().astype(float),\n",
    "                False,\n",
    "                {}\n",
    "            )\n",
    "        self.logger.debug(f'User: {self.current_session} has moving to next state: {next_state}')\n",
    "        self.current_session['task_index'] = next_state\n",
    "        user, session, count = self.current_session[['user_id', 'session_id', 'task_index']].values[0]\n",
    "        state, cum_platform_time = self._state(user, session, count)\n",
    "        self.current_session['total_reward'] = cum_platform_time\n",
    "        return (\n",
    "            state,\n",
    "            self.user_sessions['total_reward'].sum().astype(float),\n",
    "            False,\n",
    "            {}\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def _extending(self):\n",
    "        current_session = self.current_session.to_dict('records')[0]\n",
    "        if current_session['task_index'] == current_session['counts']:\n",
    "            return False\n",
    "    \n",
    "        if current_session['task_index'] <= current_session['sim_counts']:\n",
    "            return True\n",
    "\n",
    "        continue_session = self._probability_extending(current_session)\n",
    "        return all([continue_session >= 0.3, continue_session <= 0.9])\n",
    "    \n",
    "    \n",
    "    def _probability_extending(self, current_session):\n",
    "        if current_session['incentive_index'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            scale = min(5, current_session['counts'] // 4)\n",
    "            continue_session = norm(\n",
    "                loc=current_session['incentive_index'],\n",
    "                scale=scale\n",
    "            ).cdf(current_session['task_index']) + self._gaussian_noise()\n",
    "       \n",
    "        return continue_session\n",
    "        \n",
    "    def _gaussian_noise(self):\n",
    "        return 0\n",
    "        return np.clip(\n",
    "            np.random.normal(0.01, 0.05, 1)[0],\n",
    "            0.1,\n",
    "            -0.1\n",
    "        )\n",
    "     \n",
    "    def _user_session_terminate(self):\n",
    "        self.current_session['ended'] = 1\n",
    "        self._update_session_metadata(self.current_session)\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        \n",
    "        current_session = self.current_session.to_dict('records')[0]\n",
    "        \n",
    "        if current_session['incentive_index'] > 0 or action == 0:\n",
    "            self.logger.debug(f'Incentive already allocation for session or no-op: {action}, {current_session}')\n",
    "            return\n",
    "        \n",
    "    \n",
    "        self.logger.debug('Taking action and allocating incentive')\n",
    "        self.current_session['incentive_index'] = self.current_session['task_index']\n",
    "        self.current_session['reward_allocated'] = action\n",
    "        \n",
    "        self.logger.debug('Taking action and allocating incentive: updating user session')\n",
    "        self.logger.debug(f'User session: {self.current_session}')\n",
    "\n",
    "    def reset(self):\n",
    "        self.user_sessions = self.user_sessions.sample(frac=1)\n",
    "        self.user_sessions['incentive_index'] = 0\n",
    "        self.user_sessions['task_index'] = 0\n",
    "        self.user_sessions['ended'] = 0\n",
    "        self.user_sessions['total_reward'] = 0\n",
    "        self.user_sessions['total_reward'] = self.user_sessions['total_reward'].astype(float)\n",
    "        \n",
    "        self._seed_user_session()\n",
    "        self._update_session_metadata(self.current_session)\n",
    "        user, session, count = self.current_session[['user_id', 'session_id', 'task_index']].values[0]\n",
    "        self.current_episode += 1\n",
    "        return self._state(user, session, count)[0]\n",
    "        \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        print('rendering')\n",
    "        \n",
    "    def dists(self):\n",
    "        incentive_index = self.user_sessions['incentive_index'].values\n",
    "        distance_end = (self.user_sessions['counts'] - self.user_sessions['incentive_index']).values\n",
    "        distance_reward = (self.user_sessions['total_reward'] - self.user_sessions['incentive_index']).values\n",
    "        return np.array([incentive_index, distance_end, distance_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, StopTrainingOnMaxEpisodes, CheckpointCallback\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "import logging\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "CUM_SESSION_EVENT_RAW = 3\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "from pprint import pformat\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "torch.set_printoptions(precision=2, linewidth=200, sci_mode=False)\n",
    "\n",
    "\n",
    "S3_BASELINE_PATH = 's3://dissertation-data-dmiller'\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--read_path', type=str, default='datasets/torch_ready_data')\n",
    "    parser.add_argument('--n_files', type=str, default=2)\n",
    "    parser.add_argument('--n_sequences', type=int, default=10)\n",
    "    parser.add_argument('--n_features', type=int, default=18)\n",
    "    parser.add_argument('--n_episodes', type=int, default=20)\n",
    "    parser.add_argument('--return_distribution', type=str, default='stack_overflow_v1')\n",
    "    parser.add_argument('--agent', type=str, default='constant_20')\n",
    "    parser.add_argument('--device', type=str, default='cpu')\n",
    "    parser.add_argument('--event_sample', type=float, default=0.25)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def train_eval_split(dataset, logger):\n",
    "    train_split = int(dataset.shape[0] * TRAIN_SPLIT)\n",
    "    eval_split = int(dataset.shape[0] * EVAL_SPLIT)\n",
    "    test_split = dataset.shape[0] - train_split - eval_split\n",
    "    logger.info(f'Train size: 0:{train_split}, eval size: {train_split}:{train_split+eval_split}: test size: {train_split + eval_split}:{dataset.shape[0]}')\n",
    "    train_dataset, eval_dataset, test_split = dataset[:train_split], dataset[train_split:train_split+eval_split], dataset[train_split+eval_split:]\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'eval': eval_dataset,\n",
    "        'test': test_split\n",
    "    }\n",
    "\n",
    "def generate_metadata(dataset, logger):\n",
    "     \n",
    "    logger.info('Generating metadata tasks per session')\n",
    "    sessions = pd.DataFrame(\n",
    "        dataset[:, [USER_INDEX, SESSION_INDEX]],\n",
    "        columns=['user_id', 'session_id']\n",
    "    )\n",
    "    \n",
    "    event_in_session = pd.DataFrame(\n",
    "        dataset[:, [USER_INDEX, SESSION_INDEX, CUM_SESSION_EVENT_RAW]],\n",
    "        columns=['user_id', 'session_id', 'event_in_session']\n",
    "    )\n",
    "    \n",
    "    event_in_session['event_in_session'] = event_in_session['event_in_session'].astype(int)\n",
    "    \n",
    "    event_in_session['event_in_session_resampled'] = event_in_session.groupby(['user_id', 'session_id']).cumcount() + 1\n",
    "    \n",
    "    sessions = sessions.groupby(['user_id', 'session_id']).size().reset_index(name='counts')\n",
    "    sessions['sim_counts'] = (sessions['counts'] * 0.8).astype(int)\n",
    "    sessions['sim_counts'] = sessions['sim_counts'].apply(lambda x: 1 if x == 0 else x)\n",
    "    sessions['incentive_index'] = 0\n",
    "    \n",
    "    sessions['task_index'] = 0\n",
    "    sessions['total_reward'] = 0\n",
    "    sessions['total_reward'] = sessions['total_reward'].astype(float)\n",
    "    sessions['ended'] = 0\n",
    "    return sessions\n",
    "\n",
    "\n",
    "def run_reinforcement_learning_incentives(environment, logger, n_episodes=1):\n",
    "    for epoch in range(n_episodes):\n",
    "        environment_comp = False\n",
    "        state = environment.reset()\n",
    "        i = 0\n",
    "        while not environment_comp:\n",
    "            next_action = (\n",
    "                1 if np.random.uniform(low=0, high=1) > 0.8 else 0\n",
    "            )\n",
    "            state, rewards, environment_comp, meta = environment.step(next_action)\n",
    "            i +=1\n",
    "            if i % 100 == 0:\n",
    "                logger.info(f'Step: {i} - Reward: {rewards}')\n",
    "                \n",
    "        logger.info(f'Epoch: {epoch} - Reward: {rewards}')\n",
    "        print(environment.user_sessions.head(10))\n",
    "\n",
    "    \n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    exec_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    \n",
    "    read_path, n_files, n_sequences, n_features, n_episodes, device, event_sample = (\n",
    "        args.read_path, \n",
    "        args.n_files, \n",
    "        args.n_sequences, \n",
    "        args.n_features, \n",
    "        args.n_episodes, \n",
    "        args.device,\n",
    "        args.event_sample\n",
    "    )\n",
    "    \n",
    "    npz_extractor = NPZExtractor(\n",
    "        read_path,\n",
    "        n_files,\n",
    "        n_sequences,\n",
    "        None,\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    cpu_count = int(os.cpu_count() * .8)\n",
    "   \n",
    "    logger.info(f'Starting experiment at {exec_time}') \n",
    "    logger.info(f'Extracting dataset from npz files to tensor' )\n",
    "    dataset = np.concatenate(npz_extractor.get_dataset_pointer(), axis=1)\n",
    "    logger.info('Dataset shape: {}'.format(dataset.shape))\n",
    "    train_data = train_eval_split(dataset, logger)['train']\n",
    "    logger.info(f'Following sampling: {train_data.shape}')\n",
    "    train_data = train_data[:int(train_data.shape[0] * event_sample)]\n",
    "    logger.info(f'Dataset shape: {train_data.shape}: generating metadata tensor')\n",
    "    sessions_train = generate_metadata(train_data, logger)\n",
    "    logger.info(f'Metadata train: {sessions_train.shape}')\n",
    "    logger.info(f'resetting number of sessions to sample: {sessions_train.shape}')\n",
    "\n",
    "    logger.info(f'Creating vectorized training environment: num envs: {cpu_count}')\n",
    "   \n",
    "    citizen_science_vec = SubprocVecEnv([lambda: CitizenScienceEnv(sessions_train, train_data, n_sequences, n_features) for _ in range(cpu_count)])\n",
    "\n",
    "    \"\"\"\n",
    "    Eval environment is not used in training and is used after training to evaluate the agent\n",
    "    \"\"\"    \n",
    "    logger.info(f'Vectorized environments created, wrapping with monitor')\n",
    "    \n",
    "    base_path = os.path.join(\n",
    "        S3_BASELINE_PATH,\n",
    "        'reinforcement_learning_incentives',\n",
    "        f'n_files_{n_files}',\n",
    "        'results',\n",
    "        exec_time,\n",
    "    ) \n",
    " \n",
    "    tensorboard_dir, checkpoint_dir = (\n",
    "        os.path.join(base_path, 'training_metrics'),\n",
    "        os.path.join(base_path, 'checkpoints'),\n",
    "    )\n",
    " \n",
    "    monitor_train = VecMonitor(citizen_science_vec)\n",
    "    agent = DQN(\n",
    "        'MlpPolicy',\n",
    "        monitor_train,\n",
    "        verbose=1,\n",
    "        device=args.device,\n",
    "        batch_size=4096,\n",
    "        tensorboard_log=tensorboard_dir,\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=10000 // 2,\n",
    "        name_prefix='a2c',\n",
    "        save_path=checkpoint_dir,\n",
    "        verbose=1,\n",
    "    )\n",
    "        \n",
    "    callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=n_episodes, verbose=1)\n",
    "    \n",
    "    dist_callback = DistributionCallback()\n",
    "    callback_list = CallbackList([dist_callback, callback_max_episodes, checkpoint_callback])\n",
    "\n",
    "    logger.info(pformat([\n",
    "        'n_episodes: {}'.format(n_episodes),\n",
    "        'read_path: {}'.format(read_path),\n",
    "        'n_files: {}'.format(n_files),\n",
    "        'n_sequences: {}'.format(n_sequences),\n",
    "        'n_features: {}'.format(n_features),\n",
    "        'total_timesteps: {}'.format(dataset.shape[0] -1),\n",
    "        'device: {}'.format(device),\n",
    "        'tensorboard_dir: {}'.format(tensorboard_dir),\n",
    "        'checkpoint_dir: {}'.format(checkpoint_dir),\n",
    "        'event_sample: {}'.format(event_sample)\n",
    "    ]))\n",
    "\n",
    "    agent.learn(\n",
    "        total_timesteps=int(10e7),\n",
    "        log_interval=100, \n",
    "        progress_bar=True,\n",
    "        callback=callback_list\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument:\n",
    "    read_path = 'torch_ready_data'\n",
    "    n_files = 2\n",
    "    n_sequences = 10\n",
    "    n_features = 18\n",
    "    n_episodes = 20\n",
    "    device = 'cuda'\n",
    "    event_sample = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/17/2023 06:26:30 PM Starting experiment at 2023-04-17-18-26\n",
      "04/17/2023 06:26:30 PM Extracting dataset from npz files to tensor\n",
      "04/17/2023 06:26:30 PM Loading pointer to dataset: torch_ready_data/files_used_2/sequence_index_0: derived from torch_ready_data/files_used_2/sequence_index_0.npz\n",
      "04/17/2023 06:26:30 PM Loading pointer to dataset: torch_ready_data/files_used_2/sequence_index_10: derived from torch_ready_data/files_used_2/sequence_index_10.npz\n",
      "04/17/2023 06:26:30 PM Loading: torch_ready_data/files_used_2/sequence_index_0/arr_0.npy\n",
      "04/17/2023 06:26:30 PM Loading: torch_ready_data/files_used_2/sequence_index_10/arr_0.npy\n",
      "04/17/2023 06:26:34 PM Dataset shape: (2566734, 210)\n",
      "04/17/2023 06:26:34 PM Train size: 0:1796713, eval size: 1796713:2181723: test size: 2181723:2566734\n",
      "04/17/2023 06:26:34 PM Following sampling: (1796713, 210)\n",
      "04/17/2023 06:26:34 PM Dataset shape: (898356, 210): generating metadata tensor\n",
      "04/17/2023 06:26:34 PM Generating metadata tasks per session\n",
      "04/17/2023 06:26:35 PM Metadata train: (14724, 8)\n",
      "04/17/2023 06:26:35 PM resetting number of sessions to sample: (14724, 8)\n",
      "04/17/2023 06:26:35 PM Creating vectorized training environment: num envs: 6\n",
      "04/17/2023 06:26:59 PM Vectorized environments created, wrapping with monitor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/17/2023 06:27:01 PM ['n_episodes: 20',\n",
      " 'read_path: torch_ready_data',\n",
      " 'n_files: 2',\n",
      " 'n_sequences: 10',\n",
      " 'n_features: 18',\n",
      " 'total_timesteps: 2566733',\n",
      " 'device: cuda',\n",
      " 'tensorboard_dir: '\n",
      " 's3://dissertation-data-dmiller/reinforcement_learning_incentives/n_files_2/results/2023-04-17-18-26/training_metrics',\n",
      " 'checkpoint_dir: '\n",
      " 's3://dissertation-data-dmiller/reinforcement_learning_incentives/n_files_2/results/2023-04-17-18-26/checkpoints',\n",
      " 'event_sample: 0.5']\n",
      "04/17/2023 06:27:01 PM Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to s3://dissertation-data-dmiller/reinforcement_learning_incentives/n_files_2/results/2023-04-17-18-26/training_metrics/DQN_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f286c6c206914f16a9361a3713c26d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/1023914865.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArgument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31/2753504516.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    201\u001b[0m     ]))\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     agent.learn(\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10e7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/stable_baselines3/dqn/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     ) -> SelfDQN:\n\u001b[0;32m--> 269\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             rollout = self.collect_rollouts(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mtrain_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;31m# Rescale and perform action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_monitor.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_returns\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_lengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "main(Argument)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
