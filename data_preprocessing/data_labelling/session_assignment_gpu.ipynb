{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease                         \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease                 \n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Fetched 222 kB in 1s (417 kB/s)    \n",
      "Reading package lists... Done\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "E: Unable to locate package dotenv\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install awscli dotenv zip -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "\n",
    "import logging\n",
    "import os\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "import pprint\n",
    "import numpy as np\n",
    "import cudf as pd\n",
    "import pandas as cpu_pd\n",
    "\n",
    "import dask_cudf as dd\n",
    "import pprint\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--read_path', type=str, default='datasets/encoded_time_data')\n",
    "    parser.add_argument('--write_path', type=str, default='datasets/frequency_encoded_2')\n",
    "    parser.add_argument('--n_files', type=int, default=2)\n",
    "    parser.add_argument('--use_gpu', type=bool, default=False)\n",
    "    parser.add_argument('--test_env', type=bool, default=True)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "ALL_COLUMNS = [\n",
    "    \"project_id\",\n",
    "    \"workflow_id\",\n",
    "    \"user_id\",\n",
    "    \"country\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\",\n",
    "    \"day_of_week\",\n",
    "    \"date_time\",\n",
    "    \"previous_date_time\",\n",
    "    \"time_diff_seconds\",\n",
    "    \"5_minute_session_count\",\n",
    "    \"30_minute_session_count\",\n",
    "    \"max_session_time\",\n",
    "    \"time_until_end_of_session\",\n",
    "    \"label\",\n",
    "    \"task_within_session_count\",\n",
    "    \"project_workflow_count\",\n",
    "    \"user_count\",\n",
    "    \"country_count\"\n",
    "]\n",
    "\n",
    "LOAD_COLUMNS = [\n",
    "    \"project_id\",\n",
    "    \"workflow_id\",\n",
    "    \"user_id\",\n",
    "    \"country\",  \n",
    "    \"date_time\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_U_ID = [2373355, 10, 4301]\n",
    "SAMPLE_COLS = [\n",
    "    'user_id',\n",
    "    'date_time',\n",
    "    'row_count',\n",
    "    'label_30',\n",
    "    'session_30',\n",
    "]\n",
    "def outer_apply_inflection_points(inflections):\n",
    "    def inner_find_session(row):\n",
    "        inflection_for_user = inflections[row['user_id']]\n",
    "        index_for_row = row['row_count']\n",
    "        return 1 + np.searchsorted(inflection_for_user, index_for_row, side='left')\n",
    "\n",
    "    return inner_find_session\n",
    "\n",
    "def get_inflection_points_5(subset):\n",
    "    return subset[subset['label_5'] == False].index.values\n",
    "\n",
    "def get_inflection_points_30(subset):\n",
    "    return subset[subset['label_30'] == False].index.values\n",
    "\n",
    "class SessionCalculate:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, df, write_path, use_gpu, test_env) -> None:\n",
    "        self.df = df\n",
    "        self.write_path = write_path\n",
    "        self.use_gpu = use_gpu\n",
    "        self.test_env = test_env\n",
    "    \n",
    "    def calculate_inflections(self):\n",
    "       \n",
    "        self.logger.info('Calculating subsequent date time')\n",
    "        self.df['next_date_time'] = self.df.groupby('user_id')['date_time'].shift(-1)\n",
    "        self.df = self.df.drop_duplicates(subset=['user_id', 'date_time'], keep='last').reset_index()\n",
    "        if self.use_gpu:\n",
    "            self.logger.info('Bringing to CPU for second calculation')\n",
    "            self.df = self.df.to_pandas()\n",
    "           \n",
    "            \n",
    "        self.df['diff_seconds'] = (self.df['next_date_time'] - self.df['date_time']).apply(lambda x: x.total_seconds())\n",
    "        self.logger.info('Diff seconds calculated')\n",
    "\n",
    "        self.df['diff_minutes'] = (self.df['diff_seconds'] / 60)\n",
    "        self.df['label_5'] = (self.df['diff_minutes'] < 5)\n",
    "        self.df['label_30'] = self.df['diff_minutes'] < 30\n",
    "\n",
    "        \n",
    "        self.logger.info(f'Labels calculated: removing rows with diff seconds > 0')\n",
    "    \n",
    " \n",
    "        self.df = self.df.drop(columns=['next_date_time', 'diff_seconds'])\n",
    "        self.logger.info(f'Number of rows following drop: {self.df.shape[0]}')\n",
    "        self.logger.info(f'Sorting rows by date time and applying row count')\n",
    "        self.df = self.df.sort_values(['date_time']).reset_index()\n",
    "        self.df['user_id'] = self.df['user_id'].astype('int32')\n",
    "        self.df['row_count'] = self.df.index.values\n",
    "        self.logger.info(f'Sorted rows and applied row count on updated index')  \n",
    "        self.logger.info('Calculating inflection points')\n",
    "        \n",
    "        inflections_5_merge = self.df[self.df['label_5'] == False]\n",
    "        inflections_30_merge = self.df[self.df['label_30'] == False]\n",
    "        \n",
    "        inflections_5_merge['session_5'] = inflections_5_merge.groupby('user_id').cumcount() + 1\n",
    "        inflections_30_merge['session_30'] = inflections_30_merge.groupby('user_id').cumcount() + 1\n",
    "        \n",
    "        inflections_5_merge = inflections_5_merge[['user_id', 'row_count', 'session_5']].sort_values(['row_count'])\n",
    "        inflections_30_merge = inflections_30_merge[['user_id', 'row_count', 'session_30']].sort_values(['row_count'])\n",
    "        \n",
    "   \n",
    "        self.logger.info('Inflection points calcularting for 5') \n",
    "        self.df = cpu_pd.merge_asof(self.df, inflections_5_merge, on='row_count', by='user_id', direction='forward')\n",
    "        self.logger.info('Inflection points calcularting for 30')\n",
    "        self.df = cpu_pd.merge_asof(self.df, inflections_30_merge, on='row_count', by='user_id', direction='forward')\n",
    "        self.logger.info('Inflection points calculated')\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.logger.info('Bringing back to GPU for final calculations')\n",
    "            self.df = pd.from_pandas(self.df)\n",
    "              \n",
    "        self.logger.info('Inflections calculated')\n",
    "   \n",
    "    \n",
    "    def write_inflections_parquet(self):\n",
    "    \n",
    "        self.df = self.df.drop(columns=['index', 'level_0'])\n",
    "       \n",
    "        if not self.test_env:\n",
    "            self.df = self.df.drop(columns=['diff_minutes', 'row_count'])\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            import dask_cudf as ddf\n",
    "            self.logger.info('Bringing back to dask GPU for final calculations')\n",
    "            self.df = ddf.from_cudf(self.df, npartitions=30)\n",
    "        else:\n",
    "            import dask.dataframe as ddf\n",
    "            self.logger.info('Bringing back to dask CPU for final calculations')\n",
    "            self.df = ddf.from_pandas(self.df, npartitions=30)\n",
    "        \n",
    "        self.logger.info(f'Writing inflections to {self.write_path}')    \n",
    "        # write_path = self.write_path + '.parquet.gzip'\n",
    "        self.df.to_parquet(self.write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "def main(args):\n",
    "    cpu_pd.set_option('display.max_columns', None)\n",
    "    cpu_pd.set_option('display.max_rows', None)\n",
    "    cpu_pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    cpu_pd.set_option('display.width', 500)\n",
    "\n",
    "    logger = get_logger()\n",
    "    read_path, write_path, n_files, use_gpu, test_env = args.read_path, args.write_path, args.n_files, args.use_gpu, args.test_env\n",
    "    logger.info(f'Starting Session Assignment')\n",
    "    logger.info(f'Read: {read_path}, Write: {write_path}, N Files: {n_files}, GPU: {use_gpu}, Test Env: {test_env}')\n",
    "    files_to_read = sorted(list(glob.iglob(f'{read_path}/*.csv')))\n",
    "    logger.info(f'Found {len(files_to_read)} files to read')\n",
    "    df = dd.read_csv(files_to_read[:n_files], usecols=LOAD_COLUMNS).compute()\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    df = df.sort_values(by=['date_time'])\n",
    "    session_calculator = SessionCalculate(df, args.write_path, args.use_gpu, args.test_env)\n",
    "    session_calculator.calculate_inflections()\n",
    "    \n",
    "    session_calculator.write_inflections_parquet()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    read_path = 'frequency_encoded_data'\n",
    "    write_path = 'labelled_session_count_data'\n",
    "    n_files = 61\n",
    "    use_gpu = True\n",
    "\n",
    "    test_env = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 15:08:45,433 - __main__ - INFO - Starting Session Assignment\n",
      "2023-03-19 15:08:45,434 - __main__ - INFO - Read: frequency_encoded_data, Write: frequency_encoded_data_2, N Files: 61, GPU: True, Test Env: True\n",
      "2023-03-19 15:08:45,435 - __main__ - INFO - Found 60 files to read\n",
      "2023-03-19 15:08:57,012 - __main__ - INFO - Calculating subsequent date time\n",
      "2023-03-19 15:08:57,170 - __main__ - INFO - Bringing to CPU for second calculation\n",
      "2023-03-19 15:10:07,440 - __main__ - INFO - Diff seconds calculated\n",
      "2023-03-19 15:10:07,765 - __main__ - INFO - Labels calculated: removing rows with diff seconds > 0\n",
      "2023-03-19 15:10:09,033 - __main__ - INFO - Number of rows following drop: 38500990\n",
      "2023-03-19 15:10:09,034 - __main__ - INFO - Sorting rows by date time and applying row count\n",
      "2023-03-19 15:10:18,104 - __main__ - INFO - Sorted rows and applied row count on updated index\n",
      "2023-03-19 15:10:18,105 - __main__ - INFO - Calculating inflection points\n",
      "2023-03-19 15:10:19,549 - __main__ - INFO - Inflection points calcularting for 5\n",
      "2023-03-19 15:10:28,915 - __main__ - INFO - Inflection points calcularting for 30\n",
      "2023-03-19 15:10:38,943 - __main__ - INFO - Inflection points calculated\n",
      "2023-03-19 15:10:38,944 - __main__ - INFO - Bringing back to GPU for final calculations\n",
      "2023-03-19 15:10:41,072 - __main__ - INFO - Inflections calculated\n",
      "2023-03-19 15:10:41,099 - __main__ - INFO - Bringing back to dask GPU for final calculations\n",
      "2023-03-19 15:10:41,183 - __main__ - INFO - Writing inflections to frequency_encoded_data_2\n"
     ]
    }
   ],
   "source": [
    "main(Arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_session_assignment(df):\n",
    "\n",
    "    df['date_time'] = cpu_pd.to_datetime(df['date_time'])\n",
    "    df = df.sort_values(['date_time'])\n",
    "\n",
    "    less_5 = df[df['label_5'] == True]\n",
    "    less_30 = df[df['label_30'] == True]\n",
    "    \n",
    "    assert less_5.shape[0] == df[df['diff_minutes'] < 5].shape[0]\n",
    "    assert less_30.shape[0] == df[df['diff_minutes'] < 30].shape[0]\n",
    "        \n",
    "def test_session_boundary(calculated_df):\n",
    "    calculated_df['date_time'] = cpu_pd.to_datetime(calculated_df['date_time'])\n",
    "    calculated_df = calculated_df.sort_values(['date_time'])\n",
    "    \n",
    "    for user_stats, df_subset in calculated_df.groupby(['user_id', 'session_30']):\n",
    "        df_subset_in_range = df_subset.iloc[:df_subset.shape[0] - 2]\n",
    "        inflection = df_subset.iloc[-1]\n",
    "        if df_subset_in_range.shape[0] > 2:\n",
    "            df_subset_in_range = df_subset_in_range.iloc[0: df_subset_in_range.shape[0] - 2]\n",
    "            assert df_subset_in_range[df_subset_in_range['label_30'] == False].shape[0] == 0\n",
    "            assert inflection['label_30'] == False\n",
    "        \n",
    "        if inflection.session_30 > 1:\n",
    "            user_id, session_30 = user_stats\n",
    "            previous_inflections = calculated_df[\n",
    "                (calculated_df['user_id'] == user_id) & (calculated_df['session_30'] == session_30 - 1)\n",
    "            ]\n",
    "            \n",
    "            max_row = previous_inflections.iloc[-1]\n",
    "            if max_row.shape[0] > 0:\n",
    "                assert max_row['label_30'] == False\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet('frequency_encoded_data_2').compute().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_users = df[['user_id']].drop_duplicates().reset_index(drop=True).sample(1000)\n",
    "df = df[df['user_id'].isin(sample_users['user_id'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_session_assignment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_session_boundary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r labelled_session_count_data.zip labelled_session_count_data -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./frequency_encoded_data.zip to s3://dissertation-data-dmiller/labelled_session_count_data.zip\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp labelled_session_count_data.zip s3://dissertation-data-dmiller/labelled_session_count_data.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
