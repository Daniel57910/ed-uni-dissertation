{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch python-dotenv boto3 --quiet awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import boto3\n",
    "from pprint import pformat\n",
    "from torch import nn\n",
    "import io\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "LABEL = [\n",
    "    \"label\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"glob_session_time_raw\", \n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"user_count\",\n",
    "    \"project_count\",\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "LOAD_COLS = LABEL + METADATA + OUT_FEATURE_COLUMNS\n",
    "\n",
    "S3_BUCKET = 'dissertation-data-dmiller'\n",
    "BASE_CHECK_PATH = 'lstm_experiments/checkpoints'\n",
    "\n",
    "\n",
    "LSTM_CHECKPOINTS_10 = {\n",
    "    'LSTM SEQ 1': 'lstm-experiments/ordinal-sequence-length-10-window-1/lightning_logs/version_0/checkpoints/clickstream-ordinal-sequence_length-10-window-1epoch=39-loss_valid=0.61.ckpt',\n",
    "    'LSTM SEQ 10': 'lstm-experiments/ordinal-sequence-length-10-window-10/lightning_logs/version_0/checkpoints/clickstream-ordinal-sequence_length-10epoch=03-loss_valid=0.60.ckpt',\n",
    "    'LSTM SEQ 20': 'lstm-experiments/ordinal-sequence-length-20-window-10/lightning_logs/version_0/checkpoints/clickstream-ordinal-sequence_length-20-window-10epoch=23-loss_valid=0.61.ckpt',\n",
    "    'LSTM SEQ 30': 'lstm-experiments/ordinal-sequence-length-30-window-10/lightning_logs/version_0/checkpoints/clickstream-ordinal-sequence_length-30-window-10epoch=25-loss_valid=0.61.ckpt',\n",
    "    'LSTM SEQ 40': 'lstm-experiments/ordinal-sequence-length-40-window-20/lightning_logs/version_1/checkpoints/clickstream-ordinal-sequence_length-40-window-20epoch=75-loss_valid=0.61.ckpt'\n",
    "}\n",
    "\n",
    "LSTM_CHECKPOINTS_20 = {\n",
    "    'LSTM SEQ 1': 'lstm-experiments/ordinal-sequence-length-1-window-20/lightning_logs/version_0/checkpoints/clickstream-ordinal-sequence_length-1-window-20epoch=61-loss_valid=0.61.ckpt',\n",
    "    'LSTM SEQ 10': 'lstm-experiments/ordinal-sequence-length-10-window-20/lightning_logs/version_1/checkpoints/clickstream-ordinal-sequence_length-10-window-20epoch=13-loss_valid=0.62.ckpt',\n",
    "    'LSTM SEQ 20': 'lstm-experiments/ordinal-sequence-length-20-window-10/lightning_logs/version_0/checkpoints/clickstream-ordinal-sequence_length-20-window-10epoch=23-loss_valid=0.61.ckpt',\n",
    "    'LSTM SEQ 30': 'lstm-experiments/ordinal-sequence-length-30-window-20/lightning_logs/version_0/checkpoints/clickstream-ordinal-sequence_length-30-window-20epoch=21-loss_valid=0.61.ckpt',\n",
    "    'LSTM SEQ 40': 'lstm-experiments/ordinal-sequence-length-30-window-20/lightning_logs/version_0/checkpoints/clickstream-ordinal-sequence_length-30-window-20epoch=21-loss_valid=0.61.ckpt'\n",
    "}  \n",
    " \n",
    "LSTM_CHECKPOINTS_30 = {\n",
    "    'LSTM SEQ 1': 'lstm-experiments/ordinal-sequence-length-10/lightning_logs/version_3/checkpoints/clickstream-ordinal-sequence_length-1epoch=33-loss_valid=0.59.ckpt',\n",
    "    'LSTM SEQ 10': 'lstm-experiments/ordinal-sequence-length-10/lightning_logs/version_1/checkpoints/clickstream-ordinal-sequence_length-10epoch=27-loss_valid=0.59.ckpt',\n",
    "    'LSTM SEQ 20': 'lstm-experiments/ordinal-sequence-length-20/lightning_logs/version_0/checkpoints/clickstream-ordinal-sequence_length-20epoch=23-loss_valid=0.59.ckpt',\n",
    "    'LSTM SEQ 30': 'lstm-experiments/ordinal-sequence-length-30/lightning_logs/version_0/checkpoints/clickstream-ordinal-sequence_length-30epoch=61-loss_valid=0.59.ckpt',\n",
    "    'LSTM SEQ 30 H': 'lstm-experiments/heuristic-ordinal-sequence-length-30/lightning_logs/version_8/checkpoints/clickstream-heuristic-ordinal-sequence_length-30epoch=67-loss_valid=0.59.ckpt',\n",
    "    'LSTM SEQ 40': 'lstm-experiments/ordinal-sequence-length-40/lightning_logs/version_0/checkpoints/clickstream-ordinal-sequence_length-40epoch=75-loss_valid=0.59.ckpt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data_module\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "LABEL_INDEX = 1\n",
    "TOTAL_EVENTS_INDEX = 2\n",
    "BATCHES = 1000000\n",
    "\n",
    "    \n",
    "class ClickstreamDataset(Dataset):\n",
    "    def __init__(self, dataset_pointer_list) -> None:\n",
    "        \"\"\"\n",
    "        Yield data in batches of BATCHES\n",
    "        \"\"\"\n",
    "        self.events = dataset_pointer_list\n",
    "        self.size = self.events[0].shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        events = [np.array([event[idx]]) for event in self.events]\n",
    "        return np.concatenate(events, axis=1)\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load torch_model_bases\n",
    "import torch \n",
    "from torch import nn\n",
    "N_FEATURES = 22\n",
    "class LSTMOrdinal(nn.Module):\n",
    "    def __init__(self,  hidden_size=32, dropout=0.2) -> None:\n",
    "        super(LSTMOrdinal, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=N_FEATURES,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.output = nn.Linear(\n",
    "            hidden_size,\n",
    "            1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1]\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "class NPZExtractor:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, input_path, n_files, n_sequences, s3_client, pred_window) -> None:\n",
    "        self.input_path = input_path\n",
    "        self.n_files = n_files\n",
    "        self.n_sequences = n_sequences\n",
    "        self.s3_client = s3_client\n",
    "        self.pred_window = pred_window\n",
    "        self.read_path = os.path.join(\n",
    "            self.input_path,\n",
    "            f'files_used_{self.n_files}_window_{self.pred_window}'\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_dataset_pointer(self):\n",
    "\n",
    "        if not os.path.exists(self.read_path):\n",
    "            self.logger.info(f'Creating directory: {self.read_path}')\n",
    "            os.makedirs(self.read_path)\n",
    "\n",
    "\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            key_zip, key_npy = (\n",
    "                os.path.join(self.read_path, f'sequence_index_{_}.npz'),\n",
    "                os.path.join(self.read_path, f'sequence_index_{_}')\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f'Loading pointer to dataset: {key_npy}: derived from {key_zip}')\n",
    "\n",
    "\n",
    "            if not os.path.exists(key_zip):\n",
    "                self.logger.info(f'Zip file to extract: {key_zip}: npy file to load: {key_npy}')\n",
    "                self.s3_client.download_file(\n",
    "                    'dissertation-data-dmiller',\n",
    "                    key_zip,\n",
    "                    key_zip\n",
    "                )\n",
    "            if not os.path.exists(key_npy):\n",
    "                self.logger.info(f'Zip file downloaded: {key_zip}: npy file to load: {key_npy}')\n",
    "\n",
    "                self._zip_extract(key_zip, key_npy)\n",
    "\n",
    "        lz_concatenated_results = self._lazy_concatenate()\n",
    "        return lz_concatenated_results\n",
    "\n",
    "    def _zip_extract(self, key_zip, key_npy):\n",
    "        self.logger.info(f'Extracting file: {key_zip} -> {key_npy}')\n",
    "\n",
    "        with zipfile.ZipFile(key_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=key_npy, members=['arr_0.npy'])\n",
    "\n",
    "        self.logger.info(f'Zip file exracted: {key_zip} -> {key_npy}/arr_0.npy')\n",
    "\n",
    "    def _lazy_concatenate(self):\n",
    "        lz_concat = []\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            path_to_load = os.path.join(self.read_path, f'sequence_index_{_}', f'arr_0.npy')\n",
    "            self.logger.info(f'Loading: {path_to_load}')\n",
    "            lz_concat.append(np.load(path_to_load, mmap_mode='r'))\n",
    "        return lz_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load likelihood_engagement_cpu\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import logging\n",
    "import boto3\n",
    "from pprint import pformat\n",
    "from torch import nn\n",
    "import io\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)    \n",
    "pd.set_option('mode.use_inf_as_na', True)\n",
    "torch.set_printoptions(sci_mode=False, linewidth=400, precision=2)\n",
    "np.set_printoptions(suppress=True, precision=4, linewidth=200)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "METADATA_INDEX = 13\n",
    "\n",
    "logger = logging.getLogger('likelihood_engagement')\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--n_files', type=int, default=2)\n",
    "    parser.add_argument('--n_sequences', type=int, default=20)\n",
    "    parser.add_argument('--file_path', type=str, default='datasets/torch_ready_data')\n",
    "    parser.add_argument('--write_path', type=str, default='datasets/rl_ready_data')\n",
    "    parser.add_argument('--model_type', type=str, default='ordinal')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def _extract_features(tensor, n_sequences, n_features):\n",
    "    \n",
    "    features_dict = {}\n",
    "    tensor = tensor.squeeze()\n",
    "    metadata = tensor[:, :METADATA_INDEX]\n",
    "    features = tensor[:, METADATA_INDEX:]\n",
    "                \n",
    "    features = torch.flip(\n",
    "        torch.reshape(features, (features.shape[0], 41, 22)),\n",
    "        dims=[1]\n",
    "    )\n",
    "    \n",
    "    features_dict['LSTM SEQ 1'] =  features[:, -1, :].unsqueeze(1).clone()\n",
    "    features_dict['LSTM SEQ 10'] = features[:, 30:, :].clone()\n",
    "    features_dict['LSTM SEQ 20'] = features[:, 20:, :].clone()\n",
    "    features_dict['LSTM SEQ 30'] = features[:, 10:, :].clone()\n",
    "    features_dict['LSTM SEQ 30 H']=features[:, 10:, :].clone()\n",
    "    features_dict['LSTM SEQ 40'] = features.clone()\n",
    "    return metadata, features_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_models(checkpoints: dict, s3_client, device):\n",
    "    \"\"\"_summary_\n",
    "    Downloads models from s3 and loads them into memory.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for name, checkpoint in checkpoints.items():\n",
    "        logger.info(f'Downloading model: {name}')\n",
    "        response = s3_client.get_object(\n",
    "            Bucket=S3_BUCKET,\n",
    "            Key=checkpoint\n",
    "        )\n",
    "        buffer = io.BytesIO(response['Body'].read())\n",
    "        state = torch.load(buffer, map_location=torch.device(device))\n",
    "        model = LSTMOrdinal()\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        model.to(device)\n",
    "        models[name] = model\n",
    "    return models\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_static_predictions(args):\n",
    "    \n",
    "    user_metadata_container = []\n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    logger.info('Generating static prediction likelihoods for experiment')\n",
    "        \n",
    "    write_path = os.path.join(args.write_path, f'files_used_{args.n_files}_window_{args.data_window}')\n",
    "    file_path = os.path.join(args.file_path, f'files_used_{args.n_files}_window_{args.data_window}')\n",
    "    logger.info(\n",
    "        f'Writing to: {write_path}: reading from {file_path}'\n",
    "    )\n",
    "    npz_extractor = NPZExtractor(\n",
    "        args.file_path,\n",
    "        args.n_files,\n",
    "        args.n_sequences,\n",
    "        client,\n",
    "        10\n",
    "           \n",
    "    )\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'   \n",
    "    logger.info(f'Setting device to {device}')\n",
    "    \n",
    "    logger.info('generating dataset pointer')\n",
    "    dataset = npz_extractor.get_dataset_pointer()\n",
    "    \n",
    "    logger.info('Downloading model checkpoint')\n",
    "    \n",
    "    write_path = os.path.join(args.write_path, f'files_used_{args.n_files}_window_{args.data_window}')\n",
    "    if not os.path.exists(write_path):\n",
    "        logger.info(f'Creating directory: {write_path}')\n",
    "        os.makedirs(write_path)\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    if args.data_window == 10:\n",
    "        lstm_checkpoints = LSTM_CHECKPOINTS_10.copy()\n",
    "    elif args.data_window == 20:\n",
    "        lstm_checkpoints = LSTM_CHECKPOINTS_20.copy()\n",
    "    else:\n",
    "        lstm_checkpoints = LSTM_CHECKPOINTS_30.copy()\n",
    "    \n",
    "    logger.info(f'Downloading models from checkpoints {lstm_checkpoints.keys()}: window: {args.data_window}')\n",
    "    \n",
    "    models = get_models(lstm_checkpoints, client, device)\n",
    "    \n",
    "    dataset = ClickstreamDataset(dataset)\n",
    "    loader = DataLoader(dataset, batch_size=2048*4, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    activation = nn.Sigmoid()\n",
    "    \n",
    "\n",
    "    p_bar = tqdm.tqdm(loader, total=len(loader))\n",
    "    \n",
    "    for indx, data in enumerate(p_bar):\n",
    "        p_bar.set_description(f'Processing batch: {indx}')\n",
    "        data = data.to(device)\n",
    "        \n",
    "        metadata, features_dict = _extract_features(data, args.n_sequences + 1, 22)\n",
    "        preds_1 = activation(models['LSTM SEQ 1'](features_dict['LSTM SEQ 1']))\n",
    "        preds_10 = activation(models['LSTM SEQ 10'](features_dict['LSTM SEQ 10']))\n",
    "        preds_20 = activation(models['LSTM SEQ 20'](features_dict['LSTM SEQ 20']))\n",
    "        preds_30 = activation(models['LSTM SEQ 30'](features_dict['LSTM SEQ 30']))\n",
    "        # preds_30_h = activation(models['LSTM SEQ 30 H'](features_dict['LSTM SEQ 30 H']))\n",
    "        preds_40 = activation(models['LSTM SEQ 40'](features_dict['LSTM SEQ 40']))\n",
    "       \n",
    "        user_metadata = torch.cat((\n",
    "                metadata,\n",
    "                features_dict['LSTM SEQ 1'].squeeze(),\n",
    "                preds_1,\n",
    "                preds_10,\n",
    "                preds_20,\n",
    "                preds_30,\n",
    "                # preds_30_h,\n",
    "                preds_40),\n",
    "            1\n",
    "        )\n",
    "        \n",
    "        user_metadata_container.append(user_metadata)\n",
    "\n",
    "   \n",
    "    predicted_data = torch.cat(user_metadata_container, dim=0)\n",
    "    logger.info(f'Predicted data shape: {predicted_data.shape}: generating df')\n",
    "    predicted_data = pd.DataFrame(predicted_data.cpu().numpy(), columns=LABEL + METADATA + OUT_FEATURE_COLUMNS + list(models.keys()))\n",
    "    logger.info(f'Decoding date time data: merging to date time: {predicted_data.shape}')\n",
    "   \n",
    "    predicted_data = predicted_data.sort_values(by=['year', 'month', 'day', 'hour', 'minute', 'second'])\n",
    "    write_path = os.path.join(write_path, f'data_for_auc_window_{args.data_window}')\n",
    "    logger.info(f'Writing to parquet: {write_path}')\n",
    "    cols = ['label', 'user_id', 'session_30_raw', 'cum_session_time_raw', 'glob_session_time_raw'] + [col for col in predicted_data.columns if 'LSTM' in col]\n",
    "    predicted_data = predicted_data[cols]\n",
    "    logger.info(f'Writing to parquet: {write_path}')\n",
    "    predicted_data.to_parquet(write_path, index=False)\n",
    "    logger.info(f'Generating evaluation dataset')\n",
    "    predicted_data_eval = predicted_data[int(len(predicted_data) * 0.7):]\n",
    "    predicted_data_path = f'{write_path}_eval'\n",
    "    logger.info(f'Writing predicted data to: {predicted_data_path}: shape: {predicted_data_eval.shape}')\n",
    "    predicted_data_eval.to_parquet(predicted_data_path, index=False)\n",
    "    # return predicted_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    n_files = 30\n",
    "    n_sequences = 40\n",
    "    file_path = 'torch_ready_data'\n",
    "    write_path = 'rl_ready_data'\n",
    "    data_window = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 05:23:56,913 Generating static prediction likelihoods for experiment\n",
      "2023-07-12 05:23:56,914 Writing to: rl_ready_data/files_used_30_window_20: reading from torch_ready_data/files_used_30_window_20\n",
      "2023-07-12 05:23:56,914 Setting device to cuda\n",
      "2023-07-12 05:23:56,915 generating dataset pointer\n",
      "2023-07-12 05:23:56,915 Loading pointer to dataset: torch_ready_data/files_used_30_window_10/sequence_index_0: derived from torch_ready_data/files_used_30_window_10/sequence_index_0.npz\n",
      "2023-07-12 05:23:56,916 Loading pointer to dataset: torch_ready_data/files_used_30_window_10/sequence_index_10: derived from torch_ready_data/files_used_30_window_10/sequence_index_10.npz\n",
      "2023-07-12 05:23:56,917 Loading pointer to dataset: torch_ready_data/files_used_30_window_10/sequence_index_20: derived from torch_ready_data/files_used_30_window_10/sequence_index_20.npz\n",
      "2023-07-12 05:23:56,917 Loading pointer to dataset: torch_ready_data/files_used_30_window_10/sequence_index_30: derived from torch_ready_data/files_used_30_window_10/sequence_index_30.npz\n",
      "2023-07-12 05:23:56,918 Loading pointer to dataset: torch_ready_data/files_used_30_window_10/sequence_index_40: derived from torch_ready_data/files_used_30_window_10/sequence_index_40.npz\n",
      "2023-07-12 05:23:56,918 Loading: torch_ready_data/files_used_30_window_10/sequence_index_0/arr_0.npy\n",
      "2023-07-12 05:23:56,923 Loading: torch_ready_data/files_used_30_window_10/sequence_index_10/arr_0.npy\n",
      "2023-07-12 05:23:56,926 Loading: torch_ready_data/files_used_30_window_10/sequence_index_20/arr_0.npy\n",
      "2023-07-12 05:23:56,929 Loading: torch_ready_data/files_used_30_window_10/sequence_index_30/arr_0.npy\n",
      "2023-07-12 05:23:56,932 Loading: torch_ready_data/files_used_30_window_10/sequence_index_40/arr_0.npy\n",
      "2023-07-12 05:23:56,934 Downloading model checkpoint\n",
      "2023-07-12 05:23:56,935 Creating directory: rl_ready_data/files_used_30_window_20\n",
      "2023-07-12 05:23:56,941 Downloading models from checkpoints dict_keys(['LSTM SEQ 1', 'LSTM SEQ 10', 'LSTM SEQ 20', 'LSTM SEQ 30', 'LSTM SEQ 40']): window: 20\n",
      "2023-07-12 05:23:56,941 Downloading model: LSTM SEQ 1\n",
      "2023-07-12 05:23:58,009 Downloading model: LSTM SEQ 10\n",
      "2023-07-12 05:23:58,175 Downloading model: LSTM SEQ 20\n",
      "2023-07-12 05:23:58,331 Downloading model: LSTM SEQ 30\n",
      "2023-07-12 05:23:59,140 Downloading model: LSTM SEQ 40\n",
      "Processing batch: 4699: 100%|██████████| 4700/4700 [03:35<00:00, 21.78it/s]\n",
      "2023-07-12 05:27:35,064 Predicted data shape: torch.Size([38500990, 40]): generating df\n",
      "2023-07-12 05:27:39,594 Decoding date time data: merging to date time: (38500990, 40)\n",
      "2023-07-12 05:28:02,824 Writing to parquet: rl_ready_data/files_used_30_window_20/data_for_auc_window_20\n",
      "2023-07-12 05:28:04,870 Writing to parquet: rl_ready_data/files_used_30_window_20/data_for_auc_window_20\n",
      "2023-07-12 05:28:10,845 Generating evaluation dataset\n",
      "2023-07-12 05:28:10,846 Writing predicted data to: rl_ready_data/files_used_30_window_20/data_for_auc_window_20_eval: shape: (11550297, 10)\n"
     ]
    }
   ],
   "source": [
    "generate_static_predictions(Arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: rl_ready_data/files_used_30_window_20/data_for_auc_window_20_eval to s3://dissertation-data-dmiller/rl_ready_data/files_used_30_window_20/data_for_auc_window_20_eval\n",
      "upload: rl_ready_data/files_used_30_window_20/data_for_auc_window_20 to s3://dissertation-data-dmiller/rl_ready_data/files_used_30_window_20/data_for_auc_window_20\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync rl_ready_data/ s3://dissertation-data-dmiller/rl_ready_data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
