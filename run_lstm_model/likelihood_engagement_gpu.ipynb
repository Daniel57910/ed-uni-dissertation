{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch python-dotenv boto3 --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load constant\n",
    "LABEL = [\n",
    "    \"session_terminates_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \n",
    "    \"cum_session_event_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \n",
    "    \"cum_platform_event_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time_minutes\",\n",
    "]\n",
    "\n",
    "DATE_TIME = [\n",
    "    \"date_time\",\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\",\n",
    "    \"timestamp_raw\",\n",
    "    \"date_hour_sin\",\n",
    "    \n",
    "    \"date_hour_cos\",\n",
    "    \"session_5_count\",\n",
    "    \"session_30_count\",\n",
    "    \n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time_minutes\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time_minutes\",\n",
    "    \"cum_platform_events\",\n",
    "    \n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"rolling_session_time\",\n",
    "    \n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"session_event_count\",\n",
    "]\n",
    "\n",
    "\n",
    "TORCH_LOAD_COLS = LABEL + METADATA + DATE_TIME + OUT_FEATURE_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data_module\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "LABEL_INDEX = 1\n",
    "TOTAL_EVENTS_INDEX = 2\n",
    "BATCHES = 1000000\n",
    "\n",
    "    \n",
    "class ClickstreamDataset(Dataset):\n",
    "    def __init__(self, dataset_pointer_list) -> None:\n",
    "        \"\"\"\n",
    "        Yield data in batches of BATCHES\n",
    "        \"\"\"\n",
    "        self.events = dataset_pointer_list\n",
    "        self.size = self.events[0].shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        events = [np.array([event[idx]]) for event in self.events]\n",
    "        return np.concatenate(events, axis=1)\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load torch_model_bases\n",
    "import torch \n",
    "from torch import nn\n",
    "N_FEATURES = 18\n",
    "class LSTMOrdinal(nn.Module):\n",
    "    def __init__(self,  hidden_size=32, dropout=0.2) -> None:\n",
    "        super(LSTMOrdinal, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=N_FEATURES,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.output = nn.Linear(\n",
    "            hidden_size,\n",
    "            1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1]\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load npz_extractor\n",
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "class NPZExtractor:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, input_path, n_files, n_sequences, s3_client, data_partition) -> None:\n",
    "        self.input_path = input_path\n",
    "        self.n_files = n_files\n",
    "        self.n_sequences = n_sequences\n",
    "        self.s3_client = s3_client\n",
    "        self.data_partition = data_partition\n",
    "\n",
    "\n",
    "    def get_dataset_pointer(self):\n",
    "\n",
    "        read_path = os.path.join(self.input_path, f'files_used_{self.n_files}')\n",
    "        if not os.path.exists(read_path):\n",
    "            self.logger.info(f'Creating directory: {read_path}')\n",
    "            os.makedirs(read_path)\n",
    "\n",
    "\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            key_zip, key_npy = (\n",
    "                os.path.join(read_path, f'sequence_index_{_}.npz'),\n",
    "                os.path.join(read_path, f'sequence_index_{_}')\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f'Loading pointer to dataset: {key_npy}: derived from {key_zip}')\n",
    "\n",
    "\n",
    "            if not os.path.exists(key_zip):\n",
    "                self.logger.info(f'Zip file to extract: {key_zip}: npy file to load: {key_npy}')\n",
    "                self.s3_client.download_file(\n",
    "                    'dissertation-data-dmiller',\n",
    "                    key_zip,\n",
    "                    key_zip\n",
    "                )\n",
    "            if not os.path.exists(key_npy):\n",
    "                self.logger.info(f'Zip file downloaded: {key_zip}: npy file to load: {key_npy}')\n",
    "\n",
    "                self._zip_extract(key_zip, key_npy)\n",
    "\n",
    "        lz_concatenated_results = self._lazy_concatenate()\n",
    "\n",
    "        if self.data_partition:\n",
    "            return [p[:self.data_partition] for p in lz_concatenated_results]\n",
    "        else:\n",
    "            return lz_concatenated_results\n",
    "\n",
    "\n",
    "    def _zip_extract(self, key_zip, key_npy):\n",
    "        self.logger.info(f'Extracting file: {key_zip} -> {key_npy}')\n",
    "\n",
    "        with zipfile.ZipFile(key_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=key_npy, members=['arr_0.npy'])\n",
    "\n",
    "        self.logger.info(f'Zip file exracted: {key_zip} -> {key_npy}/arr_0.npy')\n",
    "\n",
    "    def _lazy_concatenate(self):\n",
    "        lz_concat = []\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            path_to_load = os.path.join(self.input_path, f'files_used_{self.n_files}', f'sequence_index_{_}', f'arr_0.npy')\n",
    "            self.logger.info(f'Loading: {path_to_load}')\n",
    "            lz_concat.append(np.load(path_to_load, mmap_mode='r'))\n",
    "        return lz_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load likelihood_engagement_cpu\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import boto3\n",
    "from pprint import pformat\n",
    "from torch import nn\n",
    "import io\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "CHECK_COLS = LABEL + METADATA + DATE_TIME + ['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.set_printoptions(sci_mode=False, linewidth=400, precision=2)\n",
    "np.set_printoptions(suppress=True, precision=4, linewidth=200)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    import cudf as pd\n",
    "    import pandas as cpu_pd\n",
    "    cpu_pd.set_option('display.max_columns', 500)\n",
    "    cpu_pd.set_option('display.width', 1000)\n",
    "    \n",
    "    import numpy as np\n",
    "else:\n",
    "    import pandas as pd\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.precision', 4)\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "\n",
    "CHECKPOINT_DIR='s3://dissertation-data-dmiller/lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_10/data_partition_None/2023_03_30_07_54'\n",
    "METADATA_INDEX = 12\n",
    "logger = logging.getLogger('likelihood_engagement')\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--n_files', type=int, default=2)\n",
    "    parser.add_argument('--n_sequences', type=int, default=10)\n",
    "    parser.add_argument('--file_path', type=str, default='datasets/torch_ready_data')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default=CHECKPOINT_DIR)\n",
    "    parser.add_argument('--write_path', type=str, default='datasets/lstm_predictions')\n",
    "    parser.add_argument('--model_type', type=str, default='ordinal')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def _extract_features(tensor, n_sequences, n_features):\n",
    "      \n",
    "    tensor = tensor.squeeze(1)\n",
    "    metadata, features = tensor[:, :METADATA_INDEX], tensor[:, METADATA_INDEX:] \n",
    "        \n",
    "    features = torch.flip(\n",
    "        torch.reshape(features, (features.shape[0], n_sequences, n_features)),\n",
    "        dims=[1]\n",
    "    )\n",
    "        \n",
    "    return metadata, features\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_static_predictions(args):\n",
    "    \n",
    "    user_metadata_container = []\n",
    "    \n",
    "    logger.info('Generating static prediction likelihoods for experiment')\n",
    "    npz_extractor = NPZExtractor(\n",
    "        args.file_path,\n",
    "        args.n_files,\n",
    "        args.n_sequences,\n",
    "        None,\n",
    "        None\n",
    "           \n",
    "    )\n",
    "    \n",
    "    logger.info('generating dataset pointer')\n",
    "    dataset = npz_extractor.get_dataset_pointer()\n",
    "    \n",
    "    \n",
    "    logger.info('Downloading model checkpoint')\n",
    "    \n",
    "    write_path = os.path.join(args.write_path, f'files_used_{args.n_files}/{args.model_type}_seq_{args.n_sequences}')\n",
    "    if not os.path.exists(write_path):\n",
    "        logger.info(f'Creating directory: {write_path}')\n",
    "        os.makedirs(write_path)\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    checkpoint = client.get_object(\n",
    "        Bucket='dissertation-data-dmiller',\n",
    "        Key='lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_10/data_partition_None/2023_03_30_07_54/clickstream-epoch=83-loss_valid=0.29.ckpt'\n",
    "    )\n",
    "    \n",
    "\n",
    "    logger.info('Loading model checkpoint')\n",
    "    \n",
    "    buffer = io.BytesIO(checkpoint['Body'].read())\n",
    "    map_location = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "   \n",
    "    logger.info('checkpoint loaded from buffer. Loading model')\n",
    "    model_state = torch.load(buffer, map_location=torch.device(map_location))\n",
    "    model = LSTMOrdinal()\n",
    "    model.load_state_dict(model_state['state_dict'])\n",
    "    model = model.cuda() if torch.cuda.is_available() else model.cpu()\n",
    "    logger.info(f'Model loaded. Creating dataset: n_events {dataset[0].shape[0]}')\n",
    "    \n",
    "    dataset = ClickstreamDataset(dataset)\n",
    "    logger.info(f'Dataset created. Creating loader')\n",
    "    loader = DataLoader(dataset, batch_size=65536, shuffle=False)\n",
    "    \n",
    "\n",
    "    p_bar = tqdm.tqdm(loader)\n",
    "    \n",
    "    activation_fx = nn.Sigmoid().cuda()\n",
    "    \n",
    "    for indx, data in enumerate(p_bar):\n",
    "        p_bar.set_description(f'Processing batch: {indx}')\n",
    "        metadata, features = _extract_features(data, args.n_sequences + 1, 18)\n",
    "        metadata, features = metadata.cuda(), features.cuda()\n",
    "        user_metadata = metadata[:, :4]\n",
    "        preds = model(features)\n",
    "        preds = activation_fx(preds)\n",
    "        user_metadata = torch.cat([user_metadata, preds], dim=1)\n",
    "        user_metadata_container.append(user_metadata.cpu().numpy())\n",
    "\n",
    "   \n",
    "    user_metadata = np.concatenate(user_metadata_container, axis=0)\n",
    "    user_metadata = pd.DataFrame(user_metadata, columns=['user_label', 'user_id', 'session_id', 'event_id', 'prediction'])\n",
    "    \n",
    "    \n",
    "   \n",
    "    logger.info(f'Writing predictions to {write_path}/predictions.parquet')\n",
    " \n",
    "    user_metadata.to_parquet(f'{write_path}/predictions.parquet')\n",
    "\n",
    "\n",
    "def join_pred_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_files', type=int, default=2)\n",
    "    parser.add_argument('--n_sequences', type=int, default=10)\n",
    "    parser.add_argument('--model_type', type=str, default='ordinal')\n",
    "    parser.add_argument('--rl_data', type=str, default='datasets/rl_ready_data')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def join_predictions_on_original(args):\n",
    "    predictions_path = f'lstm_predictions/files_used_{args.n_files}/{args.model_type}_seq_{args.n_sequences}/predictions.parquet'\n",
    "    dataset_path = f'calculated_features/files_used_{args.n_files}.parquet'\n",
    "    if not torch.cuda.is_available():\n",
    "        predictions_path, dataset_path = (\n",
    "            os.path.join('datasets', predictions_path),\n",
    "            os.path.join('datasets', dataset_path)\n",
    "        )\n",
    "    logger.info(f'Loading predictions from {predictions_path}')\n",
    "    logger.info(f'Loading dataset from {dataset_path}')\n",
    "    \n",
    "    predictions, original = (\n",
    "        pd.read_parquet(predictions_path),\n",
    "        pd.read_parquet(dataset_path, columns=TORCH_LOAD_COLS)\n",
    "    )\n",
    "    \n",
    "    predictions = predictions.rename(columns={\n",
    "        'session_id': 'session_30_raw',\n",
    "        'event_id': 'cum_session_event_raw'\n",
    "    })\n",
    "    \n",
    "    logger.info(f'Shape of predictions: {predictions.shape}')\n",
    "    logger.info(f'Shape of original: {original.shape}')\n",
    "    \n",
    "    \n",
    "    logger.info(f'Joining predictions on original dataset')\n",
    "\n",
    "    predictions = predictions.set_index(['user_id', 'session_30_raw', 'cum_session_event_raw']) \\\n",
    "        .join(original.set_index(['user_id', 'session_30_raw', 'cum_session_event_raw'])) \\\n",
    "        .reset_index() \\\n",
    "        .drop(columns=['user_label'])\n",
    "    \n",
    "    \n",
    "    logger.info(f'Predictions joined: {predictions.shape}: columns')\n",
    "    logger.info(pformat(predictions.columns.tolist()))\n",
    "    \n",
    "    logger.info(predictions[CHECK_COLS].head(10))\n",
    "    \n",
    "    write_path = os.path.join(\n",
    "        args.rl_data,\n",
    "        f'files_used_{args.n_files}',\n",
    "        f'{args.model_type}_seq_{args.n_sequences}'\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(write_path):\n",
    "        logger.info(f'Creating directory: {write_path}')\n",
    "        os.makedirs(write_path)\n",
    "    \n",
    "    logger.info(f'Writing joined predictions to {write_path}/rl_ready_data.parquet.gzip')\n",
    "    predictions.to_parquet(f'{write_path}/rl_ready_data.parquet')\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticPredArgs:\n",
    "    n_files = 30\n",
    "    n_sequences = 10\n",
    "    file_path = 'torch_ready_data'\n",
    "    checkpoint_dir = CHECKPOINT_DIR\n",
    "    write_path = 'lstm_predictions'\n",
    "    model_type = 'ordinal'\n",
    "    \n",
    "class JoinedPredArgs:\n",
    "    n_files = 30\n",
    "    n_sequences = 10\n",
    "    model_type = 'ordinal'\n",
    "    rl_data = 'rl_ready_data'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 15:47:02,166 Generating static prediction likelihoods for experiment\n",
      "2023-04-24 15:47:02,167 generating dataset pointer\n",
      "2023-04-24 15:47:02,168 Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_0: derived from torch_ready_data/files_used_30/sequence_index_0.npz\n",
      "2023-04-24 15:47:02,170 Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_10: derived from torch_ready_data/files_used_30/sequence_index_10.npz\n",
      "2023-04-24 15:47:02,171 Loading: torch_ready_data/files_used_30/sequence_index_0/arr_0.npy\n",
      "2023-04-24 15:47:02,211 Loading: torch_ready_data/files_used_30/sequence_index_10/arr_0.npy\n",
      "2023-04-24 15:47:02,247 Downloading model checkpoint\n",
      "2023-04-24 15:47:02,249 Creating directory: lstm_predictions/files_used_30/ordinal_seq_10\n",
      "2023-04-24 15:47:02,263 Found credentials in environment variables.\n",
      "2023-04-24 15:47:03,166 Loading model checkpoint\n",
      "2023-04-24 15:47:03,397 checkpoint loaded from buffer. Loading model\n",
      "2023-04-24 15:47:04,627 Model loaded. Creating dataset: n_events 38500990\n",
      "2023-04-24 15:47:04,628 Dataset created. Creating loader\n",
      "Processing batch: 587: 100%|██████████| 588/588 [08:19<00:00,  1.18it/s]\n",
      "2023-04-24 15:55:24,791 Writing predictions to lstm_predictions/files_used_30/ordinal_seq_10/predictions.parquet\n"
     ]
    }
   ],
   "source": [
    "generate_static_predictions(StaticPredArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 15:55:25,668 Loading predictions from lstm_predictions/files_used_30/ordinal_seq_10/predictions.parquet\n",
      "2023-04-24 15:55:25,669 Loading dataset from calculated_features/files_used_30.parquet\n",
      "2023-04-24 15:55:32,964 Shape of predictions: (38500990, 5)\n",
      "2023-04-24 15:55:32,965 Shape of original: (38500990, 27)\n",
      "2023-04-24 15:55:32,966 Joining predictions on original dataset\n",
      "/opt/conda/envs/rapids/lib/python3.9/site-packages/cudf/core/join/_join_helpers.py:105: UserWarning: Can't safely cast column from int64 to float32, upcasting to float64.\n",
      "  warnings.warn(\n",
      "2023-04-24 15:55:33,389 Predictions joined: (38500990, 28): columns\n",
      "2023-04-24 15:55:33,391 ['user_id',\n",
      " 'session_30_raw',\n",
      " 'cum_session_event_raw',\n",
      " 'prediction',\n",
      " 'session_terminates_30_minutes',\n",
      " 'cum_session_time_raw',\n",
      " 'cum_platform_event_raw',\n",
      " 'global_events_user',\n",
      " 'global_session_time_minutes',\n",
      " 'date_time',\n",
      " 'country_count',\n",
      " 'timestamp_raw',\n",
      " 'date_hour_sin',\n",
      " 'date_hour_cos',\n",
      " 'session_5_count',\n",
      " 'session_30_count',\n",
      " 'cum_session_event_count',\n",
      " 'delta_last_event',\n",
      " 'cum_session_time_minutes',\n",
      " 'expanding_click_average',\n",
      " 'cum_platform_time_minutes',\n",
      " 'cum_platform_events',\n",
      " 'cum_projects',\n",
      " 'average_event_time',\n",
      " 'rolling_session_time',\n",
      " 'rolling_session_events',\n",
      " 'rolling_session_gap',\n",
      " 'session_event_count']\n",
      "2023-04-24 15:55:33,393    session_terminates_30_minutes  user_id  session_30_raw  cum_session_event_raw  cum_session_time_raw  cum_platform_event_raw  global_events_user  global_session_time_minutes           date_time  prediction\n",
      "0                           True  16077.0             1.0                  332.0             32.216667                     332                 675                    64.466667 2021-10-19 11:09:13    0.982155\n",
      "1                          False  20090.0             1.0                   17.0              0.916667                      17               15064                  2104.950000 2021-10-19 11:09:13    0.326929\n",
      "2                           True     63.0             2.0                   49.0              1.716667                     259               57392                  5638.233333 2021-10-19 11:09:13    0.926653\n",
      "3                           True     63.0             2.0                   50.0              1.733333                     260               57392                  5638.233333 2021-10-19 11:09:14    0.923478\n",
      "4                          False   9010.0             1.0                  217.0             86.783333                     217                 424                   175.033333 2021-10-19 11:09:14    0.140549\n",
      "5                           True   9958.0             1.0                   70.0             80.350000                      70                 776                   329.766667 2021-10-19 11:09:14    0.988496\n",
      "6                           True  17744.0             1.0                   87.0             18.216667                      87               30075                  8347.716667 2021-10-19 11:09:14    0.922357\n",
      "7                           True  20003.0             1.0                   42.0              1.416667                      42                1386                   142.383333 2021-10-19 11:09:14    0.967290\n",
      "8                          False  20090.0             1.0                   18.0              0.950000                      18               15064                  2104.950000 2021-10-19 11:09:15    0.346245\n",
      "9                           True     63.0             2.0                   51.0              1.766667                     261               57392                  5638.233333 2021-10-19 11:09:16    0.922651\n",
      "2023-04-24 15:55:33,403 Creating directory: rl_ready_data/files_used_30/ordinal_seq_10\n",
      "2023-04-24 15:55:33,405 Writing joined predictions to rl_ready_data/files_used_30/ordinal_seq_10/rl_ready_data.parquet.gzip\n"
     ]
    }
   ],
   "source": [
    "join_predictions_on_original(JoinedPredArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: rl_ready_data/files_used_10/ordinal_seq_10/rl_ready_data.parquet to s3://dissertation-data-dmiller/rl_ready_data/files_used_10/ordinal_seq_10/rl_ready_data.parquet\n",
      "upload: rl_ready_data/files_used_30/ordinal_seq_10/rl_ready_data.parquet to s3://dissertation-data-dmiller/rl_ready_data/files_used_30/ordinal_seq_10/rl_ready_data.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive rl_ready_data/ s3://dissertation-data-dmiller/rl_ready_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install awscli -q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
