{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113 --quiet\n",
    "!python -m pip install boto3 python-dotenv gputil --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time\",\n",
    "    \n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "LOAD_COLS = LABEL + METADATA + OUT_FEATURE_COLUMNS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf as gpu_pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as cpu_np\n",
    "import cupy as gpu_np\n",
    "import pandas as cpu_pd\n",
    "from pprint import pformat\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import os\n",
    "import numpy \n",
    "import logging\n",
    "import GPUtil\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionizeData:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, df, max_sequence_index, write_path, load_cols, feature_cols, grouper, save_s3=True):\n",
    "        self.df = df\n",
    "        self.max_sequence_index = max_sequence_index + 1\n",
    "        self.min_sequence_index = self.max_sequence_index - 10\n",
    "        self.device = self._device()\n",
    "        self.sequences = cpu_np.arange(self.min_sequence_index, self.max_sequence_index).tolist()\n",
    "        self.seq_container = []\n",
    "        self.torch_sequences = None\n",
    "        self.output_path = write_path\n",
    "        self.save_s3 = save_s3\n",
    "        self.load_columns = load_cols\n",
    "        self.feature_cols = feature_cols\n",
    "        self.grouper = grouper\n",
    "        \n",
    "\n",
    "    def _device(self):\n",
    "        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def _sequence_lazy(self):\n",
    "         return next(self._lazy_load_shifted_index())\n",
    "\n",
    "    def _shifters(self):\n",
    "        for _ in range(self.min_sequence_index, self.max_sequence_index):\n",
    "            self.logger.info(f'Loading sequence: {_} -> {self.max_sequence_index}')\n",
    "            self.seq_container.append(self._sequence_lazy())\n",
    "        if torch.cuda.is_available():\n",
    "            GPUtil.showUtilization()\n",
    "\n",
    "        sequences = torch.cat(self.seq_container, dim=1).half()\n",
    "        return sequences\n",
    "\n",
    "    def generate_sequence(self):\n",
    "\n",
    "        self.logger.info(f'Generating shifted clickstreams from {self.min_sequence_index} -> {self.max_sequence_index}')\n",
    "        sequence = self._shifters()\n",
    "\n",
    "        self.logger.info(f'Shifter shape: {sequence.shape}')\n",
    "        \n",
    "\n",
    "        self.logger.info(f'Loading intial clickstream to {self.device}')\n",
    "\n",
    "        if self.max_sequence_index == 11:\n",
    "            self.logger.info('Initial clickstream writing to disk')\n",
    "            initial_clickstream = self.df[self.load_columns]\n",
    "            print(initial_clickstream.columns)\n",
    "            print(initial_clickstream.shape)\n",
    "            initial_clickstream = self.df[self.load_columns].values.astype(gpu_np.float32)\n",
    "            \n",
    "            self.logger.info(f'Initial clickstream shape: {initial_clickstream.shape}')\n",
    "            self._sequence_to_disk(initial_clickstream, 0)\n",
    "\n",
    "        self.logger.info(f'Writing sequence to disk: {self.max_sequence_index - 1}')\n",
    "        self._sequence_to_disk(sequence.cpu().numpy(), self.max_sequence_index - 1)\n",
    "\n",
    "\n",
    "    def _sequence_to_disk(self, partition, sequence_index):\n",
    "        if self.save_s3:\n",
    "            s3_client = boto3.client(\n",
    "                's3',\n",
    "                aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "                aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "            )\n",
    "\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "\n",
    "        partition_path = os.path.join(self.output_path, f'sequence_index_{sequence_index}.npz')\n",
    "        self.logger.info(f'Saving to disk: {partition_path}')\n",
    "        numpy.savez_compressed(partition_path, partition)\n",
    "\n",
    "        if self.save_s3:\n",
    "            self.logger.info(f'Uploading to s3: dissertation-data-dmiller/{partition_path}')\n",
    "            s3_client.upload_file(partition_path, 'dissertation-data-dmiller', partition_path)\n",
    "\n",
    "    def _lazy_load_shifted_index(self):\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        indx = self.sequences.pop(0)\n",
    "        torch_container = []\n",
    "        for col in self.feature_cols:\n",
    "            sequence = self.df.groupby(self.grouper)[col].shift(indx).fillna(0).values.astype(gpu_np.float16)\n",
    "            sequence_tensor = torch.tensor(sequence).to(self.device).half()\n",
    "            torch_container.append(sequence_tensor.unsqueeze(1))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        yield torch.cat(torch_container, dim=1).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "torch.set_printoptions(sci_mode=False, precision=4, linewidth=400)\n",
    "\n",
    "cpu_np.set_printoptions(suppress=True)\n",
    "cpu_np.set_printoptions(precision=4)\n",
    "\n",
    "def get_logger():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def scale_feature_cols(df, scaler, scaler_columns):\n",
    "    df[scaler_columns] = scaler.fit_transform(df[scaler_columns])\n",
    "    return df\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    logger = get_logger()\n",
    "    logger.info('Starting sessionize_users_cpu.py with arguments')\n",
    "    logger.info(pformat(args.__dict__))\n",
    "    \n",
    "    data_read = os.path.join(args.input_path, f'files_used_{args.data_subset}')\n",
    "\n",
    "    logger.info(f'Reading data from {data_read}')\n",
    "    scaler_cols = [\n",
    "        col for col in OUT_FEATURE_COLUMNS if 'sin' not in col and 'cos' not in col\n",
    "    ]\n",
    "\n",
    "    df = gpu_pd.read_parquet(data_read, columns=LOAD_COLS + ['date_time'])\n",
    "    logger.info(f'Data read: {df.shape}')\n",
    "    logger.info('Casting date time and sorting by date time')\n",
    "    df['date_time'] = gpu_pd.to_datetime(df['date_time'])\n",
    "    df = df.sort_values(by=['date_time'])\n",
    "    logger.info('Data read: scaling scaler columns')\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    df = df.to_pandas()\n",
    "    logger.info(f'Shape before features: {df.shape}')\n",
    "    df[scaler_cols] = scaler.fit_transform(df[scaler_cols])\n",
    "    df = gpu_pd.from_pandas(df)\n",
    "    logger.info(f'Shape after features: {df.shape}')\n",
    "\n",
    "    logger.info('Scaling complete: implement sessionize')\n",
    "    \n",
    "    for seq_index in args.seq_list:\n",
    "\n",
    "        sessionize = SessionizeData(\n",
    "            df,\n",
    "            seq_index,\n",
    "            os.path.join(args.output_path, f'files_used_{args.data_subset}'),\n",
    "            LOAD_COLS,\n",
    "            OUT_FEATURE_COLUMNS,\n",
    "            GROUPBY_COLS,\n",
    "            args.save_s3\n",
    "        )\n",
    "    \n",
    "        logger.info(f'Generating sequence for {seq_index}')\n",
    "        sessionize.generate_sequence()\n",
    "    \n",
    "    logger.info(f'Sessionize complete for sequences {args.seq_list}')\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 19:05:07,332 - __main__ - INFO - Starting sessionize_users_cpu.py with arguments\n",
      "2023-04-28 19:05:07,333 - __main__ - INFO - {'data_subset': 30}\n",
      "2023-04-28 19:05:07,334 - __main__ - INFO - Reading data from calculated_features/files_used_30\n",
      "2023-04-28 19:05:09,633 - __main__ - INFO - Data read: (38500990, 35)\n",
      "2023-04-28 19:05:09,634 - __main__ - INFO - Casting date time and sorting by date time\n",
      "2023-04-28 19:05:09,713 - __main__ - INFO - Data read: scaling scaler columns\n",
      "2023-04-28 19:05:20,329 - __main__ - INFO - Shape before features: (38500990, 35)\n",
      "2023-04-28 19:05:46,806 - __main__ - INFO - Shape after features: (38500990, 35)\n",
      "2023-04-28 19:05:46,807 - __main__ - INFO - Scaling complete: implement sessionize\n",
      "2023-04-28 19:05:46,809 - __main__ - INFO - Generating sequence for 10\n",
      "2023-04-28 19:05:46,810 - __main__ - INFO - Generating shifted clickstreams from 1 -> 11\n",
      "2023-04-28 19:05:46,810 - __main__ - INFO - Loading sequence: 1 -> 11\n",
      "2023-04-28 19:05:54,267 - __main__ - INFO - Loading sequence: 2 -> 11\n",
      "2023-04-28 19:05:57,378 - __main__ - INFO - Loading sequence: 3 -> 11\n",
      "2023-04-28 19:06:00,520 - __main__ - INFO - Loading sequence: 4 -> 11\n",
      "2023-04-28 19:06:03,625 - __main__ - INFO - Loading sequence: 5 -> 11\n",
      "2023-04-28 19:06:06,747 - __main__ - INFO - Loading sequence: 6 -> 11\n",
      "2023-04-28 19:06:09,853 - __main__ - INFO - Loading sequence: 7 -> 11\n",
      "2023-04-28 19:06:13,018 - __main__ - INFO - Loading sequence: 8 -> 11\n",
      "2023-04-28 19:06:16,177 - __main__ - INFO - Loading sequence: 9 -> 11\n",
      "2023-04-28 19:06:19,285 - __main__ - INFO - Loading sequence: 10 -> 11\n",
      "2023-04-28 19:06:22,565 - __main__ - INFO - Shifter shape: torch.Size([38500990, 200])\n",
      "2023-04-28 19:06:22,566 - __main__ - INFO - Loading intial clickstream to cuda\n",
      "2023-04-28 19:06:22,566 - __main__ - INFO - Initial clickstream writing to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 37% | 33% |\n",
      "Index(['continue_work_session_30_minutes', 'user_id', 'session_30_raw',\n",
      "       'cum_platform_event_raw', 'cum_platform_time_raw',\n",
      "       'cum_session_time_raw', 'global_events_user', 'global_session_time',\n",
      "       'year', 'month', 'day', 'hour', 'minute', 'second', 'country_count',\n",
      "       'date_hour_sin', 'date_hour_cos', 'date_minute_sin', 'date_minute_cos',\n",
      "       'session_30_count', 'session_5_count', 'cum_session_event_count',\n",
      "       'delta_last_event', 'cum_session_time', 'expanding_click_average',\n",
      "       'cum_platform_time', 'cum_platform_events', 'cum_projects',\n",
      "       'average_event_time', 'rolling_session_time', 'rolling_session_events',\n",
      "       'rolling_session_gap', 'previous_session_time',\n",
      "       'previous_session_events'],\n",
      "      dtype='object')\n",
      "(38500990, 34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 19:06:22,761 - __main__ - INFO - Initial clickstream shape: (38500990, 34)\n",
      "2023-04-28 19:06:22,826 - __main__ - INFO - Saving to disk: torch_ready_data/files_used_30/sequence_index_0.npz\n",
      "2023-04-28 19:08:45,290 - __main__ - INFO - Uploading to s3: dissertation-data-dmiller/torch_ready_data/files_used_30/sequence_index_0.npz\n",
      "2023-04-28 19:09:09,351 - __main__ - INFO - Writing sequence to disk: 10\n",
      "2023-04-28 19:09:19,909 - __main__ - INFO - Saving to disk: torch_ready_data/files_used_30/sequence_index_10.npz\n",
      "2023-04-28 19:13:25,880 - __main__ - INFO - Uploading to s3: dissertation-data-dmiller/torch_ready_data/files_used_30/sequence_index_10.npz\n",
      "2023-04-28 19:13:45,776 - __main__ - INFO - Generating sequence for 20\n",
      "2023-04-28 19:13:45,777 - __main__ - INFO - Generating shifted clickstreams from 11 -> 21\n",
      "2023-04-28 19:13:45,777 - __main__ - INFO - Loading sequence: 11 -> 21\n",
      "2023-04-28 19:13:49,007 - __main__ - INFO - Loading sequence: 12 -> 21\n",
      "2023-04-28 19:13:52,181 - __main__ - INFO - Loading sequence: 13 -> 21\n",
      "2023-04-28 19:13:55,370 - __main__ - INFO - Loading sequence: 14 -> 21\n",
      "2023-04-28 19:13:58,529 - __main__ - INFO - Loading sequence: 15 -> 21\n",
      "2023-04-28 19:14:01,655 - __main__ - INFO - Loading sequence: 16 -> 21\n",
      "2023-04-28 19:14:04,799 - __main__ - INFO - Loading sequence: 17 -> 21\n",
      "2023-04-28 19:14:07,928 - __main__ - INFO - Loading sequence: 18 -> 21\n",
      "2023-04-28 19:14:11,065 - __main__ - INFO - Loading sequence: 19 -> 21\n",
      "2023-04-28 19:14:14,198 - __main__ - INFO - Loading sequence: 20 -> 21\n",
      "2023-04-28 19:14:17,477 - __main__ - INFO - Shifter shape: torch.Size([38500990, 200])\n",
      "2023-04-28 19:14:17,478 - __main__ - INFO - Loading intial clickstream to cuda\n",
      "2023-04-28 19:14:17,479 - __main__ - INFO - Writing sequence to disk: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 95% | 33% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 19:14:27,865 - __main__ - INFO - Saving to disk: torch_ready_data/files_used_30/sequence_index_20.npz\n",
      "2023-04-28 19:18:19,614 - __main__ - INFO - Uploading to s3: dissertation-data-dmiller/torch_ready_data/files_used_30/sequence_index_20.npz\n",
      "2023-04-28 19:18:35,761 - __main__ - INFO - Generating sequence for 30\n",
      "2023-04-28 19:18:35,762 - __main__ - INFO - Generating shifted clickstreams from 21 -> 31\n",
      "2023-04-28 19:18:35,762 - __main__ - INFO - Loading sequence: 21 -> 31\n",
      "2023-04-28 19:18:39,010 - __main__ - INFO - Loading sequence: 22 -> 31\n",
      "2023-04-28 19:18:42,210 - __main__ - INFO - Loading sequence: 23 -> 31\n",
      "2023-04-28 19:18:45,417 - __main__ - INFO - Loading sequence: 24 -> 31\n",
      "2023-04-28 19:18:48,499 - __main__ - INFO - Loading sequence: 25 -> 31\n",
      "2023-04-28 19:18:51,590 - __main__ - INFO - Loading sequence: 26 -> 31\n",
      "2023-04-28 19:18:54,704 - __main__ - INFO - Loading sequence: 27 -> 31\n",
      "2023-04-28 19:18:57,824 - __main__ - INFO - Loading sequence: 28 -> 31\n",
      "2023-04-28 19:19:00,961 - __main__ - INFO - Loading sequence: 29 -> 31\n",
      "2023-04-28 19:19:04,080 - __main__ - INFO - Loading sequence: 30 -> 31\n",
      "2023-04-28 19:19:07,421 - __main__ - INFO - Shifter shape: torch.Size([38500990, 200])\n",
      "2023-04-28 19:19:07,422 - __main__ - INFO - Loading intial clickstream to cuda\n",
      "2023-04-28 19:19:07,423 - __main__ - INFO - Writing sequence to disk: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 17% | 33% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 19:19:17,734 - __main__ - INFO - Saving to disk: torch_ready_data/files_used_30/sequence_index_30.npz\n",
      "2023-04-28 19:23:01,430 - __main__ - INFO - Uploading to s3: dissertation-data-dmiller/torch_ready_data/files_used_30/sequence_index_30.npz\n",
      "2023-04-28 19:23:17,336 - __main__ - INFO - Generating sequence for 40\n",
      "2023-04-28 19:23:17,337 - __main__ - INFO - Generating shifted clickstreams from 31 -> 41\n",
      "2023-04-28 19:23:17,338 - __main__ - INFO - Loading sequence: 31 -> 41\n",
      "2023-04-28 19:23:20,631 - __main__ - INFO - Loading sequence: 32 -> 41\n",
      "2023-04-28 19:23:23,857 - __main__ - INFO - Loading sequence: 33 -> 41\n",
      "2023-04-28 19:23:27,035 - __main__ - INFO - Loading sequence: 34 -> 41\n",
      "2023-04-28 19:23:30,174 - __main__ - INFO - Loading sequence: 35 -> 41\n",
      "2023-04-28 19:23:33,298 - __main__ - INFO - Loading sequence: 36 -> 41\n",
      "2023-04-28 19:23:36,417 - __main__ - INFO - Loading sequence: 37 -> 41\n",
      "2023-04-28 19:23:39,534 - __main__ - INFO - Loading sequence: 38 -> 41\n",
      "2023-04-28 19:23:42,654 - __main__ - INFO - Loading sequence: 39 -> 41\n",
      "2023-04-28 19:23:45,789 - __main__ - INFO - Loading sequence: 40 -> 41\n",
      "2023-04-28 19:23:49,122 - __main__ - INFO - Shifter shape: torch.Size([38500990, 200])\n",
      "2023-04-28 19:23:49,124 - __main__ - INFO - Loading intial clickstream to cuda\n",
      "2023-04-28 19:23:49,125 - __main__ - INFO - Writing sequence to disk: 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 61% | 33% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 19:23:59,805 - __main__ - INFO - Saving to disk: torch_ready_data/files_used_30/sequence_index_40.npz\n",
      "2023-04-28 19:27:39,187 - __main__ - INFO - Uploading to s3: dissertation-data-dmiller/torch_ready_data/files_used_30/sequence_index_40.npz\n",
      "2023-04-28 19:28:02,560 - __main__ - INFO - Sessionize complete for sequences [10, 20, 30, 40]\n"
     ]
    }
   ],
   "source": [
    "class Arguments:\n",
    "    seq_list = [10, 20, 30, 40]\n",
    "    input_path = 'calculated_features'\n",
    "    output_path = 'torch_ready_data'\n",
    "    data_subset = None\n",
    "    save_s3 = True\n",
    "\n",
    "for data_subset in [30]:\n",
    "    args = Arguments()\n",
    "    args.data_subset = data_subset\n",
    "    df = main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1da7ff4cdcccf44e7e228c52b231f7d5c5854d5618af555ed3871fd5cba609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
