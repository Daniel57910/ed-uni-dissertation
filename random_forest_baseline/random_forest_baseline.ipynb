{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score\n",
    "import os\n",
    "# import xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv awscli --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.ensemble import RandomForestClassifier as cuRFC\n",
    "from cuml.preprocessing import MinMaxScaler as cuMinMaxScaler\n",
    "import cudf as gpu_pd\n",
    "import cupy as gpu_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant.py\n",
    "FEATURE_COLUMNS = [\n",
    "    \n",
    "    \"user_count\",\n",
    "    \"project_count\",\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"cum_session_time\",\n",
    "    \"expanding_click_average\",\n",
    "   \n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"delta_last_event\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "\n",
    "METADATA = [\n",
    "    \"date_time\"\n",
    "]\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'session_minutes',\n",
    "    'size_cutoff',\n",
    "    'time_cutoff',\n",
    "    'reward'\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    \"label\",\n",
    "]\n",
    "\n",
    "LOAD_COLS = list(set(FEATURE_COLUMNS + METADATA + PREDICTION_COLS))\n",
    "\n",
    "CORE_PATH = 'calculated_features'\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.85\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval_test_preds(eval_split, test_split, model, pred_window):\n",
    "    eval_features, eval_label = eval_split[FEATURE_COLUMNS], eval_split['label']\n",
    "    test_features, test_label = test_split[FEATURE_COLUMNS], test_split['label']\n",
    "    print('Collecting eval predictions')\n",
    "    eval_preds = model.predict(eval_features)\n",
    "    print('Collecting test predictions')\n",
    "    test_preds = model.predict(test_features)\n",
    "    print('Generating acc, prec, rec, auc metrics')\n",
    "    eval_preds, eval_label, test_preds, test_label = (\n",
    "        eval_preds.to_pandas(),\n",
    "        eval_label.to_pandas(),\n",
    "        test_preds.to_pandas(),\n",
    "        test_label.to_pandas()\n",
    "    )\n",
    "    \n",
    "    metric_container = []\n",
    "    \n",
    "    \n",
    "    for window, dataset in zip([\"EVAL\", \"TEST\"], [(eval_preds, eval_label), (test_preds, test_label)]):\n",
    "        for metric, fn in zip([\"ACC\", \"PREC\", \"REC\", \"AUC\"], [accuracy_score, precision_score, recall_score, roc_auc_score]):\n",
    "            print(f\"Generating {window} -> {metric}\")\n",
    "            metric_container.append(\n",
    "                {\n",
    "                    \"Metric Name\": f\"{metric} {window} {pred_window}\",\n",
    "                    \"Metric Value\": fn(dataset[0], dataset[1])\n",
    "                }\n",
    "            )\n",
    "                  \n",
    "            \n",
    "    return metric_container \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def generate_baseline_model(args):\n",
    "    df_container = []\n",
    "    n_files = args.n_files\n",
    "    for pred_window in [10, 20, 30]:\n",
    "        read_path = os.path.join(\n",
    "            CORE_PATH,\n",
    "            f'files_used_{n_files}',\n",
    "            f'calculated_features_window_{pred_window}.parquet'\n",
    "        )\n",
    "        assert os.path.exists(read_path), f'Path {read_path} does not exist'\n",
    "        print(f'Reading {read_path}')\n",
    "        \n",
    "        input_df = pd.read_parquet(read_path, columns=FEATURE_COLUMNS + METADATA + PREDICTION_COLS)\n",
    "\n",
    "        input_df['date_time'] = pd.to_datetime(input_df['date_time'])\n",
    "        print('Sorting by date_time')\n",
    "        input_df = input_df.sort_values(by=['date_time'])\n",
    "        input_df = input_df.dropna()\n",
    "        train_split, eval_split, test_split = (\n",
    "            input_df[:int(len(input_df)*TRAIN_SPLIT)], \n",
    "            input_df[int(len(input_df)*TRAIN_SPLIT):int(len(input_df)*EVAL_SPLIT)],\n",
    "            input_df[int(len(input_df)*EVAL_SPLIT):]\n",
    "        )\n",
    "\n",
    "        print(f'Train split: {train_split.shape[0]}, eval split: {eval_split.shape[0]}, test split: {test_split.shape[0]}')\n",
    "        print('Creating scalar range -1, 1')\n",
    "        min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        feature_cols_scaling = [col for col in FEATURE_COLUMNS if 'sin' not in col or 'cos' not in col]\n",
    "        print('Scaling features and fitting scalar')\n",
    "        train_split[feature_cols_scaling] = min_max_scaler.fit_transform(train_split[feature_cols_scaling])\n",
    "        print('Scaling evaluation features on fitted scalar')\n",
    "        eval_split[feature_cols_scaling] = min_max_scaler.transform(eval_split[feature_cols_scaling])\n",
    "        print('Scaling test features on fitted scalar')\n",
    "        test_split[feature_cols_scaling] = min_max_scaler.transform(test_split[feature_cols_scaling])\n",
    "        train_split, eval_split, test_split = (\n",
    "            train_split.dropna(),\n",
    "            eval_split.dropna(),\n",
    "            test_split.dropna()\n",
    "        )\n",
    "        \n",
    "        train_split, eval_split, test_split = (\n",
    "            gpu_pd.from_pandas(train_split),\n",
    "            gpu_pd.from_pandas(eval_split),\n",
    "            gpu_pd.from_pandas(test_split)\n",
    "        )\n",
    "\n",
    "        print(f'Data preprocessed and nan dropped: {train_split.shape[0]}, eval split: {eval_split.shape[0]}, test split: {test_split.shape[0]}')\n",
    "        model = cuRFC(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=RANDOM_STATE)\n",
    "        \n",
    "        print('Fitting model')\n",
    "        model.fit(train_split[FEATURE_COLUMNS], train_split['label'])\n",
    "        print('Model fitted: generating eval preds')\n",
    "        \n",
    "        metrics =  generate_eval_test_preds(eval_split, test_split, model, pred_window)\n",
    "        df_container.extend(metrics)\n",
    "    \n",
    "    df_metrics = pd.DataFrame(df_container)\n",
    "    df_metrics.to_csv(\"baseline_metrics_full_dat.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading calculated_features/files_used_30/calculated_features_window_10.parquet\n",
      "Sorting by date_time\n",
      "Train split: 26950693, eval split: 5775148, test split: 5775149\n",
      "Creating scalar range -1, 1\n",
      "Scaling features and fitting scalar\n",
      "Scaling evaluation features on fitted scalar\n",
      "Scaling test features on fitted scalar\n",
      "Data preprocessed and nan dropped: 26950693, eval split: 5775148, test split: 5775149\n",
      "Fitting model\n",
      "Model fitted: generating eval preds\n",
      "Collecting eval predictions\n",
      "Collecting test predictions\n",
      "Generating acc, prec, rec, auc metrics\n",
      "Generating EVAL -> ACC\n",
      "Generating EVAL -> PREC\n",
      "Generating EVAL -> REC\n",
      "Generating EVAL -> AUC\n",
      "Generating TEST -> ACC\n",
      "Generating TEST -> PREC\n",
      "Generating TEST -> REC\n",
      "Generating TEST -> AUC\n",
      "Reading calculated_features/files_used_30/calculated_features_window_20.parquet\n",
      "Sorting by date_time\n",
      "Train split: 26950693, eval split: 5775148, test split: 5775149\n",
      "Creating scalar range -1, 1\n",
      "Scaling features and fitting scalar\n",
      "Scaling evaluation features on fitted scalar\n",
      "Scaling test features on fitted scalar\n",
      "Data preprocessed and nan dropped: 26950693, eval split: 5775148, test split: 5775149\n",
      "Fitting model\n",
      "Model fitted: generating eval preds\n",
      "Collecting eval predictions\n",
      "Collecting test predictions\n",
      "Generating acc, prec, rec, auc metrics\n",
      "Generating EVAL -> ACC\n",
      "Generating EVAL -> PREC\n",
      "Generating EVAL -> REC\n",
      "Generating EVAL -> AUC\n",
      "Generating TEST -> ACC\n",
      "Generating TEST -> PREC\n",
      "Generating TEST -> REC\n",
      "Generating TEST -> AUC\n",
      "Reading calculated_features/files_used_30/calculated_features_window_30.parquet\n",
      "Sorting by date_time\n",
      "Train split: 26950693, eval split: 5775148, test split: 5775149\n",
      "Creating scalar range -1, 1\n",
      "Scaling features and fitting scalar\n",
      "Scaling evaluation features on fitted scalar\n",
      "Scaling test features on fitted scalar\n",
      "Data preprocessed and nan dropped: 26950693, eval split: 5775148, test split: 5775149\n",
      "Fitting model\n",
      "Model fitted: generating eval preds\n",
      "Collecting eval predictions\n",
      "Collecting test predictions\n",
      "Generating acc, prec, rec, auc metrics\n",
      "Generating EVAL -> ACC\n",
      "Generating EVAL -> PREC\n",
      "Generating EVAL -> REC\n",
      "Generating EVAL -> AUC\n",
      "Generating TEST -> ACC\n",
      "Generating TEST -> PREC\n",
      "Generating TEST -> REC\n",
      "Generating TEST -> AUC\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    n_files = 30\n",
    "\n",
    "generate_baseline_model(Args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
