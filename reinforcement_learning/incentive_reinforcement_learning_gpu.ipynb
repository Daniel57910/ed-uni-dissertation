{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip python-dotenv --quiet\n",
    "!python -m pip install gym stable-baselines3[extra] --upgrade --quiet\n",
    "!python -m pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant.py\n",
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time\",\n",
    "    \n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    'seq_10',\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'sim_size',\n",
    "    'session_minutes',\n",
    "    'sim_minutes',\n",
    "    'reward',\n",
    "    'session_30_raw',\n",
    "    'cum_platform_time_raw',\n",
    "    'global_session_time',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load callback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat\n",
    "\n",
    "\n",
    "class DistributionCallback(BaseCallback):\n",
    "    \n",
    "    @classmethod\n",
    "    def tensorboard_setup(cls, log_dir, log_freq):\n",
    "        cls._log_dir = log_dir\n",
    "        cls._log_freq = log_freq\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        output_formats = self.logger.output_formats\n",
    "        self.tb_formatter = next(f for f in output_formats if isinstance(f, TensorBoardOutputFormat))\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._log_freq == 0:\n",
    "            dist_list = self.training_env.env_method('dists')\n",
    "            values_to_log = np.concatenate([d for d in dist_list if d.shape[0] > 0])\n",
    "\n",
    "            values_df = pd.DataFrame(\n",
    "                values_to_log, \n",
    "                columns=RL_STAT_COLS + ['ended', 'incentive_index', 'n_episodes']\n",
    "            )\n",
    "            \n",
    "            dist_session_time = (values_df['session_minutes'] - values_df['reward']).mean()\n",
    "            dist_session_end = (values_df['session_size'] - values_df['ended']).mean()\n",
    "            \n",
    "            dist_sim_time = (values_df['reward'] - values_df['sim_size']).mean()\n",
    "            dist_sim_end = (values_df['ended'] - values_df['sim_size']).mean()\n",
    "\n",
    "\n",
    "            dist_inc_session = (values_df['session_size'] - values_df['incentive_index']).mean()\n",
    "            dist_inc_end = (values_df['ended'] - values_df['incentive_index']).mean()\n",
    "            dist_inc_sim_index = (values_df['sim_size'] - values_df['incentive_index']).mean()\n",
    "\n",
    "            n_call = self.n_calls // self._log_freq\n",
    "            \n",
    "            self.tb_formatter.writer.add_scalar('distance/session/max_reward::decrease', dist_session_time, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/session/max_ended::decrease', dist_session_end, n_call)\n",
    "           \n",
    "            self.tb_formatter.writer.add_scalar('distance/session/sim_reward::increase', dist_sim_time, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/session/sim_ended::increase', dist_sim_end, n_call)\n",
    "            \n",
    "            \n",
    "            self.tb_formatter.writer.add_scalar('distance/incentive/max_incentive::decrease', dist_inc_session, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/incentive/ended_incentive', dist_inc_end, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/incentive/sim_inc_placement::decrease', dist_inc_sim_index, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/ended_sim_size::increase', dist_inc_sim_index, n_call)\n",
    "            \n",
    "            self.tb_formatter.writer.flush()\n",
    "            \n",
    "            values_df.to_parquet(f'{self._log_dir}/dist_{n_call}.parquet')\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment\n",
    "# %load environment\n",
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "import gym\n",
    "\n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, dataset, unique_episodes, unique_sessions, out_features, n_sequences):\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.unique_episodes = unique_episodes\n",
    "        self.n_episodes = 0\n",
    "        self.n_sequences = n_sequences\n",
    "        self.unique_sessions = unique_sessions\n",
    "        self.current_session = None\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        self.metadata_container = []\n",
    "        self.n_sequences = n_sequences\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(len(out_features), n_sequences + 1), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_episodes += 1\n",
    "        session_to_run = self.unique_sessions.sample(1)['session_30_raw'].values[0]\n",
    "        user_to_run = self.unique_episodes[self.unique_episodes['session_30_raw'] == session_to_run].sample(1)['user_id'].values[0]\n",
    "        self.current_session = self._get_events(user_to_run, session_to_run)\n",
    "        self.metadata = self._metadata()\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        return self._state()\n",
    "\n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        next_state, done, meta = self._calculate_next_state()\n",
    "        \n",
    "        if done:\n",
    "            curent_session_index = min(self.current_session_index, self.current_session.shape[0] - 1)\n",
    "            self.metadata['ended'] = self.current_session.iloc[curent_session_index]['cum_session_event_raw']\n",
    "            self.metadata['reward'] = self.reward\n",
    "            self.metadata_container.append(self.metadata.values)\n",
    "            return next_state, float(self.reward), done, meta\n",
    "        else:\n",
    "            self.reward = self.current_session.iloc[self.current_session_index]['reward'] \n",
    "            self.current_session_index += 1        \n",
    "        return next_state, float(self.reward), done, meta\n",
    "    \n",
    "    def _metadata(self):\n",
    "        session_metadata = self.current_session.iloc[0][RL_STAT_COLS]\n",
    "        session_metadata['ended'] = 0\n",
    "        session_metadata['incentive_index'] = 0\n",
    "        session_metadata['n_episodes'] = self.n_episodes\n",
    "        return session_metadata\n",
    "    \n",
    "    \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        if (self.current_session_index == self.current_session.shape[0]):\n",
    "            return None, True, {}\n",
    "\n",
    "        if self._continuing_in_session():\n",
    "            return self._state(), False, {}\n",
    "    \n",
    "        return None, True, {}\n",
    "        \n",
    "      \n",
    "  \n",
    "    def _continuing_in_session(self):\n",
    "        sim_counts = self.metadata['sim_size']\n",
    "        current_session_count = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "        if current_session_count < sim_counts:\n",
    "            return True\n",
    "        \n",
    "        extending_session = self._probability_extending_session(current_session_count)\n",
    "        \n",
    "        return all([extending_session >= .3, extending_session <= .7])\n",
    "        \n",
    "    \n",
    "    def _probability_extending_session(self, current_session_count):\n",
    "        if self.metadata['incentive_index'] == 0:\n",
    "            return 0\n",
    "        \n",
    "        scale = max(5, int(self.metadata['session_size'] / 4))\n",
    "        continue_session = norm(\n",
    "            loc=self.metadata['incentive_index'],\n",
    "            scale=scale\n",
    "        ).cdf(current_session_count)\n",
    "        \n",
    "        return continue_session\n",
    "        \n",
    "\n",
    "    def _get_events(self, user_id, session):\n",
    "        subset = self.dataset[\n",
    "            (self.dataset['user_id'] == user_id) &\n",
    "            (self.dataset['session_30_raw'] == session)\n",
    "        ]\n",
    "   \n",
    "        return subset.sort_values(by=['date_time']).reset_index(drop=True)\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        if action == 0 or self.metadata['incentive_index'] > 0:\n",
    "            return\n",
    "        \n",
    "        current_session_index = min(self.current_session_index, self.current_session.shape[0] - 1)\n",
    "        self.metadata['incentive_index'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "        \n",
    "    def _state(self):\n",
    "\n",
    "        if self.current_session_index > self.n_sequences:\n",
    "            events = self.current_session.iloc[self.current_session_index - (self.n_sequences + 1):self.current_session_index][self.out_features].values\n",
    "            \n",
    "        else:\n",
    "            delta = min((self.n_sequences + 1)- self.current_session_index, 10)\n",
    "            zero_cat = np.zeros((delta, len(self.out_features)))\n",
    "            events = self.current_session.iloc[:max(self.current_session_index, 1)][self.out_features].values\n",
    "            events = np.concatenate((zero_cat, events), axis=0)\n",
    "            \n",
    "\n",
    "        return events.astype(np.float32).T\n",
    "  \n",
    "    \n",
    "    def dists(self):\n",
    "        metadata_container = self.metadata_container.copy()\n",
    "        self.metadata_container = []\n",
    "        return np.array(metadata_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load policies/cnn_policy\n",
    "from typing import Dict, List, Type, Union\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "from torch import nn\n",
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomConv1dFeatures(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, n_sequences=11, n_features=21, features_dim=20):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        self.cnn_1 = nn.Sequential(\n",
    "            nn.Conv1d(n_features, n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(n_features*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(n_features*2, n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(n_features*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(n_features*2, n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(n_features*2),\n",
    "            nn.Conv1d(n_features*2, n_features*2, kernel_size=3, padding=1),\n",
    "            \n",
    "            nn.AvgPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.cnn_2 = nn.Sequential(\n",
    "            nn.Conv1d(n_features*2, n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(n_features),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(n_features, n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(n_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.act = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        out = self.cnn_1(obs)\n",
    "        out = self.cnn_2(out)\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load incentive_reinforcement_learning_cpu.py\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3.common.callbacks import CallbackList, StopTrainingOnMaxEpisodes, CheckpointCallback\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import logging\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from datetime import datetime\n",
    "from pprint import pformat\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3.common.callbacks import CallbackList, StopTrainingOnMaxEpisodes, CheckpointCallback\n",
    "from stable_baselines3 import A2C, DQN\n",
    "import logging\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from datetime import datetime\n",
    "from pprint import pformat\n",
    "import os\n",
    "\n",
    "\n",
    "ALL_COLS = METADATA + OUT_FEATURE_COLUMNS \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "torch.set_printoptions(precision=2, linewidth=200, sci_mode=False)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "S3_BASELINE_PATH = 's3://dissertation-data-dmiller'\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "CUM_SESSION_EVENT_RAW = 3\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "\n",
    "\n",
    "\n",
    "def train_eval_split(dataset, logger):\n",
    "    train_split = int(dataset.shape[0] * TRAIN_SPLIT)\n",
    "    eval_split = int(dataset.shape[0] * EVAL_SPLIT)\n",
    "    test_split = dataset.shape[0] - train_split - eval_split\n",
    "    logger.info(f'Train size: 0:{train_split}, eval size: {train_split}:{train_split+eval_split}: test size: {train_split + eval_split}:{dataset.shape[0]}')\n",
    "    train_dataset, eval_dataset, test_split = dataset[:train_split], dataset[train_split:train_split+eval_split], dataset[train_split+eval_split:]\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'eval': eval_dataset,\n",
    "        'test': test_split\n",
    "    }\n",
    "\n",
    "def generate_metadata(dataset):\n",
    "    \n",
    "    session_size = dataset.groupby(['user_id', 'session_30_raw'])['size_of_session'].max().reset_index(name='session_size')\n",
    "    session_minutes = dataset.groupby(['user_id', 'session_30_raw'])['cum_session_time_raw'].max().reset_index(name='session_minutes')\n",
    "    \n",
    "    sim_minutes = dataset.groupby(['user_id', 'session_30_raw'])['cum_session_time_raw'].quantile(.7, interpolation='nearest').reset_index(name='sim_minutes')\n",
    "    sim_size = dataset.groupby(['user_id', 'session_30_raw'])['cum_session_event_raw'].quantile(.7, interpolation='nearest').reset_index(name='sim_size')\n",
    "    \n",
    "    \n",
    "    sessions = [session_size, session_minutes, sim_minutes, sim_size]\n",
    "    sessions = reduce(lambda left, right: pd.merge(left, right, on=['user_id', 'session_30_raw']), sessions)\n",
    "    dataset = pd.merge(dataset, sessions, on=['user_id', 'session_30_raw'])\n",
    "    dataset['reward'] = dataset['cum_session_time_raw']\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parse = argparse.ArgumentParser()\n",
    "    parse.add_argument('--read_path', type=str, default='datasets/rl_ready_data')\n",
    "    parse.add_argument('--n_files', type=int, default=2)\n",
    "    parse.add_argument('--n_episodes', type=int, default=50)\n",
    "    parse.add_argument('--n_sequences', type=int, default=10)\n",
    "    parse.add_argument('--n_envs', type=int, default=100)\n",
    "    parse.add_argument('--lstm', type=str, default='seq_10')\n",
    "    parse.add_argument('--device', type=str, default='cpu')\n",
    "    parse.add_argument('--checkpoint_freq', type=int, default=1000)\n",
    "    parse.add_argument('--tb_log', type=int, default=100)\n",
    "    parse.add_argument('--feature_extractor', type=str, default='cnn') \n",
    "    args = parse.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def run_reinforcement_learning_incentives(environment, logger, n_episodes=1):\n",
    "    for epoch in range(n_episodes):\n",
    "        environment_comp = False\n",
    "        state = environment.reset()\n",
    "        i = 0\n",
    "        while not environment_comp:\n",
    "            next_action = (\n",
    "                1 if np.random.uniform(low=0, high=1) > 0.8 else 0\n",
    "            )\n",
    "            state, rewards, environment_comp, meta = environment.step(next_action)\n",
    "            i +=1\n",
    "            if i % 100 == 0:\n",
    "                logger.info(f'Step: {i} - Reward: {rewards}')\n",
    "                \n",
    "        logger.info(f'Epoch: {epoch} - Reward: {rewards}')\n",
    "        print(environment.user_sessions.head(10))\n",
    "\n",
    "\n",
    "def remove_events_in_2_minute_window(df):\n",
    "    df['second_window'] = df['second'] // 10\n",
    "    df = df.drop_duplicates(\n",
    "        subset=['user_id', 'session_30_raw', 'year', 'month', 'day', 'hour', 'minute'],\n",
    "        keep='last'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def convolve_delta_events(df):\n",
    "    df['convolved_delta_event'] = (\n",
    "        df.set_index('date_time').groupby(by=['user_id', 'session_30_raw'], group_keys=False) \\\n",
    "            .rolling('2T', min_periods=1)['delta_last_event'] \\\n",
    "            .mean()\n",
    "            .reset_index(name='convolved_event_delta')['convolved_event_delta']\n",
    "    )\n",
    "\n",
    "    df['delta_last_event'] = df['convolved_delta_event']\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "def main(args):\n",
    "    \n",
    "    exec_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    logger.info('Starting Incentive Reinforcement Learning')\n",
    "    \n",
    "    read_path, n_files, n_sequences, n_episodes, device, n_envs, lstm, tb_log, check_freq, feature_ext = (\n",
    "        args.read_path, \n",
    "        args.n_files, \n",
    "        args.n_sequences, \n",
    "        args.n_episodes, \n",
    "        args.device,\n",
    "        args.n_envs,\n",
    "        args.lstm,\n",
    "        args.tb_log,\n",
    "        args.checkpoint_freq,\n",
    "        args.feature_extractor\n",
    "    )\n",
    "    \n",
    "    read_path = os.path.join(\n",
    "        read_path,\n",
    "        f'files_used_{n_files}',\n",
    "        f'predicted_data.parquet'\n",
    "    )\n",
    "   \n",
    "    if not os.path.exists(read_path):\n",
    "        logger.info(f'Downloading data from {S3_BASELINE_PATH}/{read_path}') \n",
    "        s3 = boto3.client('s3')\n",
    "        s3.download_file(\n",
    "            S3_BASELINE_PATH,     \n",
    "            read_path,\n",
    "            read_path\n",
    "        )\n",
    "             \n",
    "            \n",
    "    logger.info(f'Reading data from {read_path}')\n",
    "    cols = ALL_COLS + [lstm] if lstm else ALL_COLS\n",
    "    out_features = OUT_FEATURE_COLUMNS + [lstm] if lstm else OUT_FEATURE_COLUMNS\n",
    "    if args.lstm:\n",
    "        logger.info(f'Including LSTM prediction: {lstm}')\n",
    "        df = pd.read_parquet(read_path, columns=cols)\n",
    "    else:\n",
    "        logger.info(f'Setting up baseline without prediction')\n",
    "        df = pd.read_parquet(read_path, columns=ALL_COLS)\n",
    "    df['date_time'] = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute', 'second']], errors='coerce')\n",
    "\n",
    "\n",
    "    df = df.sort_values(by=['date_time'])    \n",
    "    \n",
    "    logger.info(f'N events:: {df.shape[0]} creating training partitions')\n",
    "    df = df.head(int(df.shape[0] * .7))\n",
    "    logger.info(f'N events after 70% split: {df.shape[0]}')\n",
    "    size_of_session = df.groupby(['user_id', 'session_30_raw']).size().reset_index(name='size_of_session')\n",
    "    df = pd.merge(df, size_of_session, on=['user_id', 'session_30_raw'])\n",
    "    df['cum_session_event_raw'] = df.groupby(['user_id', 'session_30_raw'])['date_time'].cumcount() + 1\n",
    "    \n",
    "    logger.info(f'Convolution over 2 minute window')\n",
    "    df = convolve_delta_events(df)\n",
    "    logger.info(f'Convolving over 2 minute window complete: generating metadata')\n",
    "    df = generate_metadata(df) \n",
    "    logger.info(f'Metadata generated: selecting events only at 2 minute intervals')\n",
    "    df = df[df['minute'] % 2 == 0]\n",
    "    logger.info(f'Data read: {df.shape[0]} rows, {df.shape[1]} columns, dropping events within 2 minute window')\n",
    "    df = remove_events_in_2_minute_window(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    logger.info(f'Number of events after dropping events within 2 minute window: {df.shape[0]}')\n",
    "    \n",
    "    unique_episodes = df[['user_id', 'session_30_raw']].drop_duplicates()\n",
    "    unique_sessions = df[['session_30_raw']].drop_duplicates()\n",
    "    df = df.drop(columns=['year', 'month', 'day', 'hour', 'minute', 'second', 'second_window'])\n",
    "    \n",
    "    citizen_science_vec =DummyVecEnv([lambda: CitizenScienceEnv(df, unique_episodes, unique_sessions, out_features, 10) for i in range(n_envs)])\n",
    "    logger.info(f'Vectorized environments created')\n",
    "    \n",
    "    base_path = os.path.join(\n",
    "        S3_BASELINE_PATH,\n",
    "        'reinforcement_learning_incentives',\n",
    "        f'n_files_{n_files}',\n",
    "        feature_ext + '_' + 'label' if lstm.startswith('continue') else lstm,\n",
    "        'results',\n",
    "        exec_time,\n",
    "    ) \n",
    "    \n",
    "    \n",
    "    tensorboard_dir, checkpoint_dir = (\n",
    "        os.path.join(base_path, 'training_metrics'),\n",
    "        os.path.join(base_path, 'checkpoints')\n",
    "    )\n",
    "\n",
    "    logger.info(f'Creating callbacks, monitors and loggerss')\n",
    "    callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=n_episodes, verbose=1)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=check_freq // n_envs, save_path=checkpoint_dir, name_prefix='rl_model')\n",
    "    dist_callback = DistributionCallback()\n",
    "    DistributionCallback.tensorboard_setup(tensorboard_dir, tb_log)\n",
    "    callback_list = CallbackList([callback_max_episodes, dist_callback, checkpoint_callback])\n",
    "    monitor_train = VecMonitor(citizen_science_vec)\n",
    "   \n",
    "    if feature_ext == 'cnn':\n",
    "        logger.info('Using custom 1 dimensional CNN feature extractor')\n",
    "        policy_kwargs = dict(\n",
    "            features_extractor_class=CustomConv1dFeatures,\n",
    "            net_arch=[20, 10]\n",
    "        )\n",
    "        model = DQN(policy='MlpPolicy', env=monitor_train, verbose=1, tensorboard_log=tensorboard_dir, policy_kwargs=policy_kwargs, device=device, stats_window_size=1000)\n",
    "    else:\n",
    "        logger.info('Using default MLP feature extractor')\n",
    "        model = DQN(policy='MlpPolicy', env=monitor_train, verbose=1, tensorboard_log=tensorboard_dir, device=device, stats_window_size=1000)\n",
    "    \n",
    "    logger.info(f'Model created: policy')\n",
    "    \n",
    "    logger.info(pformat(model.policy))\n",
    "        \n",
    "    logger.info(f'Beginning training') \n",
    "            \n",
    "    logger.info(pformat([\n",
    "        'n_epochs: {}'.format(n_episodes),\n",
    "        'read_path: {}'.format(read_path),\n",
    "        'n_files: {}'.format(n_files),\n",
    "        'n_sequences: {}'.format(n_sequences),\n",
    "        'n_envs: {}'.format(n_envs),\n",
    "        'total_timesteps: {}'.format(df.shape),\n",
    "        f'unique_episodes: {unique_episodes.shape[0]}',\n",
    "        'device: {}'.format(device),\n",
    "        'tensorboard_dir: {}'.format(tensorboard_dir),\n",
    "        'checkpoint_dir: {}'.format(checkpoint_dir)\n",
    "    ]))\n",
    "    \n",
    "    model.learn(total_timesteps=100_000_000, progress_bar=True, log_interval=1000, callback=callback_list)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument:\n",
    "    read_path = 'rl_ready_data'\n",
    "    n_files = 30\n",
    "    n_sequences = 10\n",
    "    n_episodes = 500_000\n",
    "    n_envs = 1000\n",
    "    lstm = 'continue_work_session_30_minutes'\n",
    "    device = 'cuda'\n",
    "    checkpoint_freq = 100_000\n",
    "    tb_log = 1000\n",
    "    feature_extractor = 'mlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.1       |\n",
      "|    ep_rew_mean      | 230.03746 |\n",
      "|    exploration_rate | 0.942     |\n",
      "| time/               |           |\n",
      "|    episodes         | 85000     |\n",
      "|    fps              | 403       |\n",
      "|    time_elapsed     | 1516      |\n",
      "|    total_timesteps  | 612000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 36.6      |\n",
      "|    n_updates        | 140       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.04      |\n",
      "|    ep_rew_mean      | 244.52078 |\n",
      "|    exploration_rate | 0.941     |\n",
      "| time/               |           |\n",
      "|    episodes         | 86000     |\n",
      "|    fps              | 403       |\n",
      "|    time_elapsed     | 1532      |\n",
      "|    total_timesteps  | 618000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 35.7      |\n",
      "|    n_updates        | 142       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 6.28      |\n",
      "|    ep_rew_mean      | 167.43466 |\n",
      "|    exploration_rate | 0.941     |\n",
      "| time/               |           |\n",
      "|    episodes         | 87000     |\n",
      "|    fps              | 402       |\n",
      "|    time_elapsed     | 1551      |\n",
      "|    total_timesteps  | 625000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 29.8      |\n",
      "|    n_updates        | 144       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.55      |\n",
      "|    ep_rew_mean      | 235.86134 |\n",
      "|    exploration_rate | 0.94      |\n",
      "| time/               |           |\n",
      "|    episodes         | 88000     |\n",
      "|    fps              | 403       |\n",
      "|    time_elapsed     | 1570      |\n",
      "|    total_timesteps  | 633000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 17.7      |\n",
      "|    n_updates        | 146       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   1%</span> <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">768,000/100,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:31:30</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">65:58:27</span> , <span style=\"color: #800000; text-decoration-color: #800000\">418 it/s</span> ]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m   1%\u001b[0m \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768,000/100,000,000 \u001b[0m [ \u001b[33m0:31:30\u001b[0m < \u001b[36m65:58:27\u001b[0m , \u001b[31m418 it/s\u001b[0m ]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 6.97      |\n",
      "|    ep_rew_mean      | 224.08922 |\n",
      "|    exploration_rate | 0.927     |\n",
      "| time/               |           |\n",
      "|    episodes         | 107000    |\n",
      "|    fps              | 403       |\n",
      "|    time_elapsed     | 1911      |\n",
      "|    total_timesteps  | 772000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 21.1      |\n",
      "|    n_updates        | 180       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.5       |\n",
      "|    ep_rew_mean      | 294.32407 |\n",
      "|    exploration_rate | 0.925     |\n",
      "| time/               |           |\n",
      "|    episodes         | 109000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 1947      |\n",
      "|    total_timesteps  | 787000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 27.5      |\n",
      "|    n_updates        | 184       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.27      |\n",
      "|    ep_rew_mean      | 234.76776 |\n",
      "|    exploration_rate | 0.925     |\n",
      "| time/               |           |\n",
      "|    episodes         | 110000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 1964      |\n",
      "|    total_timesteps  | 794000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 37        |\n",
      "|    n_updates        | 186       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.24     |\n",
      "|    ep_rew_mean      | 268.0618 |\n",
      "|    exploration_rate | 0.924    |\n",
      "| time/               |          |\n",
      "|    episodes         | 111000   |\n",
      "|    fps              | 404      |\n",
      "|    time_elapsed     | 1981     |\n",
      "|    total_timesteps  | 801000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 20.3     |\n",
      "|    n_updates        | 188      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.31      |\n",
      "|    ep_rew_mean      | 260.81845 |\n",
      "|    exploration_rate | 0.923     |\n",
      "| time/               |           |\n",
      "|    episodes         | 112000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2000      |\n",
      "|    total_timesteps  | 809000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 22.1      |\n",
      "|    n_updates        | 190       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.46      |\n",
      "|    ep_rew_mean      | 344.89426 |\n",
      "|    exploration_rate | 0.922     |\n",
      "| time/               |           |\n",
      "|    episodes         | 113000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2016      |\n",
      "|    total_timesteps  | 816000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 37.1      |\n",
      "|    n_updates        | 191       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.56      |\n",
      "|    ep_rew_mean      | 318.98703 |\n",
      "|    exploration_rate | 0.922     |\n",
      "| time/               |           |\n",
      "|    episodes         | 114000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2034      |\n",
      "|    total_timesteps  | 823000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 23.6      |\n",
      "|    n_updates        | 193       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.58     |\n",
      "|    ep_rew_mean      | 285.8113 |\n",
      "|    exploration_rate | 0.921    |\n",
      "| time/               |          |\n",
      "|    episodes         | 115000   |\n",
      "|    fps              | 404      |\n",
      "|    time_elapsed     | 2053     |\n",
      "|    total_timesteps  | 831000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 37.6     |\n",
      "|    n_updates        | 195      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.5       |\n",
      "|    ep_rew_mean      | 281.84744 |\n",
      "|    exploration_rate | 0.92      |\n",
      "| time/               |           |\n",
      "|    episodes         | 116000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2070      |\n",
      "|    total_timesteps  | 838000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 27.7      |\n",
      "|    n_updates        | 197       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.42     |\n",
      "|    ep_rew_mean      | 279.5405 |\n",
      "|    exploration_rate | 0.92     |\n",
      "| time/               |          |\n",
      "|    episodes         | 117000   |\n",
      "|    fps              | 404      |\n",
      "|    time_elapsed     | 2089     |\n",
      "|    total_timesteps  | 846000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 25.5     |\n",
      "|    n_updates        | 199      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.98     |\n",
      "|    ep_rew_mean      | 217.4085 |\n",
      "|    exploration_rate | 0.919    |\n",
      "| time/               |          |\n",
      "|    episodes         | 118000   |\n",
      "|    fps              | 404      |\n",
      "|    time_elapsed     | 2107     |\n",
      "|    total_timesteps  | 853000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 31       |\n",
      "|    n_updates        | 201      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.53      |\n",
      "|    ep_rew_mean      | 299.41113 |\n",
      "|    exploration_rate | 0.918     |\n",
      "| time/               |           |\n",
      "|    episodes         | 119000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2125      |\n",
      "|    total_timesteps  | 860000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 51.9      |\n",
      "|    n_updates        | 202       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.36     |\n",
      "|    ep_rew_mean      | 289.2404 |\n",
      "|    exploration_rate | 0.918    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120000   |\n",
      "|    fps              | 404      |\n",
      "|    time_elapsed     | 2143     |\n",
      "|    total_timesteps  | 867000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 22.4     |\n",
      "|    n_updates        | 204      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 6.97      |\n",
      "|    ep_rew_mean      | 234.41624 |\n",
      "|    exploration_rate | 0.917     |\n",
      "| time/               |           |\n",
      "|    episodes         | 121000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2160      |\n",
      "|    total_timesteps  | 874000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 50.7      |\n",
      "|    n_updates        | 206       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7        |\n",
      "|    ep_rew_mean      | 224.8686 |\n",
      "|    exploration_rate | 0.916    |\n",
      "| time/               |          |\n",
      "|    episodes         | 122000   |\n",
      "|    fps              | 404      |\n",
      "|    time_elapsed     | 2177     |\n",
      "|    total_timesteps  | 881000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 59.3     |\n",
      "|    n_updates        | 208      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 6.96      |\n",
      "|    ep_rew_mean      | 228.19632 |\n",
      "|    exploration_rate | 0.916     |\n",
      "| time/               |           |\n",
      "|    episodes         | 123000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2195      |\n",
      "|    total_timesteps  | 888000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 53        |\n",
      "|    n_updates        | 209       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.01      |\n",
      "|    ep_rew_mean      | 247.87178 |\n",
      "|    exploration_rate | 0.915     |\n",
      "| time/               |           |\n",
      "|    episodes         | 124000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2212      |\n",
      "|    total_timesteps  | 895000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 36.4      |\n",
      "|    n_updates        | 211       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.41     |\n",
      "|    ep_rew_mean      | 265.9421 |\n",
      "|    exploration_rate | 0.914    |\n",
      "| time/               |          |\n",
      "|    episodes         | 125000   |\n",
      "|    fps              | 404      |\n",
      "|    time_elapsed     | 2230     |\n",
      "|    total_timesteps  | 902000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.5     |\n",
      "|    n_updates        | 213      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.38      |\n",
      "|    ep_rew_mean      | 242.33218 |\n",
      "|    exploration_rate | 0.914     |\n",
      "| time/               |           |\n",
      "|    episodes         | 126000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2249      |\n",
      "|    total_timesteps  | 910000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 31.8      |\n",
      "|    n_updates        | 215       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.56      |\n",
      "|    ep_rew_mean      | 276.77054 |\n",
      "|    exploration_rate | 0.913     |\n",
      "| time/               |           |\n",
      "|    episodes         | 127000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2265      |\n",
      "|    total_timesteps  | 917000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 46        |\n",
      "|    n_updates        | 217       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 6.94      |\n",
      "|    ep_rew_mean      | 245.53355 |\n",
      "|    exploration_rate | 0.912     |\n",
      "| time/               |           |\n",
      "|    episodes         | 128000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2282      |\n",
      "|    total_timesteps  | 924000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 28        |\n",
      "|    n_updates        | 218       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 6.87      |\n",
      "|    ep_rew_mean      | 234.58653 |\n",
      "|    exploration_rate | 0.912     |\n",
      "| time/               |           |\n",
      "|    episodes         | 129000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2299      |\n",
      "|    total_timesteps  | 931000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 47.7      |\n",
      "|    n_updates        | 220       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.15      |\n",
      "|    ep_rew_mean      | 256.92474 |\n",
      "|    exploration_rate | 0.911     |\n",
      "| time/               |           |\n",
      "|    episodes         | 130000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2317      |\n",
      "|    total_timesteps  | 938000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 18.5      |\n",
      "|    n_updates        | 222       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.42     |\n",
      "|    ep_rew_mean      | 242.1612 |\n",
      "|    exploration_rate | 0.91     |\n",
      "| time/               |          |\n",
      "|    episodes         | 131000   |\n",
      "|    fps              | 404      |\n",
      "|    time_elapsed     | 2337     |\n",
      "|    total_timesteps  | 946000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 32.6     |\n",
      "|    n_updates        | 224      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 7.47      |\n",
      "|    ep_rew_mean      | 282.19266 |\n",
      "|    exploration_rate | 0.909     |\n",
      "| time/               |           |\n",
      "|    episodes         | 132000    |\n",
      "|    fps              | 404       |\n",
      "|    time_elapsed     | 2354      |\n",
      "|    total_timesteps  | 953000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 23.7      |\n",
      "|    n_updates        | 226       |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main(Argument)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
