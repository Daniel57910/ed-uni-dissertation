{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.filters.hp_filter import hpfilter\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.special import kl_div\n",
    "from datetime import datetime\n",
    "import plotly\n",
    "import plotly.offline as pyo\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "PLOTLY_COLORS = plotly.colors.DEFAULT_PLOTLY_COLORS\n",
    "\n",
    "pio.renderers.defaults= 'notebook+pdf'\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_bucket(x):\n",
    "    if x <= 20:\n",
    "        return 'small'\n",
    "    if all([x > 20, x<= 40]):\n",
    "        return 'mid'\n",
    "    if all([x > 40, x<= 60]):\n",
    "        return 'large'\n",
    "\n",
    "    return 'max'\n",
    "\n",
    "def plot_contours(df, xtitle, ytitle, ztitle, target_var):\n",
    "    x_var, y_var, z_var, target_var = df[xtitle], df[ytitle], df[ztitle], df[target_var]\n",
    "    delaney_points = np.vstack([x_var, y_var]).T\n",
    "    \n",
    "    I, J, K = Delaunay(delaney_points).simplices.T\n",
    "    fig = go.Figure(go.Mesh3d(x=x_var, y=y_var, z=z_var, i=I, j=J, k=K, intensity=target_var, colorscale='thermal', alphahull=.5))\n",
    "    fig.update_layout(\n",
    "        height=500,\n",
    "        width=750,\n",
    "        scene =dict(\n",
    "        xaxis_title=xtitle,\n",
    "        yaxis_title=ytitle,\n",
    "        zaxis_title=ztitle\n",
    "    ))\n",
    "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=10))\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "\n",
    "def plot_contours_kl(baseline_df, sensitivity_df, resample=500_000):\n",
    "    \n",
    "\n",
    "    sensitivity_df = sensitivity_df.rename(columns={\n",
    "        'inc_small': 'inc_small_s',\n",
    "        'inc_medium': 'inc_mid_s',\n",
    "        'inc_large': 'inc_large_s',\n",
    "        'ended_event': 'ended_event_s',\n",
    "        'normalized_distance': 'norm_dist_s'\n",
    "\n",
    "    })\n",
    "  \n",
    "    sensitivity_df = sensitivity_df.sample(baseline_df.shape[0], replace=True)\n",
    "    sens_joined = sensitivity_df[['session_size', 'ended_event_s', 'inc_small_s', 'inc_mid_s', 'inc_large_s', 'window', 'mid', 'large', 'norm_dist_s']].set_index('session_size') \\\n",
    "        .join(baseline_df[['session_size', 'ended_event', 'normalized_distance']].set_index('session_size'), how='inner') \\\n",
    "        .reset_index() \\\n",
    "        .sample(500_000)\n",
    "        \n",
    "    \n",
    "    sens_joined['kl_div_ended'] = kl_div(sens_joined['ended_event'], sens_joined['ended_event_s'])\n",
    "    # return sens_joined\n",
    "    \n",
    "    plot_contours(sens_joined, 'mid', 'large', 'window', 'kl_div_ended') \n",
    "\n",
    "\n",
    "def plot_contours_kl_2(baseline_df, sensitivity_df, resample=500_000):\n",
    "    sensitivity_df = sensitivity_df.rename(columns={\n",
    "        'inc_small': 'inc_small_s',\n",
    "        'inc_medium': 'inc_mid_s',\n",
    "        'inc_large': 'inc_large_s',\n",
    "        'ended_event': 'ended_event_s',\n",
    "        \"normalized_distance\": \"norm_dist_s\"\n",
    "    })\n",
    "    \n",
    "    sensitivity_df = sensitivity_df.sample(baseline_df.shape[0] * 4, replace=True)\n",
    "    print(sensitivity_df.columns)\n",
    "    sess_joined = sensitivity_df[['session_size', 'ended_event_s', 'inc_small_s', 'inc_mid_s', 'inc_large_s', 'window', 'mid', 'large', 'norm_dist_s', 'soc_freq']].set_index('session_size') \\\n",
    "        .join(baseline_df[['session_size', 'ended_event', 'normalized_distance']].set_index('session_size'), how='inner') \\\n",
    "        .reset_index() \\\n",
    "        .sample(resample)\n",
    "        \n",
    "    sess_joined['kl_div_ended'] = kl_div(sess_joined['ended_event'], sess_joined['ended_event_s']) \n",
    "    plot_contours(sess_joined, 'large', 'window', 'soc_freq', 'kl_div_ended')\n",
    "    \n",
    "\n",
    "def _cutoff(inc, bucket):\n",
    "    if bucket == 'small':\n",
    "        return inc >= 10\n",
    "    \n",
    "    if bucket == 'medium':\n",
    "        return inc >= 20\n",
    "\n",
    "    return inc >= 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sensitivity = pd.read_parquet('dqn_parquet/sensitivity_analysis/dqn_pred_cnn.parquet')\n",
    "# df_sensitivity['normalized_distance'] = df_sensitivity['ended_event'] / df_sensitivity['session_size']\n",
    "# df_sensitivity['session_bucket'] = df_sensitivity['session_size'].apply(session_bucket)\n",
    "\n",
    "# df_eval = pd.read_parquet('dqn_parquet/eval/dqn_pred_cnn_15000_no_pen.parquet')\n",
    "# df_eval['normalized_distance'] = df_eval['ended_event'] / df_eval['session_size']\n",
    "# df_eval['session_bucket'] = df_eval['session_size'].apply(session_bucket)\n",
    "# df_eval['before_cutoff_small'] = df_eval[['inc_small', 'session_bucket']].apply(lambda x: _cutoff(x['inc_small'], x['session_bucket']), axis=1)\n",
    "# df_eval['before_cutoff_mid'] = df_eval[['inc_medium', 'session_bucket']].apply(lambda x: _cutoff(x['inc_medium'], x['session_bucket']), axis=1)\n",
    "# df_eval['before_cutoff_large'] = df_eval[['inc_large', 'session_bucket']].apply(lambda x: _cutoff(x['inc_large'], x['session_bucket']), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_contours_kl(df_eval[df_eval['session_bucket'] == 'small'], df_sensitivity[df_sensitivity['session_bucket'] == 'small'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_contours_kl(df_eval[df_eval['session_bucket'] == 'mid'], df_sensitivity[df_sensitivity['session_bucket'] == 'mid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_contours_kl(\n",
    "#     df_eval[(df_eval['session_bucket'] == 'large') |(df_eval['session_bucket'] == 'max')],\n",
    "#     df_sensitivity[(df_sensitivity['session_bucket'] == 'large') |(df_sensitivity['session_bucket'] == 'max')],\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sensitivity_q2 = pd.read_parquet('dqn_parquet/sensitivity_analysis/q2/dqn_pred_cnn_dist_enc_enforce.parquet')\n",
    "# df_eval_q2 = pd.read_parquet('dqn_parquet/eval/q2/dqn_pred_cnn_dist_enc_enforce_15000.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sensitivity_q2 = df_sensitivity_q2.fillna(0)\n",
    "\n",
    "# df_sensitivity_q2['normalized_distance'] = df_sensitivity_q2['ended_event'] / df_sensitivity_q2['session_size']\n",
    "# df_eval_q2['normalized_distance'] = df_eval_q2['ended_event'] / df_eval_q2['session_size']\n",
    "\n",
    "# df_sensitivity_q2['session_bucket'] = df_sensitivity_q2['session_size'].apply(session_bucket)\n",
    "# df_eval_q2['session_bucket'] = df_eval_q2['session_size'].apply(session_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_contours_kl_2(df_eval_q2[df_eval_q2['session_bucket'] == 'small'], df_sensitivity_q2[df_sensitivity_q2['session_bucket'] == 'small'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_contours_kl_2(df_eval_q2[df_eval_q2['session_bucket'] == 'mid'], df_sensitivity_q2[df_sensitivity_q2['session_bucket'] == 'mid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_contours_kl_2(df_eval_q2[df_eval_q2['session_bucket'] == 'large'], df_sensitivity_q2[df_sensitivity_q2['session_bucket'] == 'large'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_contours_kl_2(df_eval_q2[df_eval_q2['session_bucket'] == 'max'], df_sensitivity_q2[df_sensitivity_q2['session_bucket'] == 'max'], 100_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
