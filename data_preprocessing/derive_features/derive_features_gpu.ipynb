{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from pprint import pformat\n",
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    import cupy as np\n",
    "    import cudf as pd\n",
    "    import pandas as cpu_pd\n",
    "    import dask_cudf as dd\n",
    "else:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import dask.dataframe as dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(linewidth=200)\n",
    "\n",
    "cpu_pd.set_option('display.max_columns', None)\n",
    "cpu_pd.set_option('display.max_rows', None)\n",
    "cpu_pd.set_option('display.width', 500)\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LOAD_COLUMNS = [\n",
    "    'label_30',\n",
    "    'date_time',\n",
    "    'user_id',\n",
    "    'project_id',\n",
    "    'session_5',\n",
    "    'session_30',\n",
    "    'country',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "def encode_counts(df):\n",
    "    \n",
    "    user_count, project_count, country_count = (\n",
    "        df['user_id'].value_counts().reset_index().rename(columns={'user_id': 'user_count', 'index': 'user_id'}),\n",
    "        df['project_id'].value_counts().reset_index().rename(columns={'project_id': 'project_count', 'index': 'project_id'}),\n",
    "        df['country'].value_counts().reset_index().rename(columns={'country': 'country_count', 'index': 'country'})\n",
    "    )\n",
    "   \n",
    "    user_count['user_id_hash'] = user_count.index.values + 1\n",
    "    project_count['project_id_hash'] = project_count.index.values + 1\n",
    "    country_count['country_hash'] = country_count.index.values + 1\n",
    "\n",
    "    df = df.merge(user_count, on='user_id')\n",
    "    df = df.merge(project_count, on='project_id')\n",
    "    df = df.merge(country_count, on='country')\n",
    "    \n",
    "    df = df.drop(columns=['user_id', 'project_id', 'country'])\n",
    "    df = df.rename(columns={'user_id_hash': 'user_id', 'project_id_hash': 'project_id', 'country_hash': 'country'})\n",
    "    return df\n",
    "   \n",
    "def time_encodings(df):\n",
    "    \"\"\"\n",
    "    Timestamp raw encoded in units of seconds\n",
    "    \"\"\"\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    df['timestamp_raw'] = df['date_time'].astype('int64') // 10**9\n",
    "    return df\n",
    "\n",
    "def rolling_window_session_10(df, logger):\n",
    "    logger.info('Calculating rolling session time averages')\n",
    "    df = df.reset_index()\n",
    "    df['row_count'] = df.index.values\n",
    "    rolling_10 = df.set_index('row_count') \\\n",
    "        .groupby(['user_id', 'session_30']) \\\n",
    "        .rolling(10, min_periods=1)['delta_last_event'].mean() \\\n",
    "        .reset_index().rename(columns={'delta_last_event': 'rolling_10'}) \\\n",
    "        .sort_values(by='row_count')\n",
    "\n",
    "    logger.info('Rolling averages calculated: joining to df')\n",
    "    df = df.set_index('row_count').join(rolling_10[['row_count', 'rolling_10']].set_index('row_count'))\n",
    "    logger.info('Rolling averages joined to df')\n",
    "    df = df.sort_values(by='date_time')\n",
    "    return df\n",
    "\n",
    "\n",
    "def expanding_session_time_delta(df, logger):\n",
    "    logger.info('Calculating expanding session time averages')\n",
    "    df = df.reset_index()\n",
    "    df['row_count'] = df.index.values\n",
    "    expanding_window = df.set_index('row_count') \\\n",
    "        .groupby(['user_id', 'session_30']) \\\n",
    "        .rolling(1000000, min_periods=1)['delta_last_event'].mean() \\\n",
    "        .reset_index().rename(columns={'delta_last_event': 'expanding_click_average'}) \\\n",
    "        .sort_values(by='row_count')\n",
    "    \n",
    "    logger.info('Expanding averages calculated: joining to df')\n",
    "    df = df.set_index('row_count').join(expanding_window[['row_count', 'expanding_click_average']].set_index('row_count'))\n",
    "    logger.info('Expanding averages joined to df')\n",
    "    df = df.sort_values(by='date_time')\n",
    "    return df\n",
    "\n",
    "def intra_session_stats(df, logger):\n",
    "    \n",
    "    logger.info('Sorting by date_time and user_id')\n",
    "    df = df.sort_values(by=['date_time', 'user_id'])\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['user_id', 'date_time'], keep='first')\n",
    "    logger.info('Calculating cum_event_count')\n",
    "    df['cum_session_event_count'] = df.groupby(['user_id', 'session_30'])['date_time'].cumcount() + 1\n",
    "    logger.info('Cum_event_count calculated: calculating delta_last_event')\n",
    "    df['delta_last_event'] = df.groupby(['user_id', 'session_30'])['date_time'].diff()\n",
    "\n",
    "    df = df.to_pandas().sort_values(by=['date_time', 'user_id'])\n",
    "    df['delta_last_event'] = df['delta_last_event'].dt.total_seconds()\n",
    "    df['delta_last_event'] = df['delta_last_event'].fillna(0)\n",
    "    df = pd.from_pandas(df)\n",
    "    df = df.sort_values(by=['date_time', 'user_id'])\n",
    "    df['cum_session_time_minutes'] = df.groupby(['user_id', 'session_30'])['delta_last_event'].cumsum()\n",
    "    df['cum_session_time_minutes'] = df['cum_session_time_minutes'] / 60\n",
    "    logger.info('Beginning rolling window 10 calculation')\n",
    "    logger.info('Rolling window 10 calculation complete: beginning expanding window calculation')\n",
    "    logger.info('Expanding window calculation complete: returning to dask')\n",
    "    return df\n",
    "\n",
    "\n",
    "def running_user_stats(df, logger):\n",
    "    logger.info('Calculating cumulative platform time')\n",
    "    df['cum_platform_time_minutes'] = df.groupby(['user_id'])['delta_last_event'].cumsum()\n",
    "    df['cum_platform_time_minutes'] = df['cum_platform_time_minutes'] / 60\n",
    "    logger.info('Calculating cumulative platform events')\n",
    "    df['cum_platform_events'] = df.groupby(['user_id'])['delta_last_event'].cumcount() + 1\n",
    "    logger.info('Calculated cumulative platform events: calculating running unique projects')\n",
    "    \n",
    "    logger.info('Using GPU: converting to pandas')\n",
    "    df = df.to_pandas()\n",
    "    logger.info('Calculating running unique projects: converting to categorical')\n",
    "    df['cum_projects'] = (df.groupby('user_id')['project_id'].transform(lambda x: cpu_pd.CategoricalIndex(x).codes) + 1).astype('int32')\n",
    "    df = pd.from_pandas(df)\n",
    "    logger.info('Returning to GPU')\n",
    "    logger.info('Calculated running unique projects: calculating average event time delta')\n",
    "    df = df.reset_index()\n",
    "    df['row_count'] = df.index.values\n",
    "    \n",
    "    average_event_time = df.set_index('row_count') \\\n",
    "        .groupby('user_id') \\\n",
    "        .rolling(10000000, min_periods=1)['delta_last_event'].mean() \\\n",
    "        .reset_index().rename(columns={'delta_last_event': 'average_event_time'}) \\\n",
    "        .sort_values(by='row_count')\n",
    "    df = df.set_index('row_count').join(average_event_time[['row_count', 'average_event_time']].set_index('row_count'))\n",
    "    logger.info('Calculated average event time delta')\n",
    "    return df\n",
    "\n",
    "def expanding_session_time(df, session_inflection_times, logger):\n",
    " \n",
    "    logger.info('Session inflection times calculated: calculating expanding session time')\n",
    "    session_inflection_times = session_inflection_times.to_pandas()\n",
    "    session_inflection_times['time_in_session_minutes'] = (session_inflection_times['date_time_max'] - session_inflection_times['date_time_min']).dt.total_seconds() / 60\n",
    "   \n",
    "    session_inflection_times['expanding_session_time_minutes'] = session_inflection_times \\\n",
    "        .groupby(['user_id'])['time_in_session_minutes'] \\\n",
    "        .rolling(1000000, min_periods=1, closed='left') \\\n",
    "        .mean() \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'time_in_session_minutes': 'expanding_session_time_minutes'})['expanding_session_time_minutes']\n",
    "    \n",
    "    session_inflection_times = pd.from_pandas(session_inflection_times)\n",
    "    \n",
    "    logger.info('Expanding session time calculated: joining to df')\n",
    "  \n",
    "    df = pd.merge(df, session_inflection_times[['user_id', 'session_30', 'expanding_session_time_minutes']], on=['user_id', 'session_30'], how='left')\n",
    "    df['expanding_session_time_minutes'] = df['expanding_session_time_minutes'].fillna(0)\n",
    "    logger.info('Expanding session time joined to df')\n",
    "    return df\n",
    "\n",
    "def time_between_sessions(df, session_min_max, logger):\n",
    "\n",
    "    session_min_max = session_min_max.sort_values(by=['date_time_min', 'user_id'])\n",
    "    session_min_max['previous_session_end'] = session_min_max.groupby(['user_id'])['date_time_max'].shift(1)\n",
    "    \n",
    "    session_min_max['previous_session_exists'] = session_min_max['previous_session_end'].notnull()\n",
    "\n",
    "    session_min_max['previous_session_end'] = session_min_max[['previous_session_exists', 'previous_session_end', 'date_time_min']] \\\n",
    "        .apply(lambda x: x['date_time_min'] if not x['previous_session_exists'] else x['previous_session_end'], axis=1)\n",
    "\n",
    "    session_min_max = session_min_max.to_pandas()\n",
    "    logger.info('Calculating time between sessions minutes on cpu')\n",
    "    session_min_max['time_between_sessions_minutes'] = (session_min_max['date_time_min'] - session_min_max['previous_session_end']).dt.total_seconds() / 60\n",
    "  \n",
    "    session_min_max = session_min_max.drop(columns=['previous_session_exists'])\n",
    "    session_min_max = session_min_max.set_index(['date_time_min', 'session_30']) \\\n",
    "        .groupby('user_id')['time_between_sessions_minutes'] \\\n",
    "        .expanding(1) \\\n",
    "        .sum() \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'time_between_sessions_minutes': 'expanding_session_delta_minutes'}) \\\n",
    "        \n",
    "    session_min_max['expanding_session_delta_minutes'] = (session_min_max['expanding_session_delta_minutes'] / (session_min_max['session_30'] - 1)).fillna(0)\n",
    "\n",
    "    session_min_max = pd.from_pandas(session_min_max) \n",
    "    df = pd.merge(df, session_min_max[['user_id', 'session_30', 'expanding_session_delta_minutes']], on=['user_id', 'session_30'], how='left') \n",
    "    df['expanding_session_delta_minutes'] = df['expanding_session_delta_minutes'].fillna(0)\n",
    "       \n",
    "    logger.info('Time between sessions joined to df')\n",
    "    df = df.sort_values(by='date_time')\n",
    "    \n",
    "    logger.info('DF resorted by date_time')\n",
    "    return df\n",
    "\n",
    "def assign_metadata(df, logger):\n",
    "    logger.info(f'Obtaining global session time and user events')\n",
    "    global_events_user = df.groupby('user_id')['cum_platform_events'].max().reset_index().rename(columns={'cum_platform_events': 'global_events_user'})\n",
    "    global_session_time = df.groupby('user_id')['cum_platform_time_minutes'].max().reset_index().rename(columns={'cum_platform_time_minutes': 'global_session_time_minutes'})\n",
    "    \n",
    "    metadata = global_events_user.set_index('user_id').join(global_session_time.set_index('user_id'))\n",
    "     \n",
    "    logger.info('Joining metadata to df')\n",
    "    df = pd.merge(df, metadata, on='user_id', how='left')\n",
    "    return df\n",
    "    \n",
    "def main(args):\n",
    "    #\n",
    "\n",
    "    np.set_printoptions(suppress=True)\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    logger =  get_logger()\n",
    "    logger.info(f'Running feature calculation with args')\n",
    "    logger.info(pformat(args.__dict__))\n",
    "\n",
    "    files = glob.glob(f'{args.input_path}/*.parquet')\n",
    "    files = list(sorted(files))\n",
    "    logger.info(f'Found {len(files)} files in {args.input_path}')\n",
    "    logger.info(pformat(f'Loading data from'))\n",
    "    logger.info(pformat(files[:args.data_subset]))\n",
    "    df = pd.read_parquet(files[:args.data_subset], columns=INITIAL_LOAD_COLUMNS)\n",
    "    logger.info(f'Loaded data: shape = {df.shape}, min_date, max_date: {df.date_time.min()}, {df.date_time.max()}')\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    logger.info(f'Sorting data by date_time')\n",
    "    df = df.sort_values(by='date_time')\n",
    "    logger.info('Finished sorting data: encoding value counts')\n",
    "    df = encode_counts(df)\n",
    "    logger.info('Finished encoding value counts: encoding time features')\n",
    "    df = time_encodings(df) \n",
    "    \n",
    "    logger.info('Time encodings complete: encoding categorical features')\n",
    "    \n",
    "    df['country'] = df['country'].astype('category')\n",
    "    df['project_id'] = df['project_id'].astype('category')\n",
    "    df['user_id'] = df['user_id'].astype('category')\n",
    "    \n",
    "    \n",
    "    logger.info('Categorical features encoded: calculating intra-session stats')\n",
    "    df = intra_session_stats(df, logger)\n",
    "    logger.info('Beginning rolling window 10 calculation')\n",
    "    \n",
    "    df = rolling_window_session_10(df, logger)\n",
    "    logger.info('Rolling window 10 calculation complete: beginning expanding window calculation')\n",
    "    \n",
    "    df = expanding_session_time_delta(df, logger)\n",
    "    logger.info('Expanding window calculation complete: returning to dask')\n",
    "    \n",
    "    logger.info(f'Calculating running user stats')\n",
    "    df = running_user_stats(df, logger)\n",
    "    \n",
    "    logger.info('Calculating between session stats')\n",
    "   \n",
    "    session_inflection_times = df.groupby(['user_id', 'session_30']).agg({'date_time': ['min', 'max']}).reset_index()\n",
    "    session_inflection_times.columns = session_inflection_times.columns.map('_'.join).str.strip()\n",
    "\n",
    "    session_inflection_times = session_inflection_times.rename(columns={'user_id_': 'user_id', 'session_30_': 'session_30'})\n",
    "    session_inflection_times = session_inflection_times.sort_values(by=['date_time_min', 'user_id'])\n",
    "    \n",
    "    logger.info('Session inflection times calculated: columns')\n",
    "    logger.info(pformat(session_inflection_times.columns))\n",
    "    logger.info('Calculating time within session')\n",
    "    df = expanding_session_time(df, session_inflection_times.copy(), logger)\n",
    "    logger.info('Calculating time between sessions')\n",
    "    print(session_inflection_times.columns)\n",
    "    df = time_between_sessions(df, session_inflection_times.copy(), logger)\n",
    "    df['session_30_raw'] = df['session_30']\n",
    "\n",
    "    logger.info('Assigning metadata')\n",
    "    df = assign_metadata(df, logger)\n",
    "    logger.info('Metadata assigned: dropping columns')\n",
    "    \n",
    "    logger.info('Returning df to dask for writing to disk')\n",
    "       \n",
    "    output_path = os.path.join(args.output_path, f'files_used_{args.data_subset}')\n",
    "    logger.info(f'Writing to {output_path}')\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)    \n",
    "\n",
    "    df = df.rename(columns={'label_30': 'session_terminates_30_minutes'})\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'category':\n",
    "            logger.info(f'Converting {col} to int')\n",
    "            df[col] = df[col].astype(int)\n",
    "   \n",
    "    df = dd.from_cudf(df, npartitions=30)\n",
    "    \n",
    "    logger.info(f'df converted to dask: shape -> {df.shape}')\n",
    "    df = df.sort_values(by='date_time').reset_index(drop=True).to_parquet(output_path)\n",
    "\n",
    "    logger.info('Finished writing to disk')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self):\n",
    "        self.input_path = 'labelled_session_count_data/'\n",
    "        self.output_path = 'calculated_features/'\n",
    "        self.data_subset = 31\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('calculated_features/files_used_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "session_terminates_30_minutes\n",
      "date_time\n",
      "session_5\n",
      "session_30\n",
      "user_count\n",
      "user_id\n",
      "project_count\n",
      "project_id\n",
      "country_count\n",
      "country\n",
      "timestamp_raw\n",
      "cum_session_event_count\n",
      "delta_last_event\n",
      "cum_session_time_minutes\n",
      "rolling_10\n",
      "expanding_click_average\n",
      "cum_platform_time_minutes\n",
      "cum_platform_events\n",
      "cum_projects\n",
      "average_event_time\n",
      "expanding_session_time_minutes\n",
      "expanding_session_delta_minutes\n",
      "session_30_raw\n",
      "global_events_user\n",
      "global_session_time_minutes\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    print(col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1da7ff4cdcccf44e7e228c52b231f7d5c5854d5618af555ed3871fd5cba609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
