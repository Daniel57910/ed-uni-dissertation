{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.25.91 requires botocore==1.27.90, but you have botocore 1.29.157 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gym stable-baselines3[extra] boto3 scipy python-dotenv --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant\n",
    "FEATURE_COLUMNS = [\n",
    "    \n",
    "    \"user_count\",\n",
    "    \"project_count\" ,\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event\",\n",
    "    \"cum_session_time\",\n",
    "    \"expanding_click_average\",\n",
    "   \n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_event\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"delta_last_event\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_count_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"cum_session_event_raw\",\n",
    "    \"date_time\"\n",
    "]\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'session_minutes',\n",
    "    'size_cutoff',\n",
    "    'time_cutoff',\n",
    "    'reward'\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    \"label\",\n",
    "    \"pred\"\n",
    "]\n",
    "\n",
    "LOAD_COLS = list(set(FEATURE_COLUMNS + METADATA + RL_STAT_COLS + PREDICTION_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment\n",
    "# %load environment\n",
    "# %load environment\n",
    "import gym\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "MAX_EVAL_SIZE = 75\n",
    "\n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, dataset, out_features, n_sequences, params=None):\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.unique_sessions = self.dataset[['user_id', 'session_30_count_raw']].drop_duplicates()\n",
    "        self.n_sequences = n_sequences\n",
    "        self.current_session = None\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        self.n_sequences = n_sequences\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        max_session_size = self.dataset['session_size'].max()\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=91, shape=(len(out_features) + 3, n_sequences + 1), dtype=np.float32)\n",
    "        self.episode_bins = []\n",
    "        self.exp_runs = 0\n",
    "        self.params = params\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        random_session = np.random.randint(0, self.unique_sessions.shape[0])\n",
    "        \n",
    "        user_to_run, session_to_run = self.unique_sessions.iloc[random_session][['user_id', 'session_30_count_raw']]\n",
    "        self.current_session = self._get_events(user_to_run, session_to_run)\n",
    "        self.metadata = self._metadata()\n",
    "        self.current_session_index = 1\n",
    "        self.reward = 0\n",
    "        return self._state()\n",
    "    \n",
    "    def _row_to_dict(self, metadata):\n",
    "        \"\"\"\n",
    "        Convert a row of metadata to a dictionary.\n",
    "        \"\"\"\n",
    "        return metadata.to_dict()\n",
    "    \n",
    "    def _reward_exp(self, cum_session_event_raw):\n",
    "        \"\"\"\n",
    "        Reward shaping as\n",
    "            0 if cum_session_event_raw < size_cutoff\n",
    "            (cum_session_event_raw - size_cutoff) * (cum_session_event_raw / size_cutoff) otherwise\n",
    "        \"\"\"\n",
    "        if cum_session_event_raw <= self.metadata['size_cutoff']:\n",
    "            return cum_session_event_raw / self.metadata['size_cutoff']\n",
    "        \n",
    "        return (cum_session_event_raw - self.metadata['size_cutoff']) * (cum_session_event_raw / self.metadata['size_cutoff'])\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        self._take_action(action)\n",
    "            \n",
    "        next_state, done, meta = self._calculate_next_state()\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            current_session_index = self.current_session_index if \\\n",
    "                self.current_session_index != self.current_session.shape[0] else self.current_session.shape[0] - 1\n",
    "            \n",
    "            self.exp_runs += 1\n",
    "            self.metadata['ended_event'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['ended_time'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            self.metadata['exp_runs'] = self.exp_runs\n",
    "            self.episode_bins.append(self._row_to_dict(self.metadata))\n",
    "            \n",
    "            self.metadata['ended_event'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['ended_time'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            self.metadata['exp_runs'] = self.exp_runs\n",
    "            self.episode_bins.append(self._row_to_dict(self.metadata))\n",
    "           \n",
    "            cum_session_event_raw = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            reward_exp = self._reward_exp(cum_session_event_raw)\n",
    "            \n",
    "            return next_state, reward_exp , done, {}\n",
    "        else:\n",
    "            self.reward = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "            cum_session_event_raw = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "            \n",
    "            reward_exp = self._reward_exp(cum_session_event_raw)\n",
    "    \n",
    "            self.current_session_index += 1        \n",
    "            \n",
    "            return next_state, reward_exp, done, meta\n",
    "    \n",
    "    def _metadata(self):\n",
    "        session_metadata = self.current_session.iloc[0][RL_STAT_COLS].copy()\n",
    "        session_metadata['ended'] = 0\n",
    "        for meta_col in ['small', 'medium', 'large']:\n",
    "            session_metadata[f'inc_{meta_col}'] = 0\n",
    "            session_metadata[f'time_{meta_col}'] = 0\n",
    "\n",
    "        return session_metadata\n",
    "    \n",
    "    def flush_episode_bins(self):\n",
    "        episode_bins = self.episode_bins.copy()\n",
    "        self.episode_bins = []\n",
    "        return episode_bins\n",
    "    \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        if (self.current_session_index == self.current_session.shape[0]):\n",
    "            return None, True, {}\n",
    "\n",
    "        if self._continuing_in_session():\n",
    "            return self._state(), False, {}\n",
    "    \n",
    "        return None, True, {}\n",
    "         \n",
    "    def _continuing_in_session(self):\n",
    "        event_cutoff = self.current_session.iloc[self.current_session_index]['size_cutoff']\n",
    "        current_session_event = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "        if current_session_event <= event_cutoff or current_session_event  >= MAX_EVAL_SIZE:\n",
    "            return True\n",
    "        \n",
    "        param_mid = 0.1 if not self.params else self.params['mid']\n",
    "        param_large = 0.2 if not self.params else self.params['large']\n",
    "        param_window = 0.75 if not self.params else self.params['window']\n",
    "    \n",
    "        extending_low = self._probability_extending(current_session_event, self.metadata['inc_small']) - \\\n",
    "            (0.05 + np.random.normal(-0.02, 0.1, 100).mean())\n",
    "\n",
    "            \n",
    "        extending_medium = self._probability_extending(current_session_event, self.metadata['inc_medium']) - \\\n",
    "            (param_mid + np.random.normal(-0.02, 0.1, 100).mean()) \n",
    "            \n",
    "        extending_large = self._probability_extending(current_session_event, self.metadata['inc_large']) + \\\n",
    "            (param_large + np.random.normal(-0.02, 0.1, 100).mean())\n",
    "            \n",
    "        return any([\n",
    "            extending_low > 0.4 and extending_low <= param_window,\n",
    "            extending_medium > 0.4 and extending_medium <= param_window,\n",
    "            extending_large > 0.4 and extending_large <= param_window\n",
    "        ])\n",
    "        \n",
    "           \n",
    "    \n",
    "    def _probability_extending(self, current_session_event, incentive_event):\n",
    "        if incentive_event == 0:\n",
    "            return 0\n",
    "         \n",
    "        continue_session = norm(\n",
    "            loc=max(incentive_event, 1),\n",
    "            scale=max(incentive_event *.75, 1)\n",
    "        ).cdf(max(current_session_event, 1)) \n",
    "        \n",
    "        return continue_session\n",
    "        \n",
    "\n",
    "    def _get_events(self, user_id, session):\n",
    "        subset = self.dataset[\n",
    "            (self.dataset['user_id'] == user_id) &\n",
    "            (self.dataset['session_30_count_raw'] == session).copy()\n",
    "        ]\n",
    "\n",
    "        subset = subset.sort_values(by=['date_time'])\n",
    "        return subset\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        if action == 0:\n",
    "            return 1\n",
    "        \n",
    "        current_session_index = self.current_session_index if \\\n",
    "            self.current_session_index != self.current_session.shape[0] else self.current_session.shape[0] - 1\n",
    "    \n",
    "        if action == 1:\n",
    "            if self.metadata['inc_small'] > 0:\n",
    "                return 1\n",
    "\n",
    "            self.metadata['inc_small'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_small'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "    \n",
    "        elif action == 2:\n",
    "            if self.metadata['inc_medium'] > 0:\n",
    "                return 1\n",
    "            self.metadata['inc_medium'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_medium'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            if self.metadata['inc_large'] > 0:\n",
    "                return 1\n",
    "            self.metadata['inc_large'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['time_large'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "            return 1\n",
    "\n",
    "    def _state(self):\n",
    "\n",
    "        if self.current_session_index > self.n_sequences:\n",
    "            events = self.current_session.iloc[self.current_session_index - (self.n_sequences + 1):self.current_session_index][self.out_features]\n",
    "            events['inc_small'] = self.metadata['inc_small']\n",
    "            events['inc_medium'] = self.metadata['inc_medium']\n",
    "            events['inc_large'] = self.metadata['inc_large']\n",
    "            \n",
    "            events = events.values\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            delta = min((self.n_sequences + 1)- self.current_session_index, self.n_sequences)\n",
    "            zero_cat = np.zeros((delta, len(self.out_features) + 3))\n",
    "            events = self.current_session.iloc[:max(self.current_session_index, 1)][self.out_features]\n",
    "            \n",
    "            events['inc_small'] = self.metadata['inc_small']\n",
    "            events['inc_medium'] = self.metadata['inc_medium']\n",
    "            events['inc_large'] = self.metadata['inc_large']\n",
    "            \n",
    "            \n",
    "            events = np.concatenate((zero_cat, events), axis=0)\n",
    "        \n",
    "        return events.astype(np.float32).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_sensitivity_analysis.py\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import boto3\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "global logger, client\n",
    "logger = logging.getLogger(__name__)\n",
    "client = boto3.client('s3')\n",
    "import numpy as np\n",
    "from itertools import combinations, product\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "import pandas as pd\n",
    "import glob\n",
    "MIN_MAX_RANGE = (10, 90)\n",
    "from tqdm import tqdm\n",
    "N_SEQUENCES = 15\n",
    "import argparse\n",
    "S3_BASELINE_PATH = 'dissertation-data-dmiller'\n",
    "\n",
    "SENSITIVITY_PARAMS = {\n",
    "    \"window\": (.8,  .6),\n",
    "    \"mid\": {.15, .04},\n",
    "    \"large\": {.3, .09},\n",
    "    \n",
    "}\n",
    "import torch\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "def parse_args():\n",
    "    parse = argparse.ArgumentParser()\n",
    "    parse.add_argument('--algo', type=str, default='dqn_pred_cnn'),\n",
    "    parse.add_argument('--run_date', type=str, default='2023-06-13_16-11-42'),\n",
    "    parse.add_argument('--write_path', type=str, default='rl_evaluation'),\n",
    "    parse.add_argument('--n_files', type=int, default=2),\n",
    "    args = parse.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def find_s3_candidate(algo, run_date):\n",
    "    \n",
    "    folder_prefix = os.path.join(\n",
    "        'experiments',\n",
    "        algo,\n",
    "        run_date,\n",
    "        'checkpoints'\n",
    "    )\n",
    "\n",
    "    \n",
    "    logger.info(f'Looking for files in {folder_prefix}')\n",
    "    \n",
    "    files = [\n",
    "        {\n",
    "            'key': file['Key'],\n",
    "            'last_modified': file['LastModified'],\n",
    "            'check_index': int(re.sub('[^0-9]', '', file['Key'].split('/')[-1]))\n",
    "        }\n",
    "        for file in client.list_objects_v2(Bucket=S3_BASELINE_PATH, Prefix=folder_prefix)['Contents']\n",
    "    ]\n",
    "    \n",
    "    s3_candidate = sorted(files, key=lambda x: x['check_index'])[-1]['key']\n",
    "    \n",
    "    logger.info(f'Found candiate: {s3_candidate}')\n",
    "    \n",
    "    return s3_candidate\n",
    "\n",
    "def get_policy(algo, run_date):\n",
    "        \n",
    "    s3_candidate = find_s3_candidate(algo, run_date)\n",
    "    if not os.path.exists(os.path.dirname(s3_candidate)):\n",
    "        logger.info(f'Creating policy path {os.path.dirname(s3_candidate)}')\n",
    "        \n",
    "        os.makedirs(os.path.dirname(s3_candidate), exist_ok=True)\n",
    "       \n",
    "    # return s3_candidate \n",
    "    client.download_file(S3_BASELINE_PATH, s3_candidate, s3_candidate)\n",
    "    return s3_candidate\n",
    "        \n",
    "\n",
    "def simplify_experiment(vectorized_df):\n",
    "    vectorized_df = [\n",
    "        df[(df['session_size'] >= MIN_MAX_RANGE[0]) & (df['session_size'] <= MIN_MAX_RANGE[1])] for df in vectorized_df\n",
    "    ]\n",
    "    \n",
    "    vectorized_df = [\n",
    "        df for df in vectorized_df if len(df) > 0\n",
    "    ]\n",
    "\n",
    "    return vectorized_df\n",
    "\n",
    "\n",
    "def _label_or_pred(algo):\n",
    "    if 'label' in algo:\n",
    "        return 'label'\n",
    "    elif 'pred' in algo:\n",
    "        return 'pred'\n",
    "    else:\n",
    "        return None\n",
    "   \n",
    "\n",
    "def run_sensitivity_analysis(env_datasets, policy_path, feature_cols, param_combos): \n",
    "    \n",
    "    p_bar = tqdm(param_combos, unit='item')\n",
    "    out_df_container = []\n",
    "    for combo in p_bar:\n",
    "        params = {\n",
    "            \"window\": combo[0].round(2),\n",
    "            \"mid\": combo[1].round(2),\n",
    "            \"large\": combo[2].round(2)\n",
    "        }\n",
    "        p_bar.set_description(f'Running combo {params}')\n",
    "        \n",
    "        vev_envs = DummyVecEnv([lambda: CitizenScienceEnv(dataset, feature_cols, N_SEQUENCES, params) for dataset in env_datasets])\n",
    "        \n",
    "        vec_monitor = VecMonitor(vev_envs)\n",
    "        \n",
    "        model = DQN.load(\n",
    "            policy_path,\n",
    "            env=vec_monitor,\n",
    "            verbose=0,\n",
    "        )\n",
    "        \n",
    "        evaluate_policy(\n",
    "            model,\n",
    "            model.get_env(),\n",
    "            deterministic=False,\n",
    "            n_eval_episodes=2_000\n",
    "        )\n",
    "        \n",
    "        dists = model.get_env().get_attr('episode_bins')\n",
    "        values_to_log = [item for sublist in dists for item in sublist if len(sublist) > 0]\n",
    "        out_df = pd.DataFrame(values_to_log)\n",
    "        out_df['window'] = params['window']\n",
    "        out_df['mid'] = params['mid']\n",
    "        out_df['large'] = params['large']\n",
    "        out_df_container.append(out_df)\n",
    "    \n",
    "    \n",
    "    return pd.concat(out_df_container)\n",
    "        \n",
    "\n",
    "def main(args):\n",
    "    algo, run_date, write_path, n_files = args.algo, args.run_date, args.write_path, args.n_files\n",
    "\n",
    "    params_window = np.arange(*SENSITIVITY_PARAMS['window'], -.02).tolist()\n",
    "    params_mid = np.arange(*SENSITIVITY_PARAMS['mid'], -.01).tolist()\n",
    "    params_large = np.arange(*SENSITIVITY_PARAMS['large'], -.02).tolist()\n",
    "    \n",
    "    logger.info(f'Window params: {params_window}')\n",
    "    logger.info(f'Mid params: {params_mid}')\n",
    "    logger.info(f'Large params: {params_large}')\n",
    "    \n",
    "    param_combos = np.array(list(product(params_window, params_mid, params_large)))\n",
    "    logger.info(f'Combination parameters obtained: {param_combos.shape}, running monte carlo simulation on 200 random samples')\n",
    "    param_combos = param_combos[np.random.choice(param_combos.shape[0], 200, replace=False), :]\n",
    "    \n",
    "    policy_path = get_policy(algo, run_date)\n",
    "    logger.info(f'Policy path downloaded, evaluating experiment: {policy_path}')\n",
    "    \n",
    "    read_path = os.path.join('rl_ready_data_conv', f'files_used_{n_files}', 'window_1', 'batched_eval')\n",
    "    files_to_read = glob.glob(os.path.join(read_path, '*.parquet'))\n",
    "    logger.info(f'Found {len(files_to_read)} files to read')\n",
    "    env_datasets = [\n",
    "        pd.read_parquet(file, columns=LOAD_COLS) for file in files_to_read\n",
    "    ]\n",
    "\n",
    "    env_datasets = simplify_experiment(env_datasets)\n",
    "    feature_cols = FEATURE_COLUMNS + [_label_or_pred(algo)] if _label_or_pred(algo) else FEATURE_COLUMNS\n",
    "    logger.info(f'Length of features: {len(feature_cols)}')\n",
    "    logger.info(f'Running sensitivity analysis per monte carlo simulation')\n",
    "    sensitivity_df = run_sensitivity_analysis(env_datasets, policy_path, feature_cols, param_combos)\n",
    "\n",
    "    \n",
    "    write_path = os.path.join(write_path, f'sensitivity_analysis', f'{algo}.parquet')\n",
    "    if not os.path.exists(os.path.dirname(write_path)):\n",
    "        logger.info(f'Creating write path {os.path.dirname(write_path)}')\n",
    "        os.makedirs(os.path.dirname(write_path), exist_ok=True)\n",
    "    \n",
    "    logger.info(f'Writing sensitivity analysis to {write_path}')\n",
    "    sensitivity_df.to_parquet(write_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    algo = 'dqn_pred_cnn'\n",
    "    run_date = '2023-06-20_10-38-22'\n",
    "    write_path = 'rl_evaluation'\n",
    "    n_files = 30\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/21/2023 07:47:59 AM Window params: [0.8, 0.78, 0.76, 0.74, 0.72, 0.7, 0.6799999999999999, 0.6599999999999999, 0.6399999999999999, 0.6199999999999999, 0.5999999999999999]\n",
      "06/21/2023 07:47:59 AM Mid params: [0.15, 0.13999999999999999, 0.12999999999999998, 0.11999999999999997, 0.10999999999999996, 0.09999999999999995, 0.08999999999999994, 0.07999999999999993, 0.06999999999999992, 0.059999999999999915, 0.049999999999999906]\n",
      "06/21/2023 07:47:59 AM Large params: [0.3, 0.27999999999999997, 0.25999999999999995, 0.23999999999999994, 0.21999999999999992, 0.1999999999999999, 0.17999999999999988, 0.15999999999999986, 0.13999999999999985, 0.11999999999999983, 0.09999999999999981]\n",
      "06/21/2023 07:47:59 AM Combination parameters obtained: (1331, 3), running monte carlo simulation on 200 random samples\n",
      "06/21/2023 07:47:59 AM Looking for files in experiments/dqn_pred_cnn/2023-06-20_10-38-22/checkpoints\n",
      "06/21/2023 07:48:00 AM Found candiate: experiments/dqn_pred_cnn/2023-06-20_10-38-22/checkpoints/rl_model_6600000_steps.zip\n",
      "06/21/2023 07:48:00 AM Policy path downloaded, evaluating experiment: experiments/dqn_pred_cnn/2023-06-20_10-38-22/checkpoints/rl_model_6600000_steps.zip\n",
      "06/21/2023 07:48:00 AM Found 100 files to read\n",
      "06/21/2023 07:48:02 AM Length of features: 23\n",
      "06/21/2023 07:48:02 AM Running sensitivity analysis per monte carlo simulation\n",
      "Running combo {'window': 0.62, 'mid': 0.13, 'large': 0.28}:   6%|▌         | 11/200 [17:38<5:04:59, 96.82s/item]"
     ]
    }
   ],
   "source": [
    "main(Args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
