{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly as px\n",
    "import plotly.express as px\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_AUC_PATH = 'auroc_eval_data'\n",
    "CORE_CSV_PATH = 'result_csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_glob_auc(read_path, experiment):\n",
    "    print(f'Loading data')\n",
    "    df = pd.read_parquet(read_path)\n",
    "    df['row_count'] = df.index.values\n",
    "    eval_cutoff = df.shape[0] * .5\n",
    "    df['subset'] = df['row_count'].apply(lambda x: 'eval' if x < eval_cutoff else 'test')\n",
    "    df = df.drop(columns=['row_count'])\n",
    "\n",
    "    eval_df = df[df['subset'] == 'eval']\n",
    "    test_df = df[df['subset'] == 'test']\n",
    "    auc_container = []\n",
    "    for col in [col for col in eval_df.columns if 'LSTM' in col]:\n",
    "        print(f'Calculating AUROC for {col}')\n",
    "        auc_container.append(\n",
    "            {\n",
    "                'Model': col,\n",
    "                'AUROC': roc_auc_score(eval_df['label'], eval_df[col]),\n",
    "                'Acc': accuracy_score(eval_df['label'], eval_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Prec': precision_score(eval_df['label'], eval_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Rec': recall_score(eval_df['label'], eval_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Subset': 'eval'\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        auc_container.append(\n",
    "            {\n",
    "                'Model': col,\n",
    "                'AUROC': roc_auc_score(test_df['label'], test_df[col]),\n",
    "                'Acc': accuracy_score(test_df['label'], test_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Prec': precision_score(test_df['label'], test_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Rec': recall_score(test_df['label'], test_df[col].apply(lambda x: 1 if x > .5 else 0)),\n",
    "                'Subset': 'test'\n",
    "            }\n",
    "        )\n",
    "\n",
    "    out_df = pd.DataFrame(auc_container)\n",
    "    out_df['Experiment'] = experiment\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def plot_subset(df, target_var):\n",
    "    eval_exp, test_exp = df[df['Subset'] == 'eval'], df[df['Subset'] == 'test']\n",
    "    \n",
    "    eval_fig = px.line(eval_exp, x='Window', y=target_var, color='Experiment', markers=True, title=f'Evaluation {target_var} Across Prediction Experiments')\n",
    "    test_fig = px.line(test_exp, x='Window', y=target_var, color='Experiment', markers=True, title=f'Test {target_var} Across Prediction Experiments')\n",
    "\n",
    "    eval_fig.update_layout(\n",
    "        xaxis_title='Data Window Size',\n",
    "        yaxis_title=target_var,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "   \n",
    "    test_fig.update_layout(\n",
    "        xaxis_title='Data Window Size',\n",
    "        yaxis_title=target_var,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "     \n",
    "    eval_fig.write_image(f'lstm_plots/metric_graphs/{target_var}_eval.png')\n",
    "    test_fig.write_image(f'lstm_plots/metric_graphs/{target_var}_test.png')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('result_csv/result_summary_eval_test.csv')\n",
    "df['Window'] = df['Model'].apply(lambda x: 30 if 'H' in x else int(x.split(' ')[-1]))\n",
    "df = df[df['Model'] != 'LSTM SEQ 30 H']\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'Acc': 'Accuracy',\n",
    "    'Prec': 'Precision',\n",
    "    'Rec': 'Recall'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subset(df, 'AUROC')\n",
    "plot_subset(df, 'Accuracy')\n",
    "plot_subset(df, 'Precision')\n",
    "plot_subset(df, 'Recall')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
