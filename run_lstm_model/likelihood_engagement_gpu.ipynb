{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.143 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch python-dotenv boto3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: aws: not found\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dissertation-data-dmiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import boto3\n",
    "from pprint import pformat\n",
    "from torch import nn\n",
    "import io\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time\",\n",
    "    \n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "LOAD_COLS = LABEL + METADATA + OUT_FEATURE_COLUMNS\n",
    "\n",
    "S3_BUCKET = 'dissertation-data-dmiller'\n",
    "BASE_CHECK_PATH = 'lstm_experiments/checkpoints'\n",
    "\n",
    "LSTM_CHECKPOINTS = {\n",
    "    'seq_1': 'lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_1/data_partition_None/2023_05_03_11_10/clickstream-epoch=37-loss_valid=0.59.ckpt',\n",
    "    'seq_10': 'lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_10/data_partition_None/2023_04_28_19_18/clickstream-epoch=41-loss_valid=0.59.ckpt',\n",
    "    'seq_20': 'lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_20/data_partition_None/2023_04_28_23_06/clickstream-epoch=29-loss_valid=0.59.ckpt',\n",
    "    'seq_30': 'lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_30/data_partition_None/2023_04_30_15_57/clickstream-epoch=25-loss_valid=0.59.ckpt',\n",
    "    'seq_30_heuristic': 'lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_30/data_partition_None/2023_04_30_15_57/clickstream-epoch=25-loss_valid=0.59.ckpt',\n",
    "    'seq_40': 'lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_40/data_partition_None/2023_05_01_07_09/clickstream-epoch=17-loss_valid=0.59.ckpt'\n",
    "}\n",
    "\n",
    "LSTM_CHECKPOINT_EMBEDDING = {\n",
    "    'embedding_30': 'lstm_experiments/checkpoints/data_v1/n_files_30/embedded_ordinal_heuristic/sequence_length_30/data_partition_None/2023_05_10_14_18/clickstream-epoch=05-loss_valid=0.62.ckpt',\n",
    "    'embedding_30_heuristic': 'lstm_experiments/checkpoints/data_v1/n_files_30/embedded_ordinal_heuristic/sequence_length_30/data_partition_None/2023_05_10_14_18/clickstream-epoch=05-loss_valid=0.62.ckpt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data_module\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "LABEL_INDEX = 1\n",
    "TOTAL_EVENTS_INDEX = 2\n",
    "BATCHES = 1000000\n",
    "\n",
    "    \n",
    "class ClickstreamDataset(Dataset):\n",
    "    def __init__(self, dataset_pointer_list) -> None:\n",
    "        \"\"\"\n",
    "        Yield data in batches of BATCHES\n",
    "        \"\"\"\n",
    "        self.events = dataset_pointer_list\n",
    "        self.size = self.events[0].shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        events = [np.array([event[idx]]) for event in self.events]\n",
    "        return np.concatenate(events, axis=1)\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load torch_model_bases\n",
    "import torch \n",
    "from torch import nn\n",
    "N_FEATURES = 20\n",
    "class LSTMOrdinal(nn.Module):\n",
    "    def __init__(self,  hidden_size=32, dropout=0.2) -> None:\n",
    "        super(LSTMOrdinal, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=N_FEATURES,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.output = nn.Linear(\n",
    "            hidden_size,\n",
    "            1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1]\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load npz_extractor\n",
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "class NPZExtractor:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, input_path, n_files, n_sequences, s3_client, data_partition) -> None:\n",
    "        self.input_path = input_path\n",
    "        self.n_files = n_files\n",
    "        self.n_sequences = n_sequences\n",
    "        self.s3_client = s3_client\n",
    "        self.data_partition = data_partition\n",
    "\n",
    "\n",
    "    def get_dataset_pointer(self):\n",
    "\n",
    "        read_path = os.path.join(self.input_path, f'files_used_{self.n_files}')\n",
    "        if not os.path.exists(read_path):\n",
    "            self.logger.info(f'Creating directory: {read_path}')\n",
    "            os.makedirs(read_path)\n",
    "\n",
    "\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            key_zip, key_npy = (\n",
    "                os.path.join(read_path, f'sequence_index_{_}.npz'),\n",
    "                os.path.join(read_path, f'sequence_index_{_}')\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f'Loading pointer to dataset: {key_npy}: derived from {key_zip}')\n",
    "\n",
    "\n",
    "            if not os.path.exists(key_zip):\n",
    "                self.logger.info(f'Zip file to extract: {key_zip}: npy file to load: {key_npy}')\n",
    "                self.s3_client.download_file(\n",
    "                    'dissertation-data-dmiller',\n",
    "                    key_zip,\n",
    "                    key_zip\n",
    "                )\n",
    "            if not os.path.exists(key_npy):\n",
    "                self.logger.info(f'Zip file downloaded: {key_zip}: npy file to load: {key_npy}')\n",
    "\n",
    "                self._zip_extract(key_zip, key_npy)\n",
    "\n",
    "        lz_concatenated_results = self._lazy_concatenate()\n",
    "\n",
    "        if self.data_partition:\n",
    "            return [p[:self.data_partition] for p in lz_concatenated_results]\n",
    "        else:\n",
    "            return lz_concatenated_results\n",
    "\n",
    "\n",
    "    def _zip_extract(self, key_zip, key_npy):\n",
    "        self.logger.info(f'Extracting file: {key_zip} -> {key_npy}')\n",
    "\n",
    "        with zipfile.ZipFile(key_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=key_npy, members=['arr_0.npy'])\n",
    "\n",
    "        self.logger.info(f'Zip file exracted: {key_zip} -> {key_npy}/arr_0.npy')\n",
    "\n",
    "    def _lazy_concatenate(self):\n",
    "        lz_concat = []\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            path_to_load = os.path.join(self.input_path, f'files_used_{self.n_files}', f'sequence_index_{_}', f'arr_0.npy')\n",
    "            self.logger.info(f'Loading: {path_to_load}')\n",
    "            lz_concat.append(np.load(path_to_load, mmap_mode='r'))\n",
    "        return lz_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load likelihood_engagement_cpu\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import logging\n",
    "import boto3\n",
    "from pprint import pformat\n",
    "from torch import nn\n",
    "import io\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)    \n",
    "pd.set_option('mode.use_inf_as_na', True)\n",
    "torch.set_printoptions(sci_mode=False, linewidth=400, precision=2)\n",
    "np.set_printoptions(suppress=True, precision=4, linewidth=200)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "CHECKPOINT_DIR='s3://dissertation-data-dmiller/lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_10/data_partition_None/2023_03_30_07_54'\n",
    "METADATA_INDEX = 14\n",
    "logger = logging.getLogger('likelihood_engagement')\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--n_files', type=int, default=2)\n",
    "    parser.add_argument('--n_sequences', type=int, default=20)\n",
    "    parser.add_argument('--file_path', type=str, default='datasets/torch_ready_data')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default=CHECKPOINT_DIR)\n",
    "    parser.add_argument('--write_path', type=str, default='datasets/rl_ready_data')\n",
    "    parser.add_argument('--model_type', type=str, default='ordinal')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def _extract_features(tensor, n_sequences, n_features):\n",
    "    \n",
    "    features_dict = {}\n",
    "    tensor = tensor.squeeze()\n",
    "       \n",
    "    metadata, features = tensor[:, :METADATA_INDEX], tensor[:, METADATA_INDEX:] \n",
    "                \n",
    "    features = torch.flip(\n",
    "        torch.reshape(features, (features.shape[0], 41, 20)),\n",
    "        dims=[1]\n",
    "    )\n",
    "    \n",
    "    features_dict['features_40'] = features\n",
    "    features_dict['features_30'] = features[:, 10:, :]\n",
    "    features_dict['features_20'] = features[:, 20:, :]\n",
    "    features_dict['features_10'] = features[:, 30:, :]\n",
    "    features_dict['last_sequence'] = features[:, -1, :].unsqueeze(1)\n",
    "     \n",
    "    return metadata, features_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_models(checkpoints: dict, s3_client, device):\n",
    "    \"\"\"_summary_\n",
    "    Downloads models from s3 and loads them into memory.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for name, checkpoint in checkpoints.items():\n",
    "        logger.info(f'Downloading model: {name}')\n",
    "        response = s3_client.get_object(\n",
    "            Bucket=S3_BUCKET,\n",
    "            Key=checkpoint\n",
    "        )\n",
    "        buffer = io.BytesIO(response['Body'].read())\n",
    "        state = torch.load(buffer, map_location=torch.device(device), )\n",
    "        model = LSTMOrdinal()\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        model.to(device)\n",
    "        models[name] = model\n",
    "    return models\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_static_predictions(args):\n",
    "    \n",
    "    user_metadata_container = []\n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    logger.info('Generating static prediction likelihoods for experiment')\n",
    "    npz_extractor = NPZExtractor(\n",
    "        args.file_path,\n",
    "        args.n_files,\n",
    "        args.n_sequences,\n",
    "        client,\n",
    "        None\n",
    "           \n",
    "    )\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'   \n",
    "    logger.info(f'Setting device to {device}')\n",
    "    \n",
    "    logger.info('generating dataset pointer')\n",
    "    dataset = npz_extractor.get_dataset_pointer()\n",
    "    \n",
    "    logger.info('Downloading model checkpoint')\n",
    "    \n",
    "    write_path = os.path.join(args.write_path, f'files_used_{args.n_files}')\n",
    "    if not os.path.exists(write_path):\n",
    "        logger.info(f'Creating directory: {write_path}')\n",
    "        os.makedirs(write_path)\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    logger.info(f'Downloading models from checkpoints {LSTM_CHECKPOINTS.keys()}')\n",
    "    \n",
    "    models = get_models(LSTM_CHECKPOINTS, client, device)\n",
    "    \n",
    "    dataset = ClickstreamDataset(dataset)\n",
    "    loader = DataLoader(dataset, batch_size=2048*8, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    activation = nn.Sigmoid()\n",
    "    \n",
    "\n",
    "    p_bar = tqdm.tqdm(loader, total=len(loader))\n",
    "    \n",
    "    for indx, data in enumerate(p_bar):\n",
    "        p_bar.set_description(f'Processing batch: {indx}')\n",
    "        data = data.to(device)\n",
    "        \n",
    "        metadata, features_dict = _extract_features(data, args.n_sequences + 1, 20)\n",
    "        # logger.info(f'running inference on batch: {indx}')\n",
    "        try:\n",
    "            preds_1 = activation(models['seq_1'](features_dict['last_sequence']))\n",
    "            preds_10 = activation(models['seq_10'](features_dict['features_10']))\n",
    "            preds_20 = activation(models['seq_20'](features_dict['features_20']))\n",
    "            preds_30 = activation(models['seq_30'](features_dict['features_30']))\n",
    "            preds_40 = activation(models['seq_40'](features_dict['features_40']))\n",
    "        except:\n",
    "            raise Exception(f'Error processing batch: {indx}')\n",
    "        \n",
    "        heuristic_data = features_dict['features_30'].clone()\n",
    "        preds_30_heuristic = activation(models['seq_30_heuristic'](heuristic_data))\n",
    "        heuristic_scalar = torch.where(metadata[:, 4] < 25, torch.tensor(0.0), torch.tensor(1.0))\n",
    "        preds_30_heuristic = preds_30_heuristic * heuristic_scalar.unsqueeze(1)\n",
    "\n",
    "        \n",
    "        # logger.info(f'Concatenating metadata and predictions')\n",
    "        user_metadata = torch.cat([metadata, features_dict['last_sequence'].squeeze(), preds_1, preds_10, preds_20, preds_30, preds_30_heuristic, preds_40], dim=1)\n",
    "        user_metadata_container.append(user_metadata)\n",
    "\n",
    "   \n",
    "    predicted_data = torch.cat(user_metadata_container, dim=0)\n",
    "    logger.info(f'Predicted data shape: {predicted_data.shape}: generating df')\n",
    "    predicted_data = pd.DataFrame(predicted_data.cpu().numpy(), columns=LABEL + METADATA + OUT_FEATURE_COLUMNS + ['seq_1', 'seq_10', 'seq_20', 'seq_30', 'seq_30_heuristic', 'seq_40'])\n",
    "    \n",
    "    predicted_data['date_time'] = pd.to_datetime(predicted_data[['year', 'month', 'day', 'hour', 'minute', 'second']], errors='coerce').dropna()\n",
    "\n",
    "    logger.info('Decoding date time and sorting')\n",
    "    write_path = os.path.join(write_path, 'predicted_data.parquet')\n",
    "    logger.info(f'Writing to parquet: {write_path}')\n",
    "    predicted_data.to_parquet(write_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    n_files = 30\n",
    "    n_sequences = 40\n",
    "    file_path = 'torch_ready_data'\n",
    "    write_path = 'rl_ready_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 13:41:29,068 Found credentials in environment variables.\n",
      "2023-05-31 13:41:29,129 Generating static prediction likelihoods for experiment\n",
      "2023-05-31 13:41:29,185 Setting device to cuda\n",
      "2023-05-31 13:41:29,186 generating dataset pointer\n",
      "2023-05-31 13:41:29,187 Creating directory: torch_ready_data/files_used_30\n",
      "2023-05-31 13:41:29,188 Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_0: derived from torch_ready_data/files_used_30/sequence_index_0.npz\n",
      "2023-05-31 13:41:29,188 Zip file to extract: torch_ready_data/files_used_30/sequence_index_0.npz: npy file to load: torch_ready_data/files_used_30/sequence_index_0\n",
      "2023-05-31 13:41:53,853 Zip file downloaded: torch_ready_data/files_used_30/sequence_index_0.npz: npy file to load: torch_ready_data/files_used_30/sequence_index_0\n",
      "2023-05-31 13:41:53,854 Extracting file: torch_ready_data/files_used_30/sequence_index_0.npz -> torch_ready_data/files_used_30/sequence_index_0\n",
      "2023-05-31 13:42:14,150 Zip file exracted: torch_ready_data/files_used_30/sequence_index_0.npz -> torch_ready_data/files_used_30/sequence_index_0/arr_0.npy\n",
      "2023-05-31 13:42:14,151 Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_10: derived from torch_ready_data/files_used_30/sequence_index_10.npz\n",
      "2023-05-31 13:42:14,152 Zip file to extract: torch_ready_data/files_used_30/sequence_index_10.npz: npy file to load: torch_ready_data/files_used_30/sequence_index_10\n",
      "2023-05-31 13:42:24,568 Zip file downloaded: torch_ready_data/files_used_30/sequence_index_10.npz: npy file to load: torch_ready_data/files_used_30/sequence_index_10\n",
      "2023-05-31 13:42:24,569 Extracting file: torch_ready_data/files_used_30/sequence_index_10.npz -> torch_ready_data/files_used_30/sequence_index_10\n",
      "2023-05-31 13:42:58,224 Zip file exracted: torch_ready_data/files_used_30/sequence_index_10.npz -> torch_ready_data/files_used_30/sequence_index_10/arr_0.npy\n",
      "2023-05-31 13:42:58,225 Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_20: derived from torch_ready_data/files_used_30/sequence_index_20.npz\n",
      "2023-05-31 13:42:58,351 Zip file to extract: torch_ready_data/files_used_30/sequence_index_20.npz: npy file to load: torch_ready_data/files_used_30/sequence_index_20\n",
      "2023-05-31 13:44:28,670 Zip file downloaded: torch_ready_data/files_used_30/sequence_index_20.npz: npy file to load: torch_ready_data/files_used_30/sequence_index_20\n",
      "2023-05-31 13:44:28,671 Extracting file: torch_ready_data/files_used_30/sequence_index_20.npz -> torch_ready_data/files_used_30/sequence_index_20\n",
      "2023-05-31 13:45:02,809 Zip file exracted: torch_ready_data/files_used_30/sequence_index_20.npz -> torch_ready_data/files_used_30/sequence_index_20/arr_0.npy\n",
      "2023-05-31 13:45:02,810 Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_30: derived from torch_ready_data/files_used_30/sequence_index_30.npz\n",
      "2023-05-31 13:45:02,810 Zip file to extract: torch_ready_data/files_used_30/sequence_index_30.npz: npy file to load: torch_ready_data/files_used_30/sequence_index_30\n",
      "2023-05-31 13:46:29,808 Zip file downloaded: torch_ready_data/files_used_30/sequence_index_30.npz: npy file to load: torch_ready_data/files_used_30/sequence_index_30\n",
      "2023-05-31 13:46:29,808 Extracting file: torch_ready_data/files_used_30/sequence_index_30.npz -> torch_ready_data/files_used_30/sequence_index_30\n",
      "2023-05-31 13:47:02,970 Zip file exracted: torch_ready_data/files_used_30/sequence_index_30.npz -> torch_ready_data/files_used_30/sequence_index_30/arr_0.npy\n",
      "2023-05-31 13:47:02,971 Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_40: derived from torch_ready_data/files_used_30/sequence_index_40.npz\n",
      "2023-05-31 13:47:02,972 Zip file to extract: torch_ready_data/files_used_30/sequence_index_40.npz: npy file to load: torch_ready_data/files_used_30/sequence_index_40\n",
      "2023-05-31 13:48:29,456 Zip file downloaded: torch_ready_data/files_used_30/sequence_index_40.npz: npy file to load: torch_ready_data/files_used_30/sequence_index_40\n",
      "2023-05-31 13:48:29,456 Extracting file: torch_ready_data/files_used_30/sequence_index_40.npz -> torch_ready_data/files_used_30/sequence_index_40\n",
      "2023-05-31 13:49:03,819 Zip file exracted: torch_ready_data/files_used_30/sequence_index_40.npz -> torch_ready_data/files_used_30/sequence_index_40/arr_0.npy\n",
      "2023-05-31 13:49:03,820 Loading: torch_ready_data/files_used_30/sequence_index_0/arr_0.npy\n",
      "2023-05-31 13:49:24,881 Loading: torch_ready_data/files_used_30/sequence_index_10/arr_0.npy\n",
      "2023-05-31 13:49:24,892 Loading: torch_ready_data/files_used_30/sequence_index_20/arr_0.npy\n",
      "2023-05-31 13:49:24,927 Loading: torch_ready_data/files_used_30/sequence_index_30/arr_0.npy\n",
      "2023-05-31 13:49:24,939 Loading: torch_ready_data/files_used_30/sequence_index_40/arr_0.npy\n",
      "2023-05-31 13:49:24,944 Downloading model checkpoint\n",
      "2023-05-31 13:49:24,945 Creating directory: rl_ready_data_2/files_used_30\n",
      "2023-05-31 13:49:24,973 Downloading models from checkpoints dict_keys(['seq_1', 'seq_10', 'seq_20', 'seq_30', 'seq_30_heuristic', 'seq_40'])\n",
      "2023-05-31 13:49:24,973 Downloading model: seq_1\n",
      "2023-05-31 13:49:36,234 Downloading model: seq_10\n",
      "2023-05-31 13:49:36,654 Downloading model: seq_20\n",
      "2023-05-31 13:49:36,879 Downloading model: seq_30\n",
      "2023-05-31 13:49:37,114 Downloading model: seq_30_heuristic\n",
      "2023-05-31 13:49:37,299 Downloading model: seq_40\n",
      "Processing batch: 2349: 100%|██████████| 2350/2350 [02:04<00:00, 18.91it/s]\n",
      "2023-05-31 13:51:41,868 Predicted data shape: torch.Size([38500990, 40]): generating df\n",
      "2023-05-31 13:51:45,877 Decoding date time and sorting\n",
      "2023-05-31 13:51:45,878 Writing to parquet: rl_ready_data_2/files_used_30/predicted_data.parquet\n"
     ]
    }
   ],
   "source": [
    "generate_static_predictions(Arguments())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('rl_ready_data_2/files_used_30/predicted_data.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
