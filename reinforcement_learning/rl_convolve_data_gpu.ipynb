{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install --upgrade awscli python-dotenv --quiet\n",
    "%load_ext dotenv\n",
    "%dotenv env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync rl_ready_data_conv s3://dissertation-data-dmiller/rl_ready_data_conv --delete\n",
    "# !aws s3 sync rl_ready_data s3://dissertation-data-dmiller/rl_ready_data --delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm *.csv *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant\n",
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"cum_session_event_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time\",\n",
    "    \"date_time\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event\",\n",
    "    \"cum_session_time\",\n",
    "    \"cum_session_time\",\n",
    "   \n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"cum_projects\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "RESCALER_COLS = [\n",
    "    'session_30_count',\n",
    "    'session_5_count',\n",
    "    'cum_session_event',\n",
    "    'cum_session_time',\n",
    "    'cum_platform_time',\n",
    "    'cum_platform_event'\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    'seq_40',\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'sim_size',\n",
    "    'session_minutes',\n",
    "    'sim_minutes',\n",
    "    'reward',\n",
    "    'cum_session_time_raw'\n",
    "]\n",
    "\n",
    "TORCH_LOAD_COLS = [\n",
    "    'user_id',\n",
    "    'date_time'\n",
    "] + [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \"expanding_click_average\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "    \"delta_last_event\"\n",
    "] + [\n",
    "    'seq_40',\n",
    "    'session_30_raw',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import cudf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint, pformat\n",
    "import cudf as gpu_pd\n",
    "\n",
    "logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "COLS_FOR_INFLECTION = [\n",
    "    'user_id',\n",
    "    'date_time',\n",
    "] + OUT_FEATURE_COLUMNS + PREDICTION_COLS\n",
    "\n",
    "class SessionCalculate:\n",
    "    logger = logging.getLogger('rl_results_eval')\n",
    "    def __init__(self, df, use_gpu) -> None:\n",
    "        self.df = df\n",
    "        self.use_gpu = use_gpu\n",
    "            \n",
    "        \n",
    "    def calculate_inflections(self):\n",
    "      \n",
    "        self.logger.info('Calculating subsequent date time')\n",
    "        self.df['next_date_time'] = self.df.groupby('user_id')['date_time'].shift(-1)\n",
    "        self.df = self.df.drop_duplicates(subset=['user_id', 'date_time'], keep='last').reset_index()\n",
    "        if self.use_gpu:\n",
    "            self.logger.info('Bringing to CPU for second calculation')\n",
    "            self.df = self.df.to_pandas()\n",
    "           \n",
    "            \n",
    "        self.df['diff_seconds'] = (self.df['next_date_time'] - self.df['date_time']).apply(lambda x: x.total_seconds())\n",
    "        \n",
    "        self.logger.info('Diff seconds calculated')\n",
    "        if self.use_gpu:\n",
    "            self.logger.info('Bringing back to GPU for final calculations')\n",
    "            self.df = cudf.from_pandas(self.df)\n",
    "\n",
    "        self.df['diff_minutes'] = (self.df['diff_seconds'] / 60)\n",
    "        self.df['session_5'] = (self.df['diff_minutes'] < 5)\n",
    "        self.df['session_30'] = self.df['diff_minutes'] < 30\n",
    "        \n",
    "        self.df['session_30'] = self.df['session_30'].fillna(False)\n",
    "        self.df['session_5'] = self.df['session_5'].fillna(False)        \n",
    "        self.logger.info(f'Labels calculated: removing rows with diff seconds > 0')\n",
    "       \n",
    "        \n",
    "\n",
    "        self.logger.info(f'Number of rows following drop: {self.df.shape[0]}')\n",
    "        self.logger.info(f'Sorting rows by date time and applying row count')\n",
    "        self.df = self.df.sort_values(['date_time']).reset_index()\n",
    "        self.df['row_count'] = self.df.index.values\n",
    "        self.logger.info(f'Sorted rows and applied row count on updated index')  \n",
    "        self.logger.info('Calculating inflection points')\n",
    "        self.df['user_id'] = self.df['user_id'].astype('int32')\n",
    "        \n",
    "       \n",
    "        inflections_5_merge = self.df[self.df['session_5'] == False].sort_values(by=['date_time'])\n",
    "        inflections_30_merge = self.df[self.df['session_30'] == False].sort_values(by=['date_time']) \n",
    "     \n",
    "        self.logger.info('Calculating session 5 inflections') \n",
    "        inflections_5_merge['session_5'] = inflections_5_merge.groupby('user_id').cumcount() + 1\n",
    "        inflections_5_merge = inflections_5_merge.rename(columns={'session_5': 'session_5_count'})\n",
    "        \n",
    "        self.logger.info('Calculating session 30 inflections')\n",
    "        inflections_30_merge['session_30'] = inflections_30_merge.groupby('user_id').cumcount() + 1\n",
    "        inflections_30_merge = inflections_30_merge.rename(columns={'session_30': 'session_30_count'})\n",
    "        \n",
    "        inflections_5_merge = inflections_5_merge[['user_id', 'date_time', 'row_count', 'session_5_count']].sort_values(by=['row_count', 'user_id'])\n",
    "        inflections_30_merge = inflections_30_merge[['user_id', 'date_time', 'row_count', 'session_30_count']].sort_values(by=['row_count', 'user_id'])\n",
    "        inflections_5_merge = inflections_5_merge.drop(columns=['date_time'])\n",
    "        \n",
    "        inflections_30_merge = inflections_30_merge.rename(columns={'date_time': 'session_end_time'})\n",
    "\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            self.logger.info('Bringing back to GPU for labelling')\n",
    "            self.df, inflections_5_merge, inflections_30_merge = self.df.to_pandas(), inflections_5_merge.to_pandas(), inflections_30_merge.to_pandas()\n",
    "            self.df = self.df.sort_values(by=['row_count', 'user_id'])\n",
    "            self.df = pd.merge_asof(self.df, inflections_5_merge, on='row_count', by='user_id', direction='forward')\n",
    "            self.df = pd.merge_asof(self.df, inflections_30_merge, on='row_count', by='user_id', direction='forward')\n",
    "            self.df['session_terminates_30_minutes'] = (self.df['session_end_time'] - self.df['date_time']).apply(lambda x: x.total_seconds() / 60) < 30\n",
    "            self.df = cudf.from_pandas(self.df)\n",
    "        else:\n",
    "            self.logger.info('Labelling on CPU')\n",
    "            self.df = pd.merge_asof(self.df.sort_values(by=['row_count', 'user_id']), inflections_5_merge, on='row_count', by='user_id', direction='forward')\n",
    "\n",
    "            self.df = pd.merge_asof(self.df.sort_values(by=['row_count', 'user_id']), inflections_5_merge, on='row_count', by='user_id', direction='forward')\n",
    "            self.df = pd.merge_asof(self.df.sort_values(by=['row_count', 'user_id']), inflections_30_merge, on='row_count', by='user_id', direction='forward') \n",
    "            self.df['session_terminates_30_minutes'] = (self.df['session_end_time'] - self.df['date_time']).apply(lambda x: x.total_seconds() / 60) < 30\n",
    " \n",
    "        self.logger.info('Inflections calculated')\n",
    " \n",
    "        session_end_30_minutes = self.df[self.df['session_terminates_30_minutes'] == False].shape[0]\n",
    "        self.logger.info(f'Percent sessions end in 30 minutes: {session_end_30_minutes / self.df.shape[0]}')\n",
    "        self.logger.info(f'Columns for df') \n",
    "        self.logger.info(pformat(self.df.columns))\n",
    "        \n",
    "        return self.df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "global logger\n",
    "logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('rl_results_eval')\n",
    "from functools import reduce\n",
    "from pprint import pformat\n",
    "import cudf as gpu_pd\n",
    "import cupy as gpu_np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('mode.use_inf_as_na', True)\n",
    "from cuml.preprocessing import MinMaxScaler\n",
    "\n",
    "def convolve_delta_events(df, window, write_path):\n",
    "    \n",
    "    df = df.to_pandas()\n",
    "   \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    before_resample = df.shape\n",
    "    logger.info(f'Convolutional shape before resample: {before_resample}')\n",
    "    logger.info(f'Convolution over delta last event')\n",
    "    \n",
    "    df['delta_last_event'] = (\n",
    "        df.sort_values(by=['session_30_raw', 'cum_session_event_raw']) \\\n",
    "            .set_index('date_time') \\\n",
    "            .groupby(by=['user_id', 'session_30_raw'], group_keys=False) \\\n",
    "            .rolling(f'{window}T', min_periods=1)['delta_last_event'] \\\n",
    "            .mean()\n",
    "            .reset_index(name='convolved_event_delta')['convolved_event_delta'] \\\n",
    "    ) \n",
    "\n",
    "    df = df.drop(columns=['session_30_raw'])\n",
    "    df = df.loc[:,~df.columns.duplicated()].reset_index(drop=True)  \n",
    "    \n",
    "\n",
    "    \n",
    "    df = gpu_pd.from_pandas(df)\n",
    "  \n",
    "    # remove duplicate columns\n",
    "    df['year'] = df['date_time'].dt.year\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['minute'] = df['date_time'].dt.minute\n",
    "    df['second'] = df['date_time'].dt.second\n",
    "    \n",
    "    df['user_id'] = df['user_id'].astype('int32')\n",
    "    \n",
    "   \n",
    "    resampled_df = df.sort_values(by='date_time') \\\n",
    "        .drop_duplicates(subset=['user_id', 'year', 'month', 'day', 'hour', 'minute'], keep='last') \\\n",
    "        .sort_values(by=['date_time']) \\\n",
    "        .reset_index()\n",
    "    \n",
    "    logger.info(f'Convolution complete: {before_resample} -> {resampled_df.shape}')\n",
    "    logger.info(f'Writing intermediate results to {write_path}_convolve.parquet')\n",
    "    \n",
    "    resampled_df.to_parquet(write_path + '_convolve.parquet')\n",
    "    \n",
    "    logger.info(f'Recalculating inflections')\n",
    "    resample_events = SessionCalculate(resampled_df, use_gpu=True)\n",
    "    resampled_event_out = resample_events.calculate_inflections()\n",
    "   \n",
    "    logger.info(f'Events sessionized: writing to {write_path}_session.parquet')\n",
    "    resampled_event_out.to_parquet(write_path + '_session.parquet')\n",
    "    logger.info(f'Events resampled')\n",
    "    logger.info(sorted(pformat(resampled_event_out.columns)))\n",
    "    \n",
    "    return resampled_event_out\n",
    "     \n",
    "     \n",
    "\n",
    "def generate_metadata_session(dataset):\n",
    "    \n",
    "    logger.info(f'Calculating session size and minutes')\n",
    "    session_size = dataset.groupby(['user_id', 'session_30_count'])['cum_session_event'].max().reset_index(name='session_size')\n",
    "    session_minutes = dataset.groupby(['user_id', 'session_30_count'])['cum_session_time'].max().reset_index(name='session_minutes')\n",
    "    \n",
    "    \n",
    "    logger.info(f'Calculating sim size and minutes')\n",
    "    sim_minutes = dataset.groupby(['user_id', 'session_30_count'])['cum_session_time'].quantile(.7, interpolation='nearest').reset_index(name='sim_minutes')\n",
    "    sim_size = dataset.groupby(['user_id', 'session_30_count'])['cum_session_event'].quantile(.7, interpolation='nearest').reset_index(name='sim_size')\n",
    "    \n",
    "    \n",
    "    sessions = [session_size, session_minutes, sim_minutes, sim_size]\n",
    "    logger.info(f'Merging metadata')\n",
    "    sessions = reduce(lambda left, right: pd.merge(left, right, on=['user_id', 'session_30_count']), sessions)\n",
    "    \n",
    "    logger.info(f'Merging metadata complete')\n",
    "    dataset = pd.merge(dataset, sessions, on=['user_id', 'session_30_count'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def setup_data_at_window(df, window, write_path):\n",
    "    logger.info(f'Convolution over {window} minute window')\n",
    "    df = df.sort_values(by='date_time')\n",
    "    df['cum_session_event_raw'] = df.groupby(['user_id', 'session_30_raw'])['date_time'].cumcount() + 1\n",
    "    df = df.sort_values(by='date_time').reset_index(drop=True)\n",
    "    df  = convolve_delta_events(df, window, write_path)\n",
    "    logger.info(f'Convolving over {window} minute window complete: generating metadata')\n",
    "    logger.info(f'Generating metadata complete')\n",
    "    return df\n",
    "\n",
    "\n",
    "def reset_intra_session(subset_df):\n",
    "   \n",
    "    logger.info(f'Dropping sessions with less than one event') \n",
    "    \n",
    "    \n",
    "    \n",
    "    logger.info(f'Resetting cum_session_event_count')\n",
    "    subset_df['cum_session_event'] = subset_df.groupby(['user_id', 'session_30_count'])['date_time'].cumcount() + 1\n",
    "    logger.info(f'Resetting cum_session_time and setting reward')\n",
    "    subset_df = subset_df.to_pandas()\n",
    "    subset_df['reward'] = subset_df.groupby(['user_id', 'session_30_count'])['date_time'].diff().dt.total_seconds().fillna(0) / 60\n",
    "    subset_df['reward'] = subset_df[['reward', 'cum_session_event']].apply(lambda x: x['reward'] if x['cum_session_event'] > 1 else 0, axis=1)\n",
    "    subset_df['cum_session_time'] = subset_df.groupby(['user_id', 'session_30_count'])['reward'].cumsum()\n",
    "    \n",
    "    logger.info(f'Resetting cum_platform_time and cum_platform_events')\n",
    "    subset_df['cum_platform_time'] = subset_df.groupby(['user_id'])['reward'].cumsum()\n",
    "    subset_df['cum_platform_event'] = subset_df.groupby(['user_id'])['cum_session_event'].cumcount()\n",
    "    \n",
    "    subset_df = gpu_pd.from_pandas(subset_df)\n",
    "    \n",
    "    return subset_df\n",
    "\n",
    "def get_dataset(read_path, conv_path, n_files, window):\n",
    "    \n",
    "    conv_path, read_path = (\n",
    "        os.path.join(conv_path, f'files_used_{n_files}'),\n",
    "        os.path.join(read_path, f'files_used_{n_files}', 'predicted_data.parquet')\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(conv_path):\n",
    "        logger.info(f'Creating directory {conv_path}')\n",
    "        os.makedirs(conv_path)\n",
    "    \n",
    "    logger.info(f'Convolutional dataset not found at {conv_path}: creating')\n",
    "    logger.info(f'Getting dataset from {read_path}')\n",
    "    df = gpu_pd.read_parquet(read_path, columns=TORCH_LOAD_COLS)\n",
    "    \n",
    "    df['date_time'] = gpu_pd.to_datetime(df['date_time'])\n",
    "    logger.info(f'Non nan values: {df.count().min()}: 3ropping na')\n",
    "    logger.info(f'NA values dropped: {df.count().min()}')\n",
    "    \n",
    "    df = df.sort_values(by='date_time')\n",
    "    subsets = [\n",
    "        { \"name\": \"train\", \"start\": 0, \"end\": int(df.shape[0] * .7) },\n",
    "        { \"name\": \"eval\", \"start\": int(df.shape[0] * .7), \"end\": int(df.shape[0]) }\n",
    "    ]\n",
    "\n",
    "   \n",
    "    logger.info(f'Initial shape: {df.shape}: dropping na and inf')\n",
    "    df = df.dropna()\n",
    "    logger.info(f'Final shape: {df.shape}: dropping na and inf')\n",
    "    \n",
    "    for s in subsets:\n",
    "        \n",
    "        base_conv_path = os.path.join(conv_path, f'window_{window}_{s[\"name\"]}')\n",
    "        updated_conv_path = os.path.join(conv_path, f\"window_{window}_{s['name']}_final.parquet\")\n",
    "        logger.info(f'Running conv on {s[\"name\"]} subset')\n",
    "        subset_df = df[s['start']:s['end']].copy()\n",
    "        subset_df = subset_df.sort_values(by='date_time')\n",
    "        logger.info(f'Subset setup complete: {subset_df.shape}')\n",
    "        subset_df = setup_data_at_window(subset_df, window, base_conv_path)\n",
    "        logger.info(f'Subset convolution complete: {subset_df.shape}, resetting stats')\n",
    "        subset_df = subset_df.sort_values(by='date_time')\n",
    "        subset_df = reset_intra_session(subset_df)    \n",
    "        logger.info(f'Intra session stats calculated: {subset_df.shape}, saving intermediate')\n",
    "        subset_df.to_parquet(updated_conv_path + '_intra_session.parquet')\n",
    "        logger.info(f'Intra session reset complete: {subset_df.shape}')\n",
    "        logger.info(f'Stats reset complete, resetting metadata')\n",
    "        subset_df = subset_df.to_pandas()\n",
    "        subset_df = generate_metadata_session(subset_df)\n",
    "        logger.info(f'Metadata reset complete: {subset_df.shape}')\n",
    "        logger.info(f'Saving convolutional dataset to {updated_conv_path}')\n",
    "        \n",
    "        is_monotic_increasing_sess_time = subset_df.round(3).groupby(['user_id', 'session_30_count'])['cum_session_time'].is_monotonic_increasing.reset_index(name='is_monotic_increasing')\n",
    "        \n",
    "        if is_monotic_increasing_sess_time[is_monotic_increasing_sess_time['is_monotic_increasing'] == False].shape[0] > 0:\n",
    "            logger.info(f'Non monotonic increasing reward found: perc {is_monotic_increasing_sess_time[is_monotic_increasing_sess_time[\"is_monotic_increasing\"] == False].shape[0] / is_monotic_increasing_sess_time.shape[0]}')\n",
    "            logger.info(is_monotic_increasing_sess_time[is_monotic_increasing_sess_time[\"is_monotic_increasing\"] == False])\n",
    "        else:\n",
    "            logger.info(f'All rewards are monotonic increasing and no errors')\n",
    "            \n",
    "        \n",
    "        is_monotic_increasing_date_time = subset_df.round(3).groupby(['user_id'])['date_time'].is_monotonic_increasing.reset_index(name='is_monotic_increasing')\n",
    "        \n",
    "        if is_monotic_increasing_date_time[is_monotic_increasing_date_time['is_monotic_increasing'] == False].shape[0] > 0:\n",
    "            logger.info(f'Non monotonic increasing date time found: perc {is_monotic_increasing_date_time[is_monotic_increasing_date_time[\"is_monotic_increasing\"] == False].shape[0] / is_monotic_increasing_date_time.shape[0]}')\n",
    "            logger.info(is_monotic_increasing_date_time[is_monotic_increasing_date_time[\"is_monotic_increasing\"] == False])\n",
    "        else:\n",
    "            logger.info(f'All date times are monotonic increasing and no errors')\n",
    "       \n",
    "        logger.info(f'Rescaling feature cols: {RESCALER_COLS}')\n",
    "        \n",
    "        for col in RESCALER_COLS:\n",
    "            subset_df[f'{col}_raw'] = subset_df[col]\n",
    "        subset_df[RESCALER_COLS] = MinMaxScaler(feature_range=(-1, 1)).fit_transform(subset_df[RESCALER_COLS])\n",
    "       \n",
    "        logger.info(f'Writing to disk: {subset_df.shape}') \n",
    "        subset_df.to_parquet(updated_conv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    read_path = 'rl_ready_data'\n",
    "    conv_path = 'rl_ready_data_conv'\n",
    "    n_files = 30\n",
    "    window = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 10:39:55,145 - rl_results_eval - INFO - Convolutional dataset not found at rl_ready_data_conv/files_used_30: creating\n",
      "2023-06-03 10:39:55,146 - rl_results_eval - INFO - Getting dataset from rl_ready_data/files_used_30/predicted_data.parquet\n",
      "2023-06-03 10:39:58,210 - rl_results_eval - INFO - Non nan values: 36241442: 3ropping na\n",
      "2023-06-03 10:39:58,212 - rl_results_eval - INFO - NA values dropped: 36241442\n",
      "2023-06-03 10:39:58,335 - rl_results_eval - INFO - Initial shape: (36241442, 18): dropping na and inf\n",
      "2023-06-03 10:39:58,431 - rl_results_eval - INFO - Final shape: (36241442, 18): dropping na and inf\n",
      "2023-06-03 10:39:58,431 - rl_results_eval - INFO - Running conv on train subset\n",
      "2023-06-03 10:39:58,550 - rl_results_eval - INFO - Subset setup complete: (25369009, 18)\n",
      "2023-06-03 10:39:58,551 - rl_results_eval - INFO - Convolution over 1 minute window\n",
      "2023-06-03 10:40:05,397 - rl_results_eval - INFO - Convolutional shape before resample: (25369009, 19)\n",
      "2023-06-03 10:40:05,398 - rl_results_eval - INFO - Convolution over delta last event\n"
     ]
    }
   ],
   "source": [
    "get_dataset(Arguments.read_path, Arguments.conv_path, Arguments.n_files, Arguments.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r rl_ready_data/files_used_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync rl_ready_data s3://dissertation-data-dmiller/rl_ready_data --delete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
