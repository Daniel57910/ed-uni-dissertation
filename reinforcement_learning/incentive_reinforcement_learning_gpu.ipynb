{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch --quiet\n",
    "!python -m pip install gym stable-baselines3[extra] python-dotenv fsspec[\"s3\"] s3fs==2022.11.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = [\n",
    "    \"session_terminates_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \n",
    "    \"cum_session_event_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \n",
    "    \"cum_platform_event_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time_minutes\",\n",
    "]\n",
    "\n",
    "DATE_TIME = [\n",
    "    \"date_time\",\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\",\n",
    "    \"timestamp_raw\",\n",
    "    \"date_hour_sin\",\n",
    "    \n",
    "    \"date_hour_cos\",\n",
    "    \"session_5_count\",\n",
    "    \"session_30_count\",\n",
    "    \n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time_minutes\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time_minutes\",\n",
    "    \"cum_platform_events\",\n",
    "    \n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"rolling_session_time\",\n",
    "    \n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"session_event_count\",\n",
    "]\n",
    "\n",
    "METADATA_STAT_COLUMNS = [\n",
    "    'session_size',\n",
    "    'sim_size',\n",
    "    'session_minutes',\n",
    "    'ended',\n",
    "    'incentive_index',\n",
    "    'reward',\n",
    "    'n_episodes',\n",
    "    'time_in_session'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TORCH_LOAD_COLS = LABEL + METADATA + DATE_TIME + OUT_FEATURE_COLUMNS + ['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load callback\n",
    "from stable_baselines3.common.callbacks import  BaseCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DistributionCallback(BaseCallback):\n",
    "    \n",
    "    metadata_stat = METADATA_STAT_COLUMNS + ['time_in_session']\n",
    "    @classmethod\n",
    "    def tensorboard_setup(cls, log_dir, log_freq):\n",
    "        cls._log_dir = log_dir\n",
    "        cls._log_freq = log_freq\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        output_formats = self.logger.output_formats\n",
    "        self.tb_formatter = next(f for f in output_formats if isinstance(f, TensorBoardOutputFormat))\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._log_freq == 0:\n",
    "            dist_list = self.training_env.env_method('dists')\n",
    "            values_to_log = np.concatenate([d for d in dist_list if d.shape[0] > 0])\n",
    "\n",
    "            values_df = pd.DataFrame(\n",
    "                values_to_log, \n",
    "                columns=METADATA_STAT_COLUMNS\n",
    "            )\n",
    "            \n",
    "            dist_session_time = (values_df['session_minutes'] - values_df['time_in_session']).mean()\n",
    "            dist_session_end = (values_df['session_size'] - values_df['ended']).mean()\n",
    "            dist_inc_session = (values_df['session_size'] - values_df['incentive_index']).mean()\n",
    "            dist_session_end = (values_df['ended'] - values_df['incentive_index']).mean()\n",
    "            dist_inc_sim_size = (values_df['ended'] - values_df['sim_size']).mean()\n",
    "            dist_inc_sim_index = (values_df['incentive_index'] - values_df['sim_size']).mean()\n",
    "\n",
    "            n_call = self.n_calls // self._log_freq\n",
    "            \n",
    "            self.tb_formatter.writer.add_scalar('event/sess_time_sub_sime_time::decrease', dist_session_time, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('event/sess_index_sub_sim_index::decrease', dist_session_end, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('event/sim_incentive_index_sub_index_no_reward::increase', dist_inc_sim_size, n_call)\n",
    "            \n",
    "            self.tb_formatter.writer.add_scalar('event/sess_index_sub_incentive_index', dist_inc_session, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('event/sim_index_sub_incentive_index', dist_inc_sim_index, n_call)\n",
    "            self.tb_formatter.writer.flush()\n",
    "            \n",
    "            values_df.to_parquet(f'{self._log_dir}/dist_{n_call}.parquet')\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment\n",
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "\n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, dataset, unique_episodes, unique_sessions, n_sequences):\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.unique_episodes = unique_episodes\n",
    "        self.n_episodes = 0\n",
    "        self.n_sequences = n_sequences\n",
    "        self.unique_sessions = unique_sessions\n",
    "        self.current_session = None\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        self.metadata_container = []\n",
    "        self.n_sequences = n_sequences\n",
    "        self.out_features = OUT_FEATURE_COLUMNS + ['prediction']\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(n_sequences + 1, len(self.out_features)), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_episodes += 1\n",
    "        session_to_run = self.unique_sessions.sample()['session_30_raw'].values[0]\n",
    "        user_to_run = self.unique_episodes[self.unique_episodes['session_30_raw'] == session_to_run].sample()['user_id'].values[0]\n",
    "        self.current_session = self._get_events(user_to_run, session_to_run)\n",
    "        self.metadata = self._metadata()\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        return self._state()\n",
    "\n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        next_state, done, meta = self._calculate_next_state()\n",
    "        if done:\n",
    "            self.metadata['ended'] = self.current_session_index + 1\n",
    "            self.metadata['reward'] = self.reward\n",
    "            time_minute_index = min(self.current_session_index, self.current_session.shape[0] - 1)\n",
    "            self.metadata['time_in_session'] = self.current_session.iloc[time_minute_index]['cum_session_time_minutes']\n",
    "            self.metadata_container.append(self.metadata[METADATA_STAT_COLUMNS].values)\n",
    "            return next_state, self.reward, done, meta\n",
    "        else:\n",
    "            self.reward += (self.current_session.iloc[self.current_session_index]['reward'] / 60)\n",
    "            self.current_session_index += 1        \n",
    "        return next_state, self.reward, done, meta\n",
    "    \n",
    "    def _metadata(self):\n",
    "        session_metadata = self.current_session.iloc[0][['user_id', 'session_30_raw', 'session_size', 'sim_size', 'session_minutes']]\n",
    "        session_metadata['ended'] = 0\n",
    "        session_metadata['incentive_index'] = 0\n",
    "        session_metadata['reward'] = 0\n",
    "        session_metadata['n_episodes'] = self.n_episodes\n",
    "        return session_metadata\n",
    "    \n",
    "    \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        if (self.current_session_index == self.current_session.shape[0]):\n",
    "            return None, True, {}\n",
    "\n",
    "        if self._continuing_in_session():\n",
    "            return self._state(), False, {}\n",
    "    \n",
    "        return None, True, {}\n",
    "        \n",
    "      \n",
    "  \n",
    "    def _continuing_in_session(self):\n",
    "        sim_counts = self.metadata['sim_size']\n",
    "        if self.current_session_index < sim_counts:\n",
    "            return True\n",
    "        \n",
    "        extending_session = self._probability_extending_session()\n",
    "        \n",
    "        return all([extending_session >= .3, extending_session <= .7])\n",
    "        \n",
    "    \n",
    "    def _probability_extending_session(self):\n",
    "        if self.metadata['incentive_index'] == 0:\n",
    "            return 0\n",
    "        \n",
    "        scale = max(5, int(self.metadata['session_size'] / 4))\n",
    "        continue_session = norm(\n",
    "            loc=self.metadata['incentive_index'],\n",
    "            scale=scale\n",
    "        ).cdf(self.current_session_index)\n",
    "        \n",
    "        return continue_session\n",
    "        \n",
    "\n",
    "    def _get_events(self, user_id, session):\n",
    "        subset = self.dataset[\n",
    "            (self.dataset['user_id'] == user_id) &\n",
    "            (self.dataset['session_30_raw'] == session)\n",
    "        ]\n",
    "   \n",
    "        return subset.sort_values('cum_session_event_raw').reset_index(drop=True)\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        if action == 0 or self.metadata['incentive_index'] > 0:\n",
    "            return\n",
    "        \n",
    "        self.metadata['incentive_index'] = self.current_session_index\n",
    "        \n",
    "    def _state(self):\n",
    "\n",
    "        if self.current_session_index > self.n_sequences:\n",
    "            events = self.current_session.iloc[self.current_session_index - (self.n_sequences + 1):self.current_session_index][self.out_features].values\n",
    "            \n",
    "        else:\n",
    "            delta = min((self.n_sequences + 1)- self.current_session_index, 10)\n",
    "            zero_cat = np.zeros((delta, len(self.out_features)))\n",
    "            events = self.current_session.iloc[:max(self.current_session_index, 1)][self.out_features].values\n",
    "            events = np.concatenate((zero_cat, events), axis=0)\n",
    "            \n",
    "\n",
    "        return events.astype(np.float32)\n",
    "  \n",
    "    \n",
    "    def dists(self):\n",
    "        metadata_container = self.metadata_container.copy()\n",
    "        self.metadata_container = []\n",
    "        return np.array(metadata_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load incentive_reinforcement_learning_cpu.py\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "from stable_baselines3.common.callbacks import CallbackList, StopTrainingOnMaxEpisodes, CheckpointCallback\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import logging\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "CUM_SESSION_EVENT_RAW = 3\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "from pprint import pformat\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    import cudf as gpu_pd\n",
    "    from cuml.preprocessing import MinMaxScaler\n",
    "else:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "torch.set_printoptions(precision=2, linewidth=200, sci_mode=False)\n",
    "\n",
    "\n",
    "S3_BASELINE_PATH = 's3://dissertation-data-dmiller'\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--read_path', type=str, default='datasets/rl_ready_data')\n",
    "    parser.add_argument('--n_files', type=str, default=2)\n",
    "    parser.add_argument('--n_sequences', type=int, default=10)\n",
    "    parser.add_argument('--n_episodes', type=int, default=20)\n",
    "    parser.add_argument('--n_envs', type=int, default=100)\n",
    "    parser.add_argument('--lstm', type=str, default='ordinal_10')\n",
    "    parser.add_argument('--device', type=str, default='cpu')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "from stable_baselines3.common.callbacks import CallbackList, StopTrainingOnMaxEpisodes\n",
    "from stable_baselines3 import A2C\n",
    "import logging\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "CUM_SESSION_EVENT_RAW = 3\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "import pandas as pd\n",
    "# import cudf as gpu_pd\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "from pprint import pformat\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "torch.set_printoptions(precision=2, linewidth=200, sci_mode=False)\n",
    "\n",
    "\n",
    "S3_BASELINE_PATH = 's3://dissertation-data-dmiller'\n",
    "\n",
    "\n",
    "def train_eval_split(dataset, logger):\n",
    "    train_split = int(dataset.shape[0] * TRAIN_SPLIT)\n",
    "    eval_split = int(dataset.shape[0] * EVAL_SPLIT)\n",
    "    test_split = dataset.shape[0] - train_split - eval_split\n",
    "    logger.info(f'Train size: 0:{train_split}, eval size: {train_split}:{train_split+eval_split}: test size: {train_split + eval_split}:{dataset.shape[0]}')\n",
    "    train_dataset, eval_dataset, test_split = dataset[:train_split], dataset[train_split:train_split+eval_split], dataset[train_split+eval_split:]\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'eval': eval_dataset,\n",
    "        'test': test_split\n",
    "    }\n",
    "\n",
    "def generate_metadata(dataset):\n",
    "    \n",
    "    session_size = dataset.groupby(['user_id', 'session_30_raw']).size().reset_index(name='session_size')\n",
    "    session_minutes = dataset.groupby(['user_id', 'session_30_raw'])['cum_session_time_raw'].max().reset_index(name='session_minutes')\n",
    "    session_size['sim_size'] = (session_size['session_size'] * .7).astype(int).apply(lambda x: x if x > 1 else 1)\n",
    "    dataset = dataset.merge(session_size, on=['user_id', 'session_30_raw'])\n",
    "    dataset = dataset.merge(session_minutes, on=['user_id', 'session_30_raw'])\n",
    "    return dataset\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def run_reinforcement_learning_incentives(environment, logger, n_episodes=1):\n",
    "    for epoch in range(n_episodes):\n",
    "        environment_comp = False\n",
    "        state = environment.reset()\n",
    "        i = 0\n",
    "        while not environment_comp:\n",
    "            next_action = (\n",
    "                1 if np.random.uniform(low=0, high=1) > 0.8 else 0\n",
    "            )\n",
    "            state, rewards, environment_comp, meta = environment.step(next_action)\n",
    "            i +=1\n",
    "            if i % 100 == 0:\n",
    "                logger.info(f'Step: {i} - Reward: {rewards}')\n",
    "                \n",
    "        logger.info(f'Epoch: {epoch} - Reward: {rewards}')\n",
    "        print(environment.user_sessions.head(10))\n",
    "\n",
    "    \n",
    "def main(args):\n",
    "    \n",
    "    exec_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    logger.info('Starting Incentive Reinforcement Learning')\n",
    "    \n",
    "    read_path, n_files, n_sequences, n_episodes, device, n_envs, lstm = (\n",
    "        args.read_path, \n",
    "        args.n_files, \n",
    "        args.n_sequences, \n",
    "        args.n_episodes, \n",
    "        args.device,\n",
    "        args.n_envs,\n",
    "        args.lstm\n",
    "    )\n",
    "    \n",
    "    file_ext = '.gzip' if not torch.cuda.is_available() else \"\"\n",
    "    \n",
    "    read_path = os.path.join(\n",
    "        read_path,\n",
    "        f'files_used_{n_files}',\n",
    "        f'rl_ready_data.parquet{file_ext}'\n",
    "    )\n",
    "    \n",
    "    logger.info(f'Reading data from {read_path}_{n_files}.parquet')\n",
    "    if torch.cuda.is_available():\n",
    "        df = gpu_pd.read_parquet(read_path, columns=TORCH_LOAD_COLS)\n",
    "        df['date_time'] = gpu_pd.to_datetime(df['date_time'])\n",
    "    else:\n",
    "        df = pd.read_parquet(read_path, columns=TORCH_LOAD_COLS)\n",
    "        df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "        \n",
    "        \n",
    "    df = df.sort_values(by=['date_time'])\n",
    "    df = df.head(int(df.shape[0] * 0.7))\n",
    "    logger.info('Data read: generating metadata')\n",
    "    df['reward'] = df['delta_last_event']\n",
    "    df = generate_metadata(df)\n",
    "    \n",
    "    logger.info(f'Metadata generated: scaling features')\n",
    "    df[OUT_FEATURE_COLUMNS] = MinMaxScaler().fit_transform(df[OUT_FEATURE_COLUMNS])\n",
    "    logger.info(f'Features Scaled')\n",
    "\n",
    "    unique_episodes = df[['user_id', 'session_30_raw']].drop_duplicates()\n",
    "    unique_sessions = df[['session_30_raw']].drop_duplicates()\n",
    "    logger.info(f'Parralelizing environment with {n_envs} environments')\n",
    "    if torch.cuda.is_available():\n",
    "        df, unique_episodes, unique_sessions = df.to_pandas(), unique_episodes.to_pandas(), unique_sessions.to_pandas()\n",
    "        \n",
    "\n",
    "    citizen_science_vec = DummyVecEnv([lambda: CitizenScienceEnv(df, unique_episodes, unique_sessions, n_sequences) for _ in range(n_envs)])\n",
    "\n",
    "    logger.info(f'Vectorized environments created, wrapping with monitor')\n",
    "\n",
    "    base_path = os.path.join(\n",
    "        S3_BASELINE_PATH,\n",
    "        'reinforcement_learning_incentives',\n",
    "        f'n_files_{n_files}',\n",
    "        'results',\n",
    "        f'lstm_{lstm}',\n",
    "        exec_time,\n",
    "    ) \n",
    "    \n",
    "    tensorboard_dir, checkpoint_dir = (\n",
    "        os.path.join(base_path, 'training_metrics'),\n",
    "        os.path.join(base_path, 'checkpoints')\n",
    "    )\n",
    "\n",
    "    callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=n_episodes, verbose=1)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=100_000 // n_envs, save_path=checkpoint_dir, name_prefix='rl_model')\n",
    "    dist_callback = DistributionCallback()\n",
    "    DistributionCallback.tensorboard_setup(tensorboard_dir, 500)\n",
    "    callback_list = CallbackList([callback_max_episodes, dist_callback, checkpoint_callback])\n",
    "    monitor_train = VecMonitor(citizen_science_vec)\n",
    "    \n",
    "    model = A2C(\"MlpPolicy\", monitor_train, verbose=2, tensorboard_log=tensorboard_dir)\n",
    "            \n",
    "    logger.info(pformat([\n",
    "        'n_episodes: {}'.format(n_episodes),\n",
    "        'read_path: {}'.format(read_path),\n",
    "        'n_files: {}'.format(n_files),\n",
    "        'n_sequences: {}'.format(n_sequences),\n",
    "        'n_envs: {}'.format(n_envs),\n",
    "        'total_timesteps: {}'.format(df.shape),\n",
    "        f'unique_episodes: {unique_episodes.shape[0]}',\n",
    "        'device: {}'.format(device),\n",
    "        'tensorboard_dir: {}'.format(tensorboard_dir),\n",
    "        'checkpoint_dir: {}'.format(checkpoint_dir),\n",
    "        'lstm_option: {}'.format(lstm)\n",
    "    ]))\n",
    "\n",
    "\n",
    "    model.learn(total_timesteps=100_000_000, progress_bar=True, log_interval=100, callback=callback_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument:\n",
    "    read_path = 'rl_ready_data'\n",
    "    n_files = 30\n",
    "    n_sequences = 10\n",
    "    n_episodes = 500_000\n",
    "    n_envs = 250\n",
    "    lstm = 'ordinal_10'\n",
    "    device = 'cuda'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 04:29:25 PM Starting Incentive Reinforcement Learning\n",
      "04/25/2023 04:29:25 PM Reading data from rl_ready_data/files_used_30/rl_ready_data.parquet_30.parquet\n",
      "04/25/2023 04:29:30 PM Data read: generating metadata\n",
      "04/25/2023 04:29:31 PM Metadata generated: scaling features\n",
      "04/25/2023 04:29:35 PM Features Scaled\n",
      "04/25/2023 04:29:35 PM Parralelizing environment with 250 environments\n",
      "04/25/2023 04:29:44 PM Vectorized environments created, wrapping with monitor\n",
      "04/25/2023 04:29:44 PM ['n_episodes: 500000',\n",
      " 'read_path: rl_ready_data/files_used_30/rl_ready_data.parquet',\n",
      " 'n_files: 30',\n",
      " 'n_sequences: 10',\n",
      " 'n_envs: 250',\n",
      " 'total_timesteps: (26950693, 32)',\n",
      " 'unique_episodes: 457187',\n",
      " 'device: cuda',\n",
      " 'tensorboard_dir: '\n",
      " 's3://dissertation-data-dmiller/reinforcement_learning_incentives/n_files_30/results/lstm_ordinal_10/2023-04-25-16-29/training_metrics',\n",
      " 'checkpoint_dir: '\n",
      " 's3://dissertation-data-dmiller/reinforcement_learning_incentives/n_files_30/results/lstm_ordinal_10/2023-04-25-16-29/checkpoints',\n",
      " 'lstm_option: ordinal_10']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 04:30:07 PM Found credentials in environment variables.\n",
      "04/25/2023 04:30:08 PM Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to s3://dissertation-data-dmiller/reinforcement_learning_incentives/n_files_30/results/lstm_ordinal_10/2023-04-25-16-29/training_metrics/A2C_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d8723140914d1dbc38ffb8e6757fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 57.7      |\n",
      "|    ep_rew_mean        | 1325.3723 |\n",
      "| time/                 |           |\n",
      "|    fps                | 365       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 341       |\n",
      "|    total_timesteps    | 125000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.293    |\n",
      "|    explained_variance | 0.0026    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 17.6      |\n",
      "|    value_loss         | 2.54e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 52.2      |\n",
      "|    ep_rew_mean        | 1426.2798 |\n",
      "| time/                 |           |\n",
      "|    fps                | 396       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 629       |\n",
      "|    total_timesteps    | 250000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.237    |\n",
      "|    explained_variance | 0.00171   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 12.6      |\n",
      "|    value_loss         | 3.52e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 41.9     |\n",
      "|    ep_rew_mean        | 891.6065 |\n",
      "| time/                 |          |\n",
      "|    fps                | 407      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 920      |\n",
      "|    total_timesteps    | 375000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.347   |\n",
      "|    explained_variance | 0.000954 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 33.6     |\n",
      "|    value_loss         | 5.35e+04 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 45.3      |\n",
      "|    ep_rew_mean        | 1591.6954 |\n",
      "| time/                 |           |\n",
      "|    fps                | 417       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1196      |\n",
      "|    total_timesteps    | 500000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.537    |\n",
      "|    explained_variance | 0.000611  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 55.1      |\n",
      "|    value_loss         | 5.76e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 72.8     |\n",
      "|    ep_rew_mean        | 5386.509 |\n",
      "| time/                 |          |\n",
      "|    fps                | 420      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1487     |\n",
      "|    total_timesteps    | 625000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.403   |\n",
      "|    explained_variance | 0.000272 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 39.5     |\n",
      "|    value_loss         | 4.49e+04 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 52        |\n",
      "|    ep_rew_mean        | 1238.6245 |\n",
      "| time/                 |           |\n",
      "|    fps                | 425       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 1761      |\n",
      "|    total_timesteps    | 750000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.347    |\n",
      "|    explained_variance | 0.000221  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 35.6      |\n",
      "|    value_loss         | 5.1e+04   |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main(Argument)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
