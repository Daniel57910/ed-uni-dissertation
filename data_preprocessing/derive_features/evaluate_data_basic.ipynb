{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "- Number of events\n",
    "- Min date, max date\n",
    "- Ratio true false\n",
    "- Number of unique users\n",
    "- Number of unique sessions\n",
    "- Matrix of eval users and sessions in train\n",
    "- Matrix of test users and sessions in eval\n",
    "- Matrix of test users and sessions in train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_PATH = 'calculated_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289ed5d9-0a4e-4f14-8000-5f20eabc10d8.ics\n",
      "PGPMessage.pdf\n",
      "basic_descriptive_30_files.json\n",
      "core_dict.json\n",
      "files_used_2.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../datasets/calculated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(CORE_PATH, 'files_used_10')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = df.describe().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'project_id', 'session_5_count', 'session_30_count', 'user_id',\n",
       "       'country_count', 'date_hour', 'date_minute', 'date_hour_sin',\n",
       "       'date_hour_cos', 'date_minute_sin', 'date_minute_cos',\n",
       "       'cum_session_event_count', 'delta_last_event', 'cum_session_time',\n",
       "       'cum_platform_time', 'cum_platform_events', 'cum_projects',\n",
       "       'average_event_time', 'expanding_click_average', 'rolling_session_time',\n",
       "       'rolling_session_events', 'rolling_session_gap', 'session_event_count',\n",
       "       'session_time_minutes', 'previous_session_time',\n",
       "       'previous_session_events', 'session_30_raw', 'global_session_time',\n",
       "       'global_events_user', 'year', 'month', 'day', 'hour', 'minute',\n",
       "       'second', 'cum_session_time_raw', 'cum_platform_time_raw',\n",
       "       'cum_session_event_raw', 'cum_platform_event_raw',\n",
       "       'session_30_count_raw'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delta_last_event</th>\n",
       "      <th>expanding_click_average</th>\n",
       "      <th>session_30_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12833662.000</td>\n",
       "      <td>12833662.000</td>\n",
       "      <td>12833662.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.066</td>\n",
       "      <td>26.518</td>\n",
       "      <td>27.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>100.069</td>\n",
       "      <td>54.729</td>\n",
       "      <td>49.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000</td>\n",
       "      <td>3.900</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000</td>\n",
       "      <td>8.800</td>\n",
       "      <td>8.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.000</td>\n",
       "      <td>23.900</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1799.000</td>\n",
       "      <td>1316.200</td>\n",
       "      <td>458.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       delta_last_event  expanding_click_average  session_30_count\n",
       "count      12833662.000             12833662.000      12833662.000\n",
       "mean             29.066                   26.518            27.699\n",
       "std             100.069                   54.729            49.603\n",
       "min               0.000                    0.000             1.000\n",
       "25%               3.000                    3.900             2.000\n",
       "50%               6.000                    8.800             8.000\n",
       "75%              17.000                   23.900            30.000\n",
       "max            1799.000                 1316.200           458.000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description[['delta_last_event', 'expanding_click_average', 'session_30_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "description.to_csv('calculated_features/files_used_10_description.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_DICT = {\n",
    "    'n_events': {\n",
    "        'train': None,\n",
    "        'eval': None,\n",
    "        'test': None\n",
    "    },\n",
    "    'dates': {\n",
    "        'train': None,\n",
    "        'eval': None,\n",
    "        'test': None,\n",
    "    },\n",
    "    'users': {\n",
    "        'train': None,\n",
    "        'eval': None,\n",
    "        'test': None\n",
    "    },\n",
    "    'sessions': {\n",
    "        'train': None,\n",
    "        'eval': None,\n",
    "        'test': None\n",
    "    },\n",
    "    'label_count': {\n",
    "        'train': None,\n",
    "        'eval': None,\n",
    "        'test': None  \n",
    "    },\n",
    "    'user_crossover': {\n",
    "        'train_eval': None,\n",
    "        'train_test': None,\n",
    "        'eval_test': None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Error creating dataset. Could not read schema from '../../../datasets/calculated_features/289ed5d9-0a4e-4f14-8000-5f20eabc10d8.ics': Could not open Parquet input source '../../../datasets/calculated_features/289ed5d9-0a4e-4f14-8000-5f20eabc10d8.ics': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.. Is this a 'parquet' file?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(CORE_PATH, columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msession_30_raw\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdate_time\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msession_terminates_30_minutes\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39msession_terminates_30_minutes\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mnot\u001b[39;00m x)\n\u001b[1;32m      3\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39msession_terminates_30_minutes\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/torch_rl/lib/python3.9/site-packages/pandas/io/parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m     use_nullable_dtypes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 509\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    510\u001b[0m     path,\n\u001b[1;32m    511\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m    512\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    513\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    514\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    515\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    516\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/torch_rl/lib/python3.9/site-packages/pandas/io/parquet.py:227\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    221\u001b[0m     path,\n\u001b[1;32m    222\u001b[0m     kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    223\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[1;32m    224\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     pa_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mread_table(\n\u001b[1;32m    228\u001b[0m         path_or_handle, columns\u001b[39m=\u001b[39;49mcolumns, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    230\u001b[0m     result \u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mto_pandas(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mto_pandas_kwargs)\n\u001b[1;32m    232\u001b[0m     \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/torch_rl/lib/python3.9/site-packages/pyarrow/parquet/core.py:2926\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   2919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keyword is no longer supported with the new \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2921\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdatasets-based implementation. Specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2922\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_legacy_dataset=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to temporarily recover the old \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2923\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbehaviour.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2924\u001b[0m     )\n\u001b[1;32m   2925\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2926\u001b[0m     dataset \u001b[39m=\u001b[39m _ParquetDatasetV2(\n\u001b[1;32m   2927\u001b[0m         source,\n\u001b[1;32m   2928\u001b[0m         schema\u001b[39m=\u001b[39;49mschema,\n\u001b[1;32m   2929\u001b[0m         filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   2930\u001b[0m         partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[1;32m   2931\u001b[0m         memory_map\u001b[39m=\u001b[39;49mmemory_map,\n\u001b[1;32m   2932\u001b[0m         read_dictionary\u001b[39m=\u001b[39;49mread_dictionary,\n\u001b[1;32m   2933\u001b[0m         buffer_size\u001b[39m=\u001b[39;49mbuffer_size,\n\u001b[1;32m   2934\u001b[0m         filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[1;32m   2935\u001b[0m         ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes,\n\u001b[1;32m   2936\u001b[0m         pre_buffer\u001b[39m=\u001b[39;49mpre_buffer,\n\u001b[1;32m   2937\u001b[0m         coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit,\n\u001b[1;32m   2938\u001b[0m         thrift_string_size_limit\u001b[39m=\u001b[39;49mthrift_string_size_limit,\n\u001b[1;32m   2939\u001b[0m         thrift_container_size_limit\u001b[39m=\u001b[39;49mthrift_container_size_limit,\n\u001b[1;32m   2940\u001b[0m     )\n\u001b[1;32m   2941\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m   2942\u001b[0m     \u001b[39m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   2943\u001b[0m     \u001b[39m# module is not available\u001b[39;00m\n\u001b[1;32m   2944\u001b[0m     \u001b[39mif\u001b[39;00m filters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/torch_rl/lib/python3.9/site-packages/pyarrow/parquet/core.py:2477\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[39mif\u001b[39;00m partitioning \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhive\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2474\u001b[0m     partitioning \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mHivePartitioning\u001b[39m.\u001b[39mdiscover(\n\u001b[1;32m   2475\u001b[0m         infer_dictionary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 2477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mdataset(path_or_paths, filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   2478\u001b[0m                            schema\u001b[39m=\u001b[39;49mschema, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mparquet_format,\n\u001b[1;32m   2479\u001b[0m                            partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[1;32m   2480\u001b[0m                            ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/torch_rl/lib/python3.9/site-packages/pyarrow/dataset.py:762\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    751\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    752\u001b[0m     schema\u001b[39m=\u001b[39mschema,\n\u001b[1;32m    753\u001b[0m     filesystem\u001b[39m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m     selector_ignore_prefixes\u001b[39m=\u001b[39mignore_prefixes\n\u001b[1;32m    759\u001b[0m )\n\u001b[1;32m    761\u001b[0m \u001b[39mif\u001b[39;00m _is_path_like(source):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    763\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(source, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[1;32m    764\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(_is_path_like(elem) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/torch_rl/lib/python3.9/site-packages/pyarrow/dataset.py:455\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    447\u001b[0m options \u001b[39m=\u001b[39m FileSystemFactoryOptions(\n\u001b[1;32m    448\u001b[0m     partitioning\u001b[39m=\u001b[39mpartitioning,\n\u001b[1;32m    449\u001b[0m     partition_base_dir\u001b[39m=\u001b[39mpartition_base_dir,\n\u001b[1;32m    450\u001b[0m     exclude_invalid_files\u001b[39m=\u001b[39mexclude_invalid_files,\n\u001b[1;32m    451\u001b[0m     selector_ignore_prefixes\u001b[39m=\u001b[39mselector_ignore_prefixes\n\u001b[1;32m    452\u001b[0m )\n\u001b[1;32m    453\u001b[0m factory \u001b[39m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[39mformat\u001b[39m, options)\n\u001b[0;32m--> 455\u001b[0m \u001b[39mreturn\u001b[39;00m factory\u001b[39m.\u001b[39;49mfinish(schema)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/torch_rl/lib/python3.9/site-packages/pyarrow/_dataset.pyx:2062\u001b[0m, in \u001b[0;36mpyarrow._dataset.DatasetFactory.finish\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/torch_rl/lib/python3.9/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/torch_rl/lib/python3.9/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Error creating dataset. Could not read schema from '../../../datasets/calculated_features/289ed5d9-0a4e-4f14-8000-5f20eabc10d8.ics': Could not open Parquet input source '../../../datasets/calculated_features/289ed5d9-0a4e-4f14-8000-5f20eabc10d8.ics': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.. Is this a 'parquet' file?"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(CORE_PATH, columns=['user_id', 'session_30_raw', 'date_time', 'session_terminates_30_minutes'])\n",
    "df['label'] = df['session_terminates_30_minutes'].apply(lambda x: not x)\n",
    "df = df.drop(columns=['session_terminates_30_minutes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find average click time per user\n",
    "# find variance of click time per user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_30_raw</th>\n",
       "      <th>date_time</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-10-19 08:40:37</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-10-19 08:40:38</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-10-19 08:40:39</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-10-19 08:40:39</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-10-19 08:40:41</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  session_30_raw           date_time  label\n",
       "0        0               1 2021-10-19 08:40:37  False\n",
       "1        1               1 2021-10-19 08:40:38  False\n",
       "2        0               1 2021-10-19 08:40:39  False\n",
       "3        2               1 2021-10-19 08:40:39   True\n",
       "4        0               1 2021-10-19 08:40:41  False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = df.shape[0]\n",
    "\n",
    "train_part = (0, int(n_rows * 0.7))\n",
    "eval_part = (int(n_rows * 0.7), int(n_rows * 0.85))\n",
    "test_part = (int(n_rows * 0.85), n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, eval = df[train_part[0]:train_part[1]], df[test_part[0]:test_part[1]], df[eval_part[0]:eval_part[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label, eval_label, test_label = (\n",
    "    train['label'].sum() / train.shape[0],\n",
    "    eval['label'].sum() / eval.shape[0],\n",
    "    test['label'].sum() / test.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_sess, eval_user_sess, test_user_sess = (\n",
    "    train[['user_id', 'session_30_raw']].drop_duplicates(),\n",
    "    eval[['user_id', 'session_30_raw']].drop_duplicates(),\n",
    "    test[['user_id', 'session_30_raw']].drop_duplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_dict = CORE_DICT.copy()\n",
    "core_dict['n_events']['train'] = train.shape[0]\n",
    "core_dict['n_events']['eval'] = eval.shape[0]\n",
    "core_dict['n_events']['test'] = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_dict['label_count']['train'] = train_label\n",
    "core_dict['label_count']['eval'] = eval_label\n",
    "core_dict['label_count']['test'] = test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_min_max = {'min': str(train['date_time'].min()), 'max': str(train['date_time'].max()) }\n",
    "\n",
    "eval_min_max = {'min': str(eval['date_time'].min()), 'max': str(eval['date_time'].max()) }\n",
    "test_min_max = {'min': str(test['date_time'].min()), 'max': str(test['date_time'].max()) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_dict['dates']['train'] = train_min_max\n",
    "core_dict['dates']['eval'] = eval_min_max\n",
    "core_dict['dates']['test'] = test_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_dict['users']['train'] = train_user_sess['user_id'].unique().shape[0]\n",
    "core_dict['users']['eval'] = eval_user_sess['user_id'].unique().shape[0]\n",
    "core_dict['users']['test'] = test_user_sess['user_id'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_dict['sessions']['train'] = train_user_sess.shape[0]\n",
    "core_dict['sessions']['eval'] = eval_user_sess.shape[0]\n",
    "core_dict['sessions']['test'] = test_user_sess.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in_eval = train_user_sess[['user_id']].drop_duplicates().merge(eval_user_sess[['user_id']].drop_duplicates(), how='inner', on='user_id')\n",
    "train_in_test = train_user_sess[['user_id']].drop_duplicates().merge(test_user_sess[['user_id']].drop_duplicates(), how='inner', on='user_id')\n",
    "eval_in_test = eval_user_sess[['user_id']].drop_duplicates().merge(test_user_sess[['user_id']].drop_duplicates(), how='inner', on='user_id')\n",
    "core_dict['user_crossover']['train_eval'] = train_in_eval.shape[0]\n",
    "core_dict['user_crossover']['train_test'] = train_in_test.shape[0]\n",
    "core_dict['user_crossover']['eval_test'] = eval_in_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_session_eval = train_user_sess[['user_id', 'session_30_raw']].merge(eval_user_sess[['user_id', 'session_30_raw']], how='inner', on=['user_id', 'session_30_raw'])\n",
    "eval_session_test = eval_user_sess[['user_id', 'session_30_raw']].merge(test_user_sess[['user_id', 'session_30_raw']], how='inner', on=['user_id', 'session_30_raw'])\n",
    "core_dict['session_crossover'] = {\n",
    "    'train_eval': train_session_eval.shape[0],\n",
    "    'eval_test': eval_session_test.shape[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_events': {'train': 26950693, 'eval': 5775148, 'test': 5775149},\n",
       " 'dates': {'train': {'min': '2021-10-19T08:40:37.000000000',\n",
       "   'max': '2022-06-02T14:42:17.000000000'},\n",
       "  'eval': {'min': '2022-06-02T14:42:18.000000000',\n",
       "   'max': '2022-07-09T05:43:02.000000000'},\n",
       "  'test': {'min': '2022-07-09T05:43:04.000000000',\n",
       "   'max': '2022-08-14T05:13:27.000000000'}},\n",
       " 'users': {'train': 81756, 'eval': 20111, 'test': 17376},\n",
       " 'sessions': {'train': 466103, 'eval': 94549, 'test': 89998},\n",
       " 'label_count': {'train': 0.38163935153726847,\n",
       "  'eval': 0.4453023541561186,\n",
       "  'test': 0.46276087422160017},\n",
       " 'user_crossover': {'train_eval': 7133, 'train_test': 5235, 'eval_test': 5350},\n",
       " 'session_crossover': {'train_eval': 76, 'eval_test': 35}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('calculated_features/core_dict_30_files.json', 'w') as f:\n",
    "    json.dump(core_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = CORE_PATH + '/basic_descriptive_30_files.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata_path) as f:\n",
    "   meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(meta, max_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_events': {'train': 26950693, 'eval': 5775148, 'test': 5775149},\n",
       " 'dates': {'train': {'min': '2021-10-19T08:40:37',\n",
       "   'max': '2022-06-02T14:42:17'},\n",
       "  'eval': {'min': '2022-06-02T14:42:18', 'max': '2022-07-09T05:43:02'},\n",
       "  'test': {'min': '2022-07-09T05:43:04', 'max': '2022-08-14T05:13:27'}},\n",
       " 'users': {'train': 81756, 'eval': 20111, 'test': 17376},\n",
       " 'sessions': {'train': 466103, 'eval': 94549, 'test': 89998},\n",
       " 'label_count': {'train': 0.381, 'eval': 0.445, 'test': 0.462},\n",
       " 'user_crossover': {'train_eval': 7133, 'train_test': 5235, 'eval_test': 5350},\n",
       " 'session_crossover': {'train_eval': 76, 'eval_test': 35}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unravelled_dict = {}\n",
    "cross_over_dict = {}\n",
    "for k, v in meta.items():\n",
    "    if 'train' in v or 'eval' in v or 'test' in v:\n",
    "        unravelled_dict[k] = []\n",
    "        unravelled_dict[k].append(v['train'])\n",
    "        unravelled_dict[k].append(v['eval'])\n",
    "        unravelled_dict[k].append(v['test'])\n",
    "\n",
    "cross_over_dict['user_crossover'] = meta['user_crossover'].values()\n",
    "cross_over_dict['session_crossover'] = list(meta['session_crossover'].values())+ [0]\n",
    "unravelled_dict['date_min'] = [d['min'] for d in unravelled_dict['dates']]\n",
    "unravelled_dict['date_max'] = [d['max'] for d in unravelled_dict['dates']]\n",
    "del unravelled_dict['dates']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_crossover': dict_values([7133, 5235, 5350]),\n",
       " 'session_crossover': [76, 35, 0]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unravelled_dict\n",
    "cross_over_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unrav = pd.DataFrame(unravelled_dict)\n",
    "df_cross = pd.DataFrame(cross_over_dict)\n",
    "df_unrav = df_unrav,round(3)\n",
    "df_cross = df_cross.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unrav = df_unrav[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unrav['dataset'] = ['train', 'eval', 'test']\n",
    "df_cross['dataset'] = ['train eval', 'train test', 'eval test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unrav.set_index('dataset', inplace=True)\n",
    "df_cross.set_index('dataset', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrll}\n",
      "\\toprule\n",
      " & n_events & users & sessions & label_count & date_min & date_max \\\\\n",
      "dataset &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "train & 26950693 & 81756 & 466103 & 0.381000 & 2021-10-19T08:40:37 & 2022-06-02T14:42:17 \\\\\n",
      "eval & 5775148 & 20111 & 94549 & 0.445000 & 2022-06-02T14:42:18 & 2022-07-09T05:43:02 \\\\\n",
      "test & 5775149 & 17376 & 89998 & 0.462000 & 2022-07-09T05:43:04 & 2022-08-14T05:13:27 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_unrav.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      " & user_crossover & session_crossover \\\\\n",
      "dataset &  &  \\\\\n",
      "\\midrule\n",
      "train eval & 7133 & 76 \\\\\n",
      "train test & 5235 & 35 \\\\\n",
      "eval test & 5350 & 0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_cross.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
