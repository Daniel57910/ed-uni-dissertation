{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch --quiet\n",
    "!python -m pip install gym stable-baselines3[extra] python-dotenv fsspec[\"s3\"] s3fs==2022.11.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = [\n",
    "    \"session_terminates_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \n",
    "    \"cum_session_event_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \n",
    "    \"cum_platform_event_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time_minutes\",\n",
    "]\n",
    "\n",
    "DATE_TIME = [\n",
    "    \"date_time\",\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\",\n",
    "    \"timestamp_raw\",\n",
    "    \"date_hour_sin\",\n",
    "    \n",
    "    \"date_hour_cos\",\n",
    "    \"session_5_count\",\n",
    "    \"session_30_count\",\n",
    "    \n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time_minutes\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time_minutes\",\n",
    "    \"cum_platform_events\",\n",
    "    \n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"rolling_session_time\",\n",
    "    \n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"session_event_count\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "TORCH_LOAD_COLS = LABEL + METADATA + DATE_TIME + OUT_FEATURE_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load callback\n",
    "from stable_baselines3.common.callbacks import EveryNTimesteps, BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat\n",
    "from stable_baselines3.common.logger import Figure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "class DistributionCallback(BaseCallback):\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.log_freq = 500\n",
    "        output_formats = self.logger.output_formats\n",
    "        self.tb_formatter = next(f for f in output_formats if isinstance(f, TensorBoardOutputFormat))\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._log_freq == 0:\n",
    "            dist_list = self.training_env.env_method('dists')\n",
    "            values_to_log = np.concatenate([d for d in dist_list if d.shape[0] > 0])\n",
    "\n",
    "            session_size, sim_size, session_minutes, ended, incentive_index, reward = (\n",
    "                values_to_log[:, 0],\n",
    "                values_to_log[:, 1],\n",
    "                values_to_log[:, 2],\n",
    "                values_to_log[:, 3],\n",
    "                values_to_log[:, 4],\n",
    "                values_to_log[:, 5]\n",
    "            )\n",
    "            \n",
    "            dist_session_time = (session_minutes - reward).mean()\n",
    "            dist_session_end = (session_size - ended).mean()\n",
    "            dist_incentive_session = (session_size - incentive_index).mean()\n",
    "            dist_incentive_end = (ended - incentive_index).mean()\n",
    "            dist_incentive_sim_size = (ended - sim_size).mean()\n",
    "            \n",
    "            n_call = self.n_calls // self._log_freq\n",
    "            \n",
    "            self.tb_formatter.writer.add_scalar('event/sess_time_sub_sime_time::decrease', dist_session_time, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('event/sess_index_sub_sim_index::decrease', dist_session_end, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('event/sim_incentive_index_sub_index_no_reward::increase', dist_incentive_sim_size, n_call)\n",
    "            \n",
    "            self.tb_formatter.writer.add_scalar('event/sess_index_sub_incentive_index', dist_incentive_session, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('event/sim_index_sub_incentive_index', dist_incentive_end, n_call)\n",
    "            \n",
    "            self.tb_formatter.writer.flush()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "\n",
    "METADATA_STAT_COLUMNS = [\n",
    "    'session_size',\n",
    "    'sim_size',\n",
    "    'session_minutes',\n",
    "    'ended',\n",
    "    'incentive_index',\n",
    "    'reward'\n",
    "]\n",
    "\n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, dataset, n_sequences):\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.n_sequences = n_sequences\n",
    "        self.current_session = None\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        self.metadata_container = []\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(n_sequences + 1, 18), dtype=np.float32)\n",
    "        self.n_sequences = n_sequences\n",
    "\n",
    "    def reset(self):\n",
    "        session_to_run = self.dataset.sample()\n",
    "        if torch.cuda.is_available():\n",
    "            session_to_run = session_to_run.to_pandas()\n",
    "        self.current_session = self._get_events(session_to_run.to_dict('records')[0])\n",
    "        self.metadata = self._metadata()\n",
    "        self.current_session_index = 1\n",
    "        self.reward = 0\n",
    "        return self._state()\n",
    "\n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        next_state, done, meta = self._calculate_next_state()\n",
    "        if done:\n",
    "            self.metadata['ended'] = self.current_session_index\n",
    "            self.metadata['reward'] = self.reward\n",
    "            self.metadata_container.append(self.metadata[METADATA_STAT_COLUMNS].values)\n",
    "            return next_state, self.reward, done, meta\n",
    "        self.reward += (self.current_session.iloc[self.current_session_index]['reward'] / 60)\n",
    "        self.current_session_index += 1        \n",
    "        return next_state, self.reward, done, meta\n",
    "    \n",
    "    def _metadata(self):\n",
    "        session_metadata = self.current_session.iloc[0][['user_id', 'session_30_raw', 'session_size', 'sim_size', 'session_minutes']]\n",
    "        session_metadata['ended'] = 0\n",
    "        session_metadata['incentive_index'] = 0\n",
    "        session_metadata['reward'] = 0\n",
    "        return session_metadata\n",
    "    \n",
    "    \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        if self.current_session_index == self.current_session.shape[0]:\n",
    "            return None, True, {}\n",
    "        \n",
    "        if self._continuing_in_session():\n",
    "            return self._state(), False, {}\n",
    "      \n",
    "        return None, True, {}\n",
    "  \n",
    "    def _continuing_in_session(self):\n",
    "        sim_counts = self.metadata['sim_size']\n",
    "        if self.current_session_index < sim_counts:\n",
    "            return True\n",
    "        \n",
    "        extending_session = self._probability_extending_session()\n",
    "        \n",
    "        return all([extending_session >= .3, extending_session <= .8])\n",
    "        \n",
    "    \n",
    "    def _probability_extending_session(self):\n",
    "        if self.metadata['incentive_index'] == 0:\n",
    "            return 0\n",
    "        \n",
    "        scale = max(5, int(self.metadata['session_size'] / 4))\n",
    "        continue_session = norm(\n",
    "            loc=self.metadata['incentive_index'],\n",
    "            scale=scale\n",
    "        ).cdf(self.current_session_index)\n",
    "        \n",
    "        return continue_session\n",
    "        \n",
    "\n",
    "    def _get_events(self, session):\n",
    "        subset = self.dataset[\n",
    "            (self.dataset['session_30_raw'] == session['session_30_raw']) & \n",
    "            (self.dataset['user_id'] == session['user_id'])\n",
    "        ]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            subset = subset.to_pandas()\n",
    "        \n",
    "        subset = subset.astype(np.float16)\n",
    "        return subset.sort_values('cum_session_event_raw').reset_index(drop=True)\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        if action == 0 or self.metadata['incentive_index'] > 0:\n",
    "            return\n",
    "        \n",
    "        self.metadata['incentive_index'] = self.current_session_index\n",
    "        \n",
    "    def _state(self):\n",
    "\n",
    "        if self.current_session_index > self.n_sequences:\n",
    "            events = self.current_session.iloc[self.current_session_index - (self.n_sequences + 1):self.current_session_index][OUT_FEATURE_COLUMNS].values\n",
    "            \n",
    "        else:\n",
    "            delta = (self.n_sequences + 1)- self.current_session_index\n",
    "            zero_cat = np.zeros((delta, len(OUT_FEATURE_COLUMNS)))\n",
    "            events = self.current_session.iloc[:max(self.current_session_index, 1)][OUT_FEATURE_COLUMNS].values\n",
    "            events = np.concatenate((zero_cat, events), axis=0)\n",
    "            \n",
    "\n",
    "        return events.astype(np.float16)\n",
    "  \n",
    "    \n",
    "    def dists(self):\n",
    "        metadata_container = self.metadata_container.copy()\n",
    "        self.metadata_container = []\n",
    "        return np.array(metadata_container)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load incentive_reinforcement_learning_cpu.py\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "from stable_baselines3.common.callbacks import CallbackList, StopTrainingOnMaxEpisodes, CheckpointCallback\n",
    "from stable_baselines3 import A2C\n",
    "import logging\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "from pprint import pformat\n",
    "import os\n",
    "if torch.cuda.is_available():\n",
    "    import cudf as pd\n",
    "    from cuml.preprocessing import MinMaxScaler\n",
    "else:\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "torch.set_printoptions(precision=2, linewidth=200, sci_mode=False)\n",
    "\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "CUM_SESSION_EVENT_RAW = 3\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "S3_BASELINE_PATH = 's3://dissertation-data-dmiller'\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--read_path', type=str, default='datasets/calculated_features/files_used')\n",
    "    parser.add_argument('--n_files', type=str, default=2)\n",
    "    parser.add_argument('--n_sequences', type=int, default=10)\n",
    "    parser.add_argument('--n_episodes', type=int, default=20)\n",
    "    parser.add_argument('--n_envs', type=int, default=100)\n",
    "    parser.add_argument('--device', type=str, default='cpu')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def train_eval_split(dataset, logger):\n",
    "    train_split = int(dataset.shape[0] * TRAIN_SPLIT)\n",
    "    eval_split = int(dataset.shape[0] * EVAL_SPLIT)\n",
    "    test_split = dataset.shape[0] - train_split - eval_split\n",
    "    logger.info(f'Train size: 0:{train_split}, eval size: {train_split}:{train_split+eval_split}: test size: {train_split + eval_split}:{dataset.shape[0]}')\n",
    "    train_dataset, eval_dataset, test_split = dataset[:train_split], dataset[train_split:train_split+eval_split], dataset[train_split+eval_split:]\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'eval': eval_dataset,\n",
    "        'test': test_split\n",
    "    }\n",
    "\n",
    "def generate_metadata(dataset):\n",
    "    \n",
    "    session_size = dataset.groupby(['user_id', 'session_30_raw']).size().reset_index(name='session_size')\n",
    "    session_minutes = dataset.groupby(['user_id', 'session_30_raw'])['cum_session_time_raw'].max().reset_index(name='session_minutes')\n",
    "    session_size['sim_size'] = (session_size['session_size'] * .75).astype(int).apply(lambda x: x if x > 1 else 1)\n",
    "    dataset = dataset.merge(session_size, on=['user_id', 'session_30_raw'])\n",
    "    dataset = dataset.merge(session_minutes, on=['user_id', 'session_30_raw'])\n",
    "    return dataset\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def run_reinforcement_learning_incentives(environment, logger, n_episodes=1):\n",
    "    for epoch in range(n_episodes):\n",
    "        environment_comp = False\n",
    "        state = environment.reset()\n",
    "        i = 0\n",
    "        while not environment_comp:\n",
    "            next_action = (\n",
    "                1 if np.random.uniform(low=0, high=1) > 0.8 else 0\n",
    "            )\n",
    "            state, rewards, environment_comp, meta = environment.step(next_action)\n",
    "            i +=1\n",
    "            if i % 100 == 0:\n",
    "                logger.info(f'Step: {i} - Reward: {rewards}')\n",
    "                \n",
    "        logger.info(f'Epoch: {epoch} - Reward: {rewards}')\n",
    "        print(environment.user_sessions.head(10))\n",
    "\n",
    "    \n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    exec_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    logger.info('Starting Incentive Reinforcement Learning')\n",
    "    \n",
    "    read_path, n_files, n_sequences, n_episodes, device, n_envs = (\n",
    "        args.read_path, \n",
    "        args.n_files, \n",
    "        args.n_sequences, \n",
    "        args.n_episodes, \n",
    "        args.device,\n",
    "        args.n_envs\n",
    "    )\n",
    "    \n",
    "    logger.info(f'Reading data from {read_path}_{n_files}.parquet')\n",
    "    df = pd.read_parquet(f'{read_path}_{n_files}.parquet', columns=TORCH_LOAD_COLS)\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    df = df.sort_values(by=['date_time']).drop(columns=['date_time'])\n",
    "    df = df.head(int(df.shape[0] * 0.7))\n",
    "    logger.info('Data read: generating metadata')\n",
    "    df['reward'] = df['delta_last_event']\n",
    "    df = generate_metadata(df)\n",
    "    df[OUT_FEATURE_COLUMNS] = MinMaxScaler().fit_transform(df[OUT_FEATURE_COLUMNS])\n",
    "    unique_episodes = df[['user_id', 'session_30_raw']].drop_duplicates().shape[0]\n",
    "    logger.info(f'Parralelizing environment with {n_envs} environments')\n",
    "\n",
    "    citizen_science_vec = DummyVecEnv([lambda: CitizenScienceEnv(df, n_sequences) for _ in range(n_envs)])\n",
    "\n",
    "    logger.info(f'Vectorized environments created, wrapping with monitor')\n",
    "\n",
    "    base_path = os.path.join(\n",
    "        S3_BASELINE_PATH,\n",
    "        'reinforcement_learning_incentives',\n",
    "        f'n_files_{n_files}',\n",
    "        'results',\n",
    "        exec_time,\n",
    "    ) \n",
    "    \n",
    "    tensorboard_dir, checkpoint_dir = (\n",
    "        os.path.join(base_path, 'training_metrics'),\n",
    "        os.path.join(base_path, 'checkpoints')\n",
    "    )\n",
    "\n",
    "    callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=n_episodes, verbose=1)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=1000 // n_envs, save_path=checkpoint_dir, name_prefix='rl_model')\n",
    "    dist_callback = DistributionCallback()\n",
    "    callback_list = CallbackList([callback_max_episodes, dist_callback, checkpoint_callback])\n",
    "    monitor_train = VecMonitor(citizen_science_vec)\n",
    "    \n",
    "    model = A2C(\n",
    "        \"MlpPolicy\", \n",
    "        monitor_train, \n",
    "        verbose=2, \n",
    "        tensorboard_log=tensorboard_dir,\n",
    "        stats_window_size=1000)\n",
    "            \n",
    "    logger.info(pformat([\n",
    "        'n_epochs: {}'.format(n_episodes),\n",
    "        'read_path: {}'.format(read_path),\n",
    "        'n_files: {}'.format(n_files),\n",
    "        'n_sequences: {}'.format(n_sequences),\n",
    "        'n_envs: {}'.format(n_envs),\n",
    "        'total_timesteps: {}'.format(df.shape),\n",
    "        f'unique_episodes: {unique_episodes}',\n",
    "        'device: {}'.format(device),\n",
    "        'tensorboard_dir: {}'.format(tensorboard_dir),\n",
    "        'checkpoint_dir: {}'.format(checkpoint_dir)\n",
    "    ]))\n",
    "\n",
    "\n",
    "    model.learn(total_timesteps=100_000_000, progress_bar=True, log_interval=100, callback=callback_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument:\n",
    "    read_path = 'calculated_features/files_used'\n",
    "    n_files = 30\n",
    "    n_sequences = 10\n",
    "    n_episodes = 250_000\n",
    "    n_envs = 500\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/20/2023 09:19:33 AM Starting Incentive Reinforcement Learning\n",
      "04/20/2023 09:19:33 AM Reading data from calculated_features/files_used_30.parquet\n",
      "04/20/2023 09:19:38 AM Data read: generating metadata\n",
      "04/20/2023 09:19:43 AM Parralelizing environment with 500 environments\n",
      "04/20/2023 09:19:43 AM Vectorized environments created, wrapping with monitor\n",
      "04/20/2023 09:19:43 AM ['n_epochs: 250000',\n",
      " 'read_path: calculated_features/files_used',\n",
      " 'n_files: 30',\n",
      " 'n_sequences: 10',\n",
      " 'n_envs: 500',\n",
      " 'total_timesteps: (26950693, 30)',\n",
      " 'unique_episodes: 466103',\n",
      " 'device: cpu',\n",
      " 'tensorboard_dir: '\n",
      " 's3://dissertation-data-dmiller/reinforcement_learning_incentives/n_files_30/results/2023-04-20-09-19/training_metrics',\n",
      " 'checkpoint_dir: '\n",
      " 's3://dissertation-data-dmiller/reinforcement_learning_incentives/n_files_30/results/2023-04-20-09-19/checkpoints']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/20/2023 09:20:10 AM Found credentials in environment variables.\n",
      "04/20/2023 09:20:11 AM Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to s3://dissertation-data-dmiller/reinforcement_learning_incentives/n_files_30/results/2023-04-20-09-19/training_metrics/A2C_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17acee4f1074cc481486f7f6b3041d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 126      |\n",
      "|    ep_rew_mean        | 3601.674 |\n",
      "| time/                 |          |\n",
      "|    fps                | 1237     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 201      |\n",
      "|    total_timesteps    | 250000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.594   |\n",
      "|    explained_variance | 0.00096  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 70.7     |\n",
      "|    value_loss         | 3.39e+04 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 238      |\n",
      "|    ep_rew_mean        | 9342.826 |\n",
      "| time/                 |          |\n",
      "|    fps                | 1329     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 376      |\n",
      "|    total_timesteps    | 500000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.689   |\n",
      "|    explained_variance | 0.000396 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 101      |\n",
      "|    value_loss         | 5.19e+04 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 280       |\n",
      "|    ep_rew_mean        | 12217.732 |\n",
      "| time/                 |           |\n",
      "|    fps                | 1365      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 549       |\n",
      "|    total_timesteps    | 750000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.691    |\n",
      "|    explained_variance | 0.0008    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 109       |\n",
      "|    value_loss         | 6.14e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 298       |\n",
      "|    ep_rew_mean        | 15171.032 |\n",
      "| time/                 |           |\n",
      "|    fps                | 1390      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 719       |\n",
      "|    total_timesteps    | 1000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.626    |\n",
      "|    explained_variance | 0.000422  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 97.5      |\n",
      "|    value_loss         | 6.23e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 316       |\n",
      "|    ep_rew_mean        | 15936.054 |\n",
      "| time/                 |           |\n",
      "|    fps                | 1409      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 887       |\n",
      "|    total_timesteps    | 1250000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.364    |\n",
      "|    explained_variance | 0.000272  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 58.7      |\n",
      "|    value_loss         | 7.47e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 317       |\n",
      "|    ep_rew_mean        | 17770.775 |\n",
      "| time/                 |           |\n",
      "|    fps                | 1419      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 1056      |\n",
      "|    total_timesteps    | 1500000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.326    |\n",
      "|    explained_variance | 0.000206  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 49.7      |\n",
      "|    value_loss         | 6.59e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 323       |\n",
      "|    ep_rew_mean        | 17555.455 |\n",
      "| time/                 |           |\n",
      "|    fps                | 1428      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 1225      |\n",
      "|    total_timesteps    | 1750000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.419    |\n",
      "|    explained_variance | 0.000145  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 71.1      |\n",
      "|    value_loss         | 8e+04     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 314       |\n",
      "|    ep_rew_mean        | 16782.191 |\n",
      "| time/                 |           |\n",
      "|    fps                | 1433      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 1394      |\n",
      "|    total_timesteps    | 2000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.653    |\n",
      "|    explained_variance | 9.73e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 121       |\n",
      "|    value_loss         | 1.03e+05  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 319       |\n",
      "|    ep_rew_mean        | 19567.025 |\n",
      "| time/                 |           |\n",
      "|    fps                | 1439      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 1563      |\n",
      "|    total_timesteps    | 2250000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.592    |\n",
      "|    explained_variance | 5.2e-05   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 103       |\n",
      "|    value_loss         | 8.41e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 339       |\n",
      "|    ep_rew_mean        | 20352.361 |\n",
      "| time/                 |           |\n",
      "|    fps                | 1445      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 1729      |\n",
      "|    total_timesteps    | 2500000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.385    |\n",
      "|    explained_variance | 4.82e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 66.9      |\n",
      "|    value_loss         | 8.42e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 337      |\n",
      "|    ep_rew_mean        | 20426.98 |\n",
      "| time/                 |          |\n",
      "|    fps                | 1446     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 1900     |\n",
      "|    total_timesteps    | 2750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.428   |\n",
      "|    explained_variance | 7.78e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 74.9     |\n",
      "|    value_loss         | 8.45e+04 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 325       |\n",
      "|    ep_rew_mean        | 19694.783 |\n",
      "| time/                 |           |\n",
      "|    fps                | 1449      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 2242      |\n",
      "|    total_timesteps    | 3250000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.529    |\n",
      "|    explained_variance | 4.08e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 93.9      |\n",
      "|    value_loss         | 8.59e+04  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main(Argument)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
