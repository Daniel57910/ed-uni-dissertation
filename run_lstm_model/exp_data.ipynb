{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmiller/.pyenv/versions/3.10.8/envs/deep_learning_3_10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "np.set_printoptions(linewidth=400)\n",
    "np.set_printoptions(suppress=True)\n",
    "torch.set_printoptions(linewidth=400)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "LABEL_INDEX = 1\n",
    "TOTAL_EVENTS_INDEX = 2\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SCALED_COLS =[\n",
    "    'timestamp',\n",
    "    'time_diff_seconds',\n",
    "    '30_minute_session_count',\n",
    "    '5_minute_session_count',\n",
    "    'task_within_session_count',\n",
    "    'user_count',\n",
    "    'project_count',\n",
    "    'country_count',\n",
    "]\n",
    "\n",
    "GENERATED_COLS = [\n",
    "    'cum_events',\n",
    "    'cum_projects',\n",
    "    'cum_time',\n",
    "    'cum_time_within_session',\n",
    "    'av_time_across_clicks',\n",
    "    'av_time_across_clicks_session',\n",
    "    'rolling_average_tasks_within_session',\n",
    "    'rolling_av_time_within_session',\n",
    "    'rolling_time_between_sessions',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "botocore.client.S3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load npz_extractor.py\n",
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import torch\n",
    "from data_module import ClickstreamDataModule\n",
    "from data_module import ClickstreamDataset\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "import pdb\n",
    "class NPZExtractor:\n",
    "    def __init__(self, input_path, n_files, n_sequences, s3_client, data_partition=None) -> None:\n",
    "        self.input_path = input_path\n",
    "        self.n_files = n_files\n",
    "        self.n_sequences = n_sequences\n",
    "        self.s3_client = s3_client\n",
    "        self.data_partition = data_partition\n",
    "\n",
    "\n",
    "    def get_dataset_pointer(self):\n",
    "\n",
    "        read_path = os.path.join(self.input_path, f'files_used_{self.n_files}')\n",
    "        if not os.path.exists(read_path):\n",
    "            print(f'Creating directory: {read_path}')\n",
    "            os.makedirs(read_path)\n",
    "\n",
    "\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            key_zip, key_npy = (\n",
    "                os.path.join(read_path, f'sequence_index_{_}.npz'),\n",
    "                os.path.join(read_path, f'sequence_index_{_}/arr_0.npy')\n",
    "            )\n",
    "\n",
    "            print(f'Loading pointer to dataset: {key_npy}: derived from {key_zip}')\n",
    "\n",
    "            if not os.path.exists(key_npy):\n",
    "                print(f'Zip file to extract: {key_zip}: npy file to load: {key_npy}')\n",
    "                self.s3_client.download_file(\n",
    "                    'dissertation-data-dmiller',\n",
    "                    key_zip,\n",
    "                    key_zip\n",
    "                )\n",
    "                print(f'Zip file downloaded: {key_zip}')\n",
    "                self._zip_extract(key_zip, key_npy)\n",
    "\n",
    "        lz_concatenated_results = self._lazy_concatenate()\n",
    "\n",
    "        if self.data_partition is not None:\n",
    "            print(f'Returning partition based on {self.data_partition}')\n",
    "            return lz_concatenated_results[:self.data_partition]\n",
    "\n",
    "        return lz_concatenated_results\n",
    "\n",
    "    def _zip_extract(self, key_zip, key_npy):\n",
    "        print(f'Extracting file: {key_zip} -> {key_npy}')\n",
    "\n",
    "        with zipfile.ZipFile(key_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=key_npy, members=['arr_0.npy'])\n",
    "\n",
    "        print(f'Zip file exracted: {key_zip} -> {key_npy}/arr_0.npy')\n",
    "\n",
    "    def _lazy_concatenate(self):\n",
    "        lz_concat = []\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            path_to_load = os.path.join(self.input_path, f'files_used_{self.n_files}', f'sequence_index_{_}', f'arr_0.npy')\n",
    "            lz_concat.append(np.load(path_to_load))\n",
    "        return lz_concat\n",
    "\n",
    "\n",
    "# extractor = NPZExtractor(\n",
    "#     'torch_ready_data_4',\n",
    "#     5,\n",
    "#     10,\n",
    "#     boto3.client('s3')\n",
    "# )\n",
    "\n",
    "# lz_concatenated_results = extractor.get_dataset_pointer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(tensor, n_sequences):\n",
    "\n",
    "    label, total_events, user_id, project_id, country, features, shifters = (\n",
    "        tensor[:, 0],tensor[:, 1], tensor[:, 2], \n",
    "        tensor[:, 3], tensor[:, 4], tensor[:, 5:5+17], tensor[:, 5+17:]\n",
    "    )\n",
    "\n",
    "    shifters = torch.reshape(shifters, (shifters.shape[0], n_sequences-1, 18))\n",
    "    shifter_project_id, shifter_features = shifters[:, :, 0], shifters[:, :, 1:]\n",
    "\n",
    "    features, user_id, project_id, country = (\n",
    "        torch.flip(torch.cat((features.unsqueeze(1), shifter_features), dim=1), dims=[1]),\n",
    "        user_id.repeat(n_sequences).unsqueeze(1),\n",
    "        torch.flip(torch.cat((project_id.unsqueeze(1), shifter_project_id.unsqueeze(1)), dim=1), dims=[1]),\n",
    "        country.repeat(n_sequences).unsqueeze(1)\n",
    "    )\n",
    "\n",
    "    display(features.shape, user_id.shape, project_id.shape, country.shape)\n",
    "    user_id, country = torch.where(project_id == 0, 0, user_id), torch.where(project_id == 0, 0, country)\n",
    "\n",
    "    return {\n",
    "        'label': label,\n",
    "        'total_events': total_events,\n",
    "        'user_id': user_id,\n",
    "        'project_id': project_id,\n",
    "        'country': country,\n",
    "        'features': features\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer to dataset: torch_ready_data_4/files_used_5/sequence_index_0/arr_0.npy: derived from torch_ready_data_4/files_used_5/sequence_index_0.npz\n",
      "Loading pointer to dataset: torch_ready_data_4/files_used_5/sequence_index_10/arr_0.npy: derived from torch_ready_data_4/files_used_5/sequence_index_10.npz\n"
     ]
    }
   ],
   "source": [
    "extractor = NPZExtractor(\n",
    "    'torch_ready_data_4',\n",
    "    5,\n",
    "    10,\n",
    "    boto3.client('s3')\n",
    ")\n",
    "\n",
    "lz_concatenated_results = extractor.get_dataset_pointer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.concatenate(lz_concatenated_results, axis=1)\n",
    "results = torch.tensor(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = results[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, total_events, user_id, project_id, country, features, shifters = sample[:, 0], sample[:, 1], sample[:, 2], sample[:, 3], sample[:, 4], sample[:, 5:5+17], sample[:, 5+17:]\n",
    "shifters = torch.reshape(shifters, (shifters.shape[0], 10, 18))\n",
    "shifter_project_id, shifter_features = shifters[:, :, 0], shifters[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, user_id, project_id, country = features.unsqueeze(1), user_id.unsqueeze(1), project_id.unsqueeze(1), country.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.cat((features, shifter_features), dim=1)\n",
    "features = torch.flip(features, dims=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id, project, country, = user_id.repeat(1, 11), torch.flip(torch.cat((project_id, shifter_project_id), dim=1), dims=[1]), country.repeat(1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id, country = torch.where(project == 0, 0, user_id), torch.where(project == 0, 0, country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 239.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1da7ff4cdcccf44e7e228c52b231f7d5c5854d5618af555ed3871fd5cba609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
