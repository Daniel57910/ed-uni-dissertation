{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 22.10.1+2.gca9a422da9 requires cupy-cuda115<12.0.0a0,>=9.5.0, which is not installed.\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.152 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install --upgrade awscli python-dotenv pqdm --quiet\n",
    "%load_ext dotenv\n",
    "%dotenv env\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync rl_ready_data_conv s3://dissertation-data-dmiller/rl_ready_data_conv --delete --quiet\n",
    "!aws s3 sync rl_ready_data s3://dissertation-data-dmiller/rl_ready_data --delete --quiet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync torch_ready_data s3://dissertation-data-dmiller/torch_ready_data --include \"*\" --exclude \"*.npy\" --delete --dryrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant\n",
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\"\n",
    "    \"date_time\",\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"user_count\",\n",
    "    \"project_count\",\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "RESCALER_COLS = [\n",
    "    'session_30_count',\n",
    "    'session_5_count',\n",
    "    'cum_session_event',\n",
    "    'cum_session_time',\n",
    "    'cum_platform_time',\n",
    "    'cum_platform_event',\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    'pred',\n",
    "    'label'\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'sim_size',\n",
    "    'session_minutes',\n",
    "    'sim_minutes',\n",
    "    'reward',\n",
    "    'cum_session_time_raw'\n",
    "]\n",
    "\n",
    "TORCH_LOAD_COLS = [\n",
    "    'user_id',\n",
    "    'date_time'\n",
    "] + [\n",
    "    \"user_count\",\n",
    "    \"project_count\",\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \"expanding_click_average\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "    \"delta_last_event\"\n",
    "] + [\n",
    "    'continue_work_session_30_minutes',\n",
    "    'session_30_raw',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import cudf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint, pformat\n",
    "import cudf as gpu_pd\n",
    "\n",
    "logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "COLS_FOR_INFLECTION = [\n",
    "    'user_id',\n",
    "    'date_time',\n",
    "] + OUT_FEATURE_COLUMNS + PREDICTION_COLS\n",
    "\n",
    "class SessionCalculate:\n",
    "    logger = logging.getLogger('rl_results_eval')\n",
    "    def __init__(self, df, use_gpu) -> None:\n",
    "        self.df = df\n",
    "        self.use_gpu = use_gpu\n",
    "            \n",
    "        \n",
    "    def calculate_inflections(self):\n",
    "      \n",
    "        self.logger.info('Calculating subsequent date time')\n",
    "        self.df['next_date_time'] = self.df.groupby('user_id')['date_time'].shift(-1)\n",
    "        self.df = self.df.drop_duplicates(subset=['user_id', 'date_time'], keep='last').reset_index()\n",
    "        if self.use_gpu:\n",
    "            self.logger.info('Bringing to CPU for second calculation')\n",
    "            self.df = self.df.to_pandas()\n",
    "           \n",
    "            \n",
    "        self.df['diff_seconds'] = (self.df['next_date_time'] - self.df['date_time']).apply(lambda x: x.total_seconds())\n",
    "        \n",
    "        self.logger.info('Diff seconds calculated')\n",
    "        if self.use_gpu:\n",
    "            self.logger.info('Bringing back to GPU for final calculations')\n",
    "            self.df = cudf.from_pandas(self.df)\n",
    "\n",
    "        self.df['diff_minutes'] = (self.df['diff_seconds'] / 60)\n",
    "        self.df['session_5'] = (self.df['diff_minutes'] < 5)\n",
    "        self.df['session_30'] = self.df['diff_minutes'] < 30\n",
    "        \n",
    "        self.df['session_30'] = self.df['session_30'].fillna(False)\n",
    "        self.df['session_5'] = self.df['session_5'].fillna(False)        \n",
    "        self.logger.info(f'Labels calculated: removing rows with diff seconds > 0')\n",
    "       \n",
    "        \n",
    "\n",
    "        self.logger.info(f'Number of rows following drop: {self.df.shape[0]}')\n",
    "        self.logger.info(f'Sorting rows by date time and applying row count')\n",
    "        self.df = self.df.sort_values(['date_time']).reset_index()\n",
    "        self.df['row_count'] = self.df.index.values\n",
    "        self.logger.info(f'Sorted rows and applied row count on updated index')  \n",
    "        self.logger.info('Calculating inflection points')\n",
    "        self.df['user_id'] = self.df['user_id'].astype('int32')\n",
    "        \n",
    "       \n",
    "        inflections_5_merge = self.df[self.df['session_5'] == False].sort_values(by=['date_time'])\n",
    "        inflections_30_merge = self.df[self.df['session_30'] == False].sort_values(by=['date_time']) \n",
    "     \n",
    "        self.logger.info('Calculating session 5 inflections') \n",
    "        inflections_5_merge['session_5'] = inflections_5_merge.groupby('user_id').cumcount() + 1\n",
    "        inflections_5_merge = inflections_5_merge.rename(columns={'session_5': 'session_5_count'})\n",
    "        \n",
    "        self.logger.info('Calculating session 30 inflections')\n",
    "        inflections_30_merge['session_30'] = inflections_30_merge.groupby('user_id').cumcount() + 1\n",
    "        inflections_30_merge = inflections_30_merge.rename(columns={'session_30': 'session_30_count'})\n",
    "        \n",
    "        inflections_5_merge = inflections_5_merge[['user_id', 'date_time', 'row_count', 'session_5_count']].sort_values(by=['row_count', 'user_id'])\n",
    "        inflections_30_merge = inflections_30_merge[['user_id', 'date_time', 'row_count', 'session_30_count']].sort_values(by=['row_count', 'user_id'])\n",
    "        inflections_5_merge = inflections_5_merge.drop(columns=['date_time'])\n",
    "        \n",
    "        inflections_30_merge = inflections_30_merge.rename(columns={'date_time': 'session_end_time'})\n",
    "\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            self.logger.info('Bringing back to GPU for labelling')\n",
    "            self.df, inflections_5_merge, inflections_30_merge = self.df.to_pandas(), inflections_5_merge.to_pandas(), inflections_30_merge.to_pandas()\n",
    "            self.df = self.df.sort_values(by=['row_count', 'user_id'])\n",
    "            self.df = pd.merge_asof(self.df, inflections_5_merge, on='row_count', by='user_id', direction='forward')\n",
    "            self.df = pd.merge_asof(self.df, inflections_30_merge, on='row_count', by='user_id', direction='forward')\n",
    "            self.df['session_terminates_30_minutes'] = (self.df['session_end_time'] - self.df['date_time']).apply(lambda x: x.total_seconds() / 60) < 30\n",
    "            self.df = cudf.from_pandas(self.df)\n",
    "        else:\n",
    "            self.logger.info('Labelling on CPU')\n",
    "            self.df = pd.merge_asof(self.df.sort_values(by=['row_count', 'user_id']), inflections_5_merge, on='row_count', by='user_id', direction='forward')\n",
    "\n",
    "            self.df = pd.merge_asof(self.df.sort_values(by=['row_count', 'user_id']), inflections_5_merge, on='row_count', by='user_id', direction='forward')\n",
    "            self.df = pd.merge_asof(self.df.sort_values(by=['row_count', 'user_id']), inflections_30_merge, on='row_count', by='user_id', direction='forward') \n",
    "            self.df['session_terminates_30_minutes'] = (self.df['session_end_time'] - self.df['date_time']).apply(lambda x: x.total_seconds() / 60) < 30\n",
    " \n",
    "        self.logger.info('Inflections calculated')\n",
    " \n",
    "        session_end_30_minutes = self.df[self.df['session_terminates_30_minutes'] == False].shape[0]\n",
    "        self.logger.info(f'Percent sessions end in 30 minutes: {session_end_30_minutes / self.df.shape[0]}')\n",
    "        self.logger.info(f'Columns for df') \n",
    "        self.logger.info(pformat(self.df.columns))\n",
    "        \n",
    "        return self.df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "global logger\n",
    "logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('rl_results_eval')\n",
    "from functools import reduce\n",
    "from pprint import pformat\n",
    "import cudf as gpu_pd\n",
    "import cupy as gpu_np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('mode.use_inf_as_na', True)\n",
    "from pqdm.processes import pqdm\n",
    "from cuml.preprocessing import MinMaxScaler\n",
    "\n",
    "def convolve_delta_events(df, window, write_path):\n",
    "    \n",
    "    df = df.to_pandas()\n",
    "   \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    before_resample = df.shape\n",
    "    logger.info(f'Convolutional shape before resample: {before_resample}')\n",
    "    logger.info(f'Convolution over delta last event')\n",
    "    \n",
    "    df['delta_last_event'] = (\n",
    "        df.sort_values(by=['session_30_raw', 'cum_session_event_raw']) \\\n",
    "            .set_index('date_time') \\\n",
    "            .groupby(by=['user_id', 'session_30_raw'], group_keys=False) \\\n",
    "            .rolling(f'{window}T', min_periods=1)['delta_last_event'] \\\n",
    "            .mean()\n",
    "            .reset_index(name='convolved_event_delta')['convolved_event_delta'] \\\n",
    "    ) \n",
    "\n",
    "    df = df.drop(columns=['session_30_raw'])\n",
    "    df = df.loc[:,~df.columns.duplicated()].reset_index(drop=True)  \n",
    "    \n",
    "\n",
    "    \n",
    "    df = gpu_pd.from_pandas(df)\n",
    "  \n",
    "    # remove duplicate columns\n",
    "    df['year'] = df['date_time'].dt.year\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['minute'] = df['date_time'].dt.minute\n",
    "    df['second'] = df['date_time'].dt.second\n",
    "    \n",
    "    df['user_id'] = df['user_id'].astype('int32')\n",
    "    \n",
    "   \n",
    "    resampled_df = df.sort_values(by='date_time') \\\n",
    "        .drop_duplicates(subset=['user_id', 'year', 'month', 'day', 'hour', 'minute'], keep='last') \\\n",
    "        .sort_values(by=['date_time']) \\\n",
    "        .reset_index()\n",
    "    \n",
    "    logger.info(f'Convolution complete: {before_resample} -> {resampled_df.shape}')\n",
    "    logger.info(f'Writing intermediate results to {write_path}_convolve.parquet')\n",
    "    \n",
    "    resampled_df.to_parquet(os.path.join(write_path, 'convolve.parquet'))\n",
    "    \n",
    "    logger.info(f'Recalculating inflections')\n",
    "    resample_events = SessionCalculate(resampled_df, use_gpu=True)\n",
    "    resampled_event_out = resample_events.calculate_inflections()\n",
    "   \n",
    "    logger.info(f'Events sessionized: writing to {write_path}_session.parquet')\n",
    "    \n",
    "    resampled_event_out.to_parquet(os.path.join(write_path, 'session.parquet'))\n",
    "    logger.info(f'Events resampled')\n",
    "    \n",
    "    return resampled_event_out\n",
    "     \n",
    "     \n",
    "\n",
    "def generate_metadata_session(dataset):\n",
    "    \n",
    "    logger.info(f'Calculating session size and minutes')\n",
    "    session_size = dataset.groupby(['user_id', 'session_30_count'])['cum_session_event'].max().reset_index(name='session_size')\n",
    "    session_minutes = dataset.groupby(['user_id', 'session_30_count'])['cum_session_time'].max().reset_index(name='session_minutes')\n",
    "    \n",
    "    \n",
    "    logger.info(f'Calculating sim size and minutes')\n",
    "    sim_minutes = dataset.groupby(['user_id', 'session_30_count'])['cum_session_time'].quantile(.5, interpolation='nearest').reset_index(name='time_cutoff')\n",
    "    sim_size = dataset.groupby(['user_id', 'session_30_count'])['cum_session_event'].quantile(.5, interpolation='nearest').reset_index(name='size_cutoff')\n",
    "    \n",
    "    \n",
    "    sessions = [session_size, session_minutes, sim_minutes, sim_size]\n",
    "    logger.info(f'Merging metadata')\n",
    "    sessions = reduce(lambda left, right: pd.merge(left, right, on=['user_id', 'session_30_count']), sessions)\n",
    "   \n",
    "    logger.info(f'Merging metadata complete')\n",
    "    dataset = pd.merge(dataset, sessions, on=['user_id', 'session_30_count'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def setup_data_at_window(df, window, write_path):\n",
    "    logger.info(f'Convolution over {window} minute window')\n",
    "    df = df.sort_values(by='date_time')\n",
    "    df['cum_session_event_raw'] = df.groupby(['user_id', 'session_30_raw'])['date_time'].cumcount() + 1\n",
    "    df = df.sort_values(by='date_time').reset_index(drop=True)\n",
    "    df  = convolve_delta_events(df, window, write_path)\n",
    "    logger.info(f'Convolving over {window} minute window complete: generating metadata')\n",
    "    logger.info(f'Generating metadata complete')\n",
    "    return df\n",
    "\n",
    "def partition_and_scale_data(df):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    train_split, eval_split = df[:int(df.shape[0] * .7)], df[int(df.shape[0] * .7):]\n",
    "    train_split[RESCALER_COLS] = scaler.fit_transform(train_split[RESCALER_COLS])\n",
    "    eval_split[RESCALER_COLS] = scaler.transform(eval_split[RESCALER_COLS])\n",
    "    return train_split, eval_split\n",
    "\n",
    "\n",
    "def _parralel_partition_users(unique_sessions, df, index, vec_df_path):\n",
    "    subset_session = df.merge(unique_sessions, on=['user_id', 'session_30_count_raw'], how='inner').reset_index(drop=True)\n",
    "    subset_session.to_parquet(f'{vec_df_path}/batch_{index}.parquet')\n",
    "    \n",
    "\n",
    "def batch_environments_for_vectorization(df, n_envs, vec_df_path):\n",
    "   \n",
    "    df[['user_id', 'session_30_count_raw']] = df[['user_id', 'session_30_count_raw']].astype(int)\n",
    "   \n",
    "    unique_sessions = df[['user_id', 'session_30_count_raw']].drop_duplicates().sample(frac=1).reset_index(drop=True)\n",
    "    logger.info(f'Unique sessions shape: {unique_sessions.shape}. Splitting into {n_envs} environments')\n",
    "    unique_session_split = np.array_split(unique_sessions, n_envs)\n",
    "    \n",
    "    unique_session_args = [{\n",
    "        'unique_sessions': sess,\n",
    "        'df': df,\n",
    "        'index': i,\n",
    "        'vec_df_path': vec_df_path,\n",
    "    } for i, sess in enumerate(unique_session_split)]\n",
    "\n",
    "    logger.info(f'Environments split: running parralel partitioning')\n",
    "    result = pqdm(unique_session_args, _parralel_partition_users, n_jobs=os.cpu_count() * 4, argument_type='kwargs')\n",
    "    logger.info(f'Environments split: finished parralel partitioning')\n",
    "    return result\n",
    "\n",
    "\n",
    "def reset_intra_session(df):\n",
    "   \n",
    "    logger.info(f'Dropping sessions with less than one event') \n",
    "    \n",
    "    \n",
    "    \n",
    "    logger.info(f'Resetting cum_session_event_count')\n",
    "    df['cum_session_event'] = df.groupby(['user_id', 'session_30_count'])['date_time'].cumcount() + 1\n",
    "    logger.info(f'Resetting cum_session_time and setting reward')\n",
    "    df = df.to_pandas()\n",
    "    df['reward'] = df.groupby(['user_id', 'session_30_count'])['date_time'].diff().dt.total_seconds().fillna(0) / 60\n",
    "    df['reward'] = df[['reward', 'cum_session_event']].apply(lambda x: x['reward'] if x['cum_session_event'] > 1 else 0, axis=1)\n",
    "    df['cum_session_time'] = df.groupby(['user_id', 'session_30_count'])['reward'].cumsum()\n",
    "    \n",
    "    logger.info(f'Resetting cum_platform_time and cum_platform_events')\n",
    "    df['cum_platform_time'] = df.groupby(['user_id'])['reward'].cumsum()\n",
    "    df['cum_platform_event'] = df.groupby(['user_id'])['cum_session_event'].cumcount()\n",
    "    \n",
    "    df = gpu_pd.from_pandas(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_dataset(read_path, conv_path, n_files, window, n_envs):\n",
    "    \n",
    "    conv_path, read_path = (\n",
    "        os.path.join(conv_path, f'files_used_{n_files}'),\n",
    "        os.path.join(read_path, f'files_used_{n_files}', 'predicted_data.parquet')\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(conv_path):\n",
    "        logger.info(f'Creating directory {conv_path}')\n",
    "        os.makedirs(conv_path)\n",
    "    \n",
    "    logger.info(f'Convolutional dataset not found at {conv_path}: creating')\n",
    "    logger.info(f'Getting dataset from {read_path}')\n",
    "    columns = TORCH_LOAD_COLS + ['seq_20' if n_files == 2 else 'seq_40']\n",
    "    df = gpu_pd.read_parquet(read_path, columns=columns)\n",
    "    \n",
    "    df['date_time'] = gpu_pd.to_datetime(df['date_time'])\n",
    "    \n",
    "    df = df.rename(columns={\n",
    "        'continue_work_session_30_minutes' if 'continue_work_session_30_minutes' in df.columns else \\\n",
    "            'session_terminates_30_minutes': 'label',\n",
    "        'seq_40' if 'seq_40' in df.columns else 'seq_20': 'pred'\n",
    "    })\n",
    "    \n",
    "\n",
    "    logger.info(f'Non nan values: {df.count().min()}: 3ropping na')\n",
    "    logger.info(f'NA values dropped: {df.count().min()}')\n",
    "    \n",
    "    df = df.sort_values(by='date_time')\n",
    "\n",
    "   \n",
    "    logger.info(f'Initial shape: {df.shape}: dropping na and inf')\n",
    "    df = df.dropna()\n",
    "    logger.info(f'Final shape: {df.shape}: dropping na and inf')\n",
    "    \n",
    "    \n",
    "        \n",
    "    base_conv_path = os.path.join(conv_path, f'window_{window}')\n",
    "    intermediate_conv_path = os.path.join(base_conv_path, 'intermediate')\n",
    "    if not os.path.exists(base_conv_path):\n",
    "        logger.info(f'Creating directory {base_conv_path}')\n",
    "        os.makedirs(base_conv_path)\n",
    "    \n",
    "    if not os.path.exists(intermediate_conv_path):\n",
    "        logger.info(f'Creating directory {intermediate_conv_path}')\n",
    "        os.makedirs(intermediate_conv_path)\n",
    "        \n",
    "    df = df.sort_values(by='date_time')\n",
    "    logger.info(f'Subset setup complete: {df.shape}')\n",
    "    df = setup_data_at_window(df, window, intermediate_conv_path)\n",
    "    logger.info(f'Subset convolution complete: {df.shape}, resetting stats')\n",
    "    df = df.sort_values(by='date_time')\n",
    "    df = reset_intra_session(df)    \n",
    "    logger.info(f'Intra session stats calculated: {df.shape}, saving intermediate')\n",
    "    df.to_parquet(os.path.join(intermediate_conv_path, 'intra_session.parquet'))\n",
    "    logger.info(f'Intra session reset complete: {df.shape}')\n",
    "    logger.info(f'Stats reset complete, resetting metadata')\n",
    "    df = df.to_pandas()\n",
    "    df = generate_metadata_session(df)\n",
    "    logger.info(f'Metadata reset complete: {df.shape}')\n",
    "        \n",
    "    is_monotic_increasing_sess_time = df.round(3).groupby(['user_id', 'session_30_count'])['cum_session_time'].is_monotonic_increasing.reset_index(name='is_monotic_increasing')\n",
    "        \n",
    "    if is_monotic_increasing_sess_time[is_monotic_increasing_sess_time['is_monotic_increasing'] == False].shape[0] > 0:\n",
    "        logger.info(f'Non monotonic increasing reward found: perc {is_monotic_increasing_sess_time[is_monotic_increasing_sess_time[\"is_monotic_increasing\"] == False].shape[0] / is_monotic_increasing_sess_time.shape[0]}')\n",
    "        logger.info(is_monotic_increasing_sess_time[is_monotic_increasing_sess_time[\"is_monotic_increasing\"] == False])\n",
    "    else:\n",
    "        logger.info(f'All rewards are monotonic increasing and no errors')\n",
    "            \n",
    "        \n",
    "    is_monotic_increasing_date_time = df.round(3).groupby(['user_id'])['date_time'].is_monotonic_increasing.reset_index(name='is_monotic_increasing')\n",
    "        \n",
    "    if is_monotic_increasing_date_time[is_monotic_increasing_date_time['is_monotic_increasing'] == False].shape[0] > 0:\n",
    "        logger.info(f'Non monotonic increasing date time found: perc {is_monotic_increasing_date_time[is_monotic_increasing_date_time[\"is_monotic_increasing\"] == False].shape[0] / is_monotic_increasing_date_time.shape[0]}')\n",
    "        logger.info(is_monotic_increasing_date_time[is_monotic_increasing_date_time[\"is_monotic_increasing\"] == False])\n",
    "    else:\n",
    "        logger.info(f'All date times are monotonic increasing and no errors')\n",
    "        \n",
    "    logger.info(f'Rescaling feature cols: {RESCALER_COLS}')\n",
    "    for col in RESCALER_COLS:\n",
    "        df[f'{col}_raw'] = df[col] \n",
    "    train_split, eval_split =  partition_and_scale_data(df)\n",
    "    \n",
    "    train_path, eval_path = (os.path.join(base_conv_path, 'train.parquet'), os.path.join(base_conv_path, 'eval.parquet'))\n",
    "    logger.info(f'Saving train splits to {train_path}')\n",
    "    train_split.to_parquet(train_path)\n",
    "    logger.info(f'Saving eval splits to {eval_path}')\n",
    "    eval_split.to_parquet(eval_path)\n",
    "    logger.info(f'Saving train and eval splits to complete: setting batches {n_envs}')\n",
    "    batched_train_path, batched_eval_path = (os.path.join(base_conv_path, 'batched_train'), os.path.join(base_conv_path, 'batched_eval'))\n",
    "    \n",
    "    if not os.path.exists(batched_train_path):\n",
    "        logger.info(f'Creating directory {batched_train_path}')\n",
    "        os.makedirs(batched_train_path)\n",
    "    \n",
    "    if not os.path.exists(batched_eval_path):\n",
    "        logger.info(f'Creating directory {batched_eval_path}')\n",
    "        os.makedirs(batched_eval_path)\n",
    "    logger.info(f'Writing training batches to {batched_train_path}')\n",
    "    batch_environments_for_vectorization(train_split, n_envs, batched_train_path)\n",
    "    logger.info(f'Writing eval batches to {batched_eval_path}')\n",
    "    batch_environments_for_vectorization(eval_split, n_envs, batched_eval_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    read_path = 'rl_ready_data'\n",
    "    conv_path = 'rl_ready_data_conv'\n",
    "    n_files = 30\n",
    "    window = 1\n",
    "    n_envs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset(Arguments.read_path, Arguments.conv_path, Arguments.n_files, Arguments.window, Arguments.n_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir rl_ready_data_conv/files_used_10/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
