{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch python-dotenv boto3 --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import boto3\n",
    "from pprint import pformat\n",
    "from torch import nn\n",
    "import io\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = [\n",
    "    \"session_terminates_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \n",
    "    \"cum_session_event_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time_minutes\",\n",
    "]\n",
    "\n",
    "DATE_TIME = [\n",
    "    \"date_time\",\n",
    "]\n",
    "\n",
    "DATE_COLS = [\n",
    "    'year',\n",
    "    'month',\n",
    "    'day',\n",
    "    'hour',\n",
    "    'minute',\n",
    "]\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\",\n",
    "    \"timestamp_raw\",\n",
    "    \"date_hour_sin\",\n",
    "    \n",
    "    \"date_hour_cos\",\n",
    "    \"session_5_count\",\n",
    "    \"session_30_count\",\n",
    "    \n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time_minutes\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time_minutes\",\n",
    "    \"cum_platform_events\",\n",
    "    \n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"rolling_session_time\",\n",
    "    \n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"session_event_count\",\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    'prediction',\n",
    "]\n",
    "\n",
    "METADATA_STAT_COLUMNS = [\n",
    "    'session_size',\n",
    "    'sim_size',\n",
    "    'session_minutes',\n",
    "    'ended',\n",
    "    'incentive_index',\n",
    "    'reward',\n",
    "    'n_episodes',\n",
    "    'time_in_session',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "TORCH_LOAD_COLS = LABEL + METADATA + DATE_TIME + OUT_FEATURE_COLUMNS + ['prediction']\n",
    "\n",
    "OUT_COLUMNS = [\n",
    "    'label',\n",
    "    'user_id',\n",
    "    'session_30',\n",
    "    'cum_session_event_raw',\n",
    "    'cum_session_time_raw',\n",
    "    'glob_platform_event',\n",
    "    'glob_platform_time',\n",
    "    'year',\n",
    "    'month',\n",
    "    'day',\n",
    "    'hour',\n",
    "    'minute',\n",
    "    'prediction'\n",
    "] + OUT_FEATURE_COLUMNS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data_module\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "LABEL_INDEX = 1\n",
    "TOTAL_EVENTS_INDEX = 2\n",
    "BATCHES = 1000000\n",
    "\n",
    "    \n",
    "class ClickstreamDataset(Dataset):\n",
    "    def __init__(self, dataset_pointer_list) -> None:\n",
    "        \"\"\"\n",
    "        Yield data in batches of BATCHES\n",
    "        \"\"\"\n",
    "        self.events = dataset_pointer_list\n",
    "        self.size = self.events[0].shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        events = [np.array([event[idx]]) for event in self.events]\n",
    "        return np.concatenate(events, axis=1)\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load torch_model_bases\n",
    "import torch \n",
    "from torch import nn\n",
    "N_FEATURES = 18\n",
    "class LSTMOrdinal(nn.Module):\n",
    "    def __init__(self,  hidden_size=32, dropout=0.2) -> None:\n",
    "        super(LSTMOrdinal, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=N_FEATURES,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.output = nn.Linear(\n",
    "            hidden_size,\n",
    "            1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1]\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load npz_extractor\n",
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "class NPZExtractor:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, input_path, n_files, n_sequences, s3_client, data_partition) -> None:\n",
    "        self.input_path = input_path\n",
    "        self.n_files = n_files\n",
    "        self.n_sequences = n_sequences\n",
    "        self.s3_client = s3_client\n",
    "        self.data_partition = data_partition\n",
    "\n",
    "\n",
    "    def get_dataset_pointer(self):\n",
    "\n",
    "        read_path = os.path.join(self.input_path, f'files_used_{self.n_files}')\n",
    "        if not os.path.exists(read_path):\n",
    "            self.logger.info(f'Creating directory: {read_path}')\n",
    "            os.makedirs(read_path)\n",
    "\n",
    "\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            key_zip, key_npy = (\n",
    "                os.path.join(read_path, f'sequence_index_{_}.npz'),\n",
    "                os.path.join(read_path, f'sequence_index_{_}')\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f'Loading pointer to dataset: {key_npy}: derived from {key_zip}')\n",
    "\n",
    "\n",
    "            if not os.path.exists(key_zip):\n",
    "                self.logger.info(f'Zip file to extract: {key_zip}: npy file to load: {key_npy}')\n",
    "                self.s3_client.download_file(\n",
    "                    'dissertation-data-dmiller',\n",
    "                    key_zip,\n",
    "                    key_zip\n",
    "                )\n",
    "            if not os.path.exists(key_npy):\n",
    "                self.logger.info(f'Zip file downloaded: {key_zip}: npy file to load: {key_npy}')\n",
    "\n",
    "                self._zip_extract(key_zip, key_npy)\n",
    "\n",
    "        lz_concatenated_results = self._lazy_concatenate()\n",
    "\n",
    "        if self.data_partition:\n",
    "            return [p[:self.data_partition] for p in lz_concatenated_results]\n",
    "        else:\n",
    "            return lz_concatenated_results\n",
    "\n",
    "\n",
    "    def _zip_extract(self, key_zip, key_npy):\n",
    "        self.logger.info(f'Extracting file: {key_zip} -> {key_npy}')\n",
    "\n",
    "        with zipfile.ZipFile(key_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=key_npy, members=['arr_0.npy'])\n",
    "\n",
    "        self.logger.info(f'Zip file exracted: {key_zip} -> {key_npy}/arr_0.npy')\n",
    "\n",
    "    def _lazy_concatenate(self):\n",
    "        lz_concat = []\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            path_to_load = os.path.join(self.input_path, f'files_used_{self.n_files}', f'sequence_index_{_}', f'arr_0.npy')\n",
    "            self.logger.info(f'Loading: {path_to_load}')\n",
    "            lz_concat.append(np.load(path_to_load, mmap_mode='r'))\n",
    "        return lz_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CHECKPOINT_DIR='s3://dissertation-data-dmiller/lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_10/data_partition_None/2023_03_30_07_54'\n",
    "METADATA_INDEX = 12\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger('likelihood_engagement')\n",
    "np.set_printoptions(suppress=True, precision=3, linewidth=200)\n",
    "torch.set_printoptions(sci_mode=False, precision=3)\n",
    "def _extract_features(tensor, n_sequences, n_features):\n",
    "        \n",
    "    metadata, features = tensor[:, :, :METADATA_INDEX], tensor[:, :, METADATA_INDEX:] \n",
    "    metadata, features = metadata.squeeze(), features.squeeze()\n",
    "    metadata, features = metadata.squeeze(), features.squeeze()\n",
    "                \n",
    "    features = torch.flip(\n",
    "        torch.reshape(features, (features.shape[0], n_sequences, n_features)),\n",
    "        dims=[1]\n",
    "    )\n",
    "        \n",
    "    return metadata, features\n",
    "\n",
    "def _extract_last_sequence(tensor):\n",
    "    \"\"\"_summary_\n",
    "    Extracts the last sequence from a tensor of sequences.\n",
    "    \"\"\"\n",
    "    return tensor[:, -1, :]\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_static_predictions(args):\n",
    "    \n",
    "    user_metadata_container = []\n",
    "    \n",
    "    logger.info('Generating static prediction likelihoods for experiment')\n",
    "    npz_extractor = NPZExtractor(\n",
    "        args.file_path,\n",
    "        args.n_files,\n",
    "        args.n_sequences,\n",
    "        None,\n",
    "        None\n",
    "           \n",
    "    )\n",
    "    \n",
    "    logger.info('generating dataset pointer')\n",
    "    dataset = npz_extractor.get_dataset_pointer()\n",
    "    \n",
    "    logger.info('Downloading model checkpoint')\n",
    "    \n",
    "    write_path = os.path.join(args.write_path, f'files_used_{args.n_files}')\n",
    "    if not os.path.exists(write_path):\n",
    "        logger.info(f'Creating directory: {write_path}')\n",
    "        os.makedirs(write_path)\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    checkpoint = client.get_object(\n",
    "        Bucket='dissertation-data-dmiller',\n",
    "        Key='lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_10/data_partition_None/2023_03_30_07_54/clickstream-epoch=83-loss_valid=0.29.ckpt'\n",
    "    )\n",
    "    \n",
    "\n",
    "    logger.info('Loading model checkpoint')\n",
    "    \n",
    "    buffer = io.BytesIO(checkpoint['Body'].read())\n",
    "   \n",
    "    logger.info('checkpoint loaded from buffer. Loading model')\n",
    "    model_state = torch.load(buffer, map_location=torch.device('cuda'))\n",
    "    model = LSTMOrdinal()\n",
    "    model.load_state_dict(model_state['state_dict'])\n",
    "    model = model.cuda()\n",
    "    logger.info(f'Model loaded. Creating dataset: n_events {dataset[0].shape[0]}')\n",
    "    \n",
    "    dataset = ClickstreamDataset(dataset)\n",
    "    loader = DataLoader(dataset, batch_size=8192*32, shuffle=False)\n",
    "    \n",
    "\n",
    "    p_bar = tqdm.tqdm(loader, total=len(loader))\n",
    "    \n",
    "    for indx, data in enumerate(p_bar):\n",
    "        p_bar.set_description(f'Processing batch: {indx}')\n",
    "        data = data.cuda()\n",
    "        metadata, features =_extract_features(data, args.n_sequences + 1, 18)\n",
    "        last_sequence = _extract_last_sequence(features)\n",
    "        preds = model(features)\n",
    "        preds = nn.Sigmoid()(preds)\n",
    "        user_metadata = torch.cat([metadata, preds, last_sequence], dim=1)\n",
    "        user_metadata_container.append(user_metadata)\n",
    "        \n",
    "    predicted_data = torch.tensor(user_metadata).cpu().numpy()\n",
    "    predicted_data = pd.DataFrame(predicted_data, columns=OUT_COLUMNS)\n",
    "    predicted_data = predicted_data.rename(columns={\n",
    "        'prediction': f'pred_{args.model_type}_{args.n_sequences}'\n",
    "    })\n",
    "     \n",
    "    \n",
    "    predicted_data = predicted_data.drop(columns=DATE_COLS)\n",
    "    output_path = os.path.join(write_path, f'{args.model_type}_seq_{args.n_sequences}.parquet')\n",
    "    logger.info(f'Writing rl ready data: {output_path}')\n",
    "    \n",
    "    logger.info(f'Percentage data correct: {predicted_data.count().min() / predicted_data.shape[0]}')\n",
    "    predicted_data.to_parquet(output_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    n_files = 2\n",
    "    n_sequences = 10\n",
    "    file_path = 'torch_ready_data'\n",
    "    checkpoint_dir = CHECKPOINT_DIR\n",
    "    write_path = 'rl_ready_data'\n",
    "    model_type = 'ordinal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 10:49:22,473 INFO Generating static prediction likelihoods for experiment\n",
      "2023-04-26 10:49:22,474 INFO generating dataset pointer\n",
      "2023-04-26 10:49:22,474 INFO Loading pointer to dataset: torch_ready_data/files_used_2/sequence_index_0: derived from torch_ready_data/files_used_2/sequence_index_0.npz\n",
      "2023-04-26 10:49:22,475 INFO Loading pointer to dataset: torch_ready_data/files_used_2/sequence_index_10: derived from torch_ready_data/files_used_2/sequence_index_10.npz\n",
      "2023-04-26 10:49:22,475 INFO Loading: torch_ready_data/files_used_2/sequence_index_0/arr_0.npy\n",
      "2023-04-26 10:49:22,478 INFO Loading: torch_ready_data/files_used_2/sequence_index_10/arr_0.npy\n",
      "2023-04-26 10:49:22,481 INFO Downloading model checkpoint\n",
      "2023-04-26 10:49:23,268 INFO Loading model checkpoint\n",
      "2023-04-26 10:49:23,504 INFO checkpoint loaded from buffer. Loading model\n",
      "2023-04-26 10:49:23,523 INFO Model loaded. Creating dataset: n_events 2566734\n",
      "Processing batch: 9: 100%|██████████| 10/10 [00:29<00:00,  2.93s/it]\n",
      "/tmp/ipykernel_463/3191057956.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predicted_data = torch.tensor(user_metadata).cpu().numpy()\n",
      "2023-04-26 10:49:52,903 INFO Writing rl ready data: rl_ready_data/files_used_2/ordinal_seq_10.parquet\n",
      "2023-04-26 10:49:52,910 INFO Percentage data correct: 1.0\n"
     ]
    }
   ],
   "source": [
    "generate_static_predictions(Arguments())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
