{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install --upgrade awscli python-dotenv --quiet\n",
    "%load_ext dotenv\n",
    "%dotenv env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync rl_ready_data_conv s3://dissertation-data-dmiller/rl_ready_data_conv --delete\n",
    "# !aws s3 sync rl_ready_data s3://dissertation-data-dmiller/rl_ready_data --delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm *.csv *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant\n",
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"cum_session_event_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time\",\n",
    "    \"date_time\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event\",\n",
    "    \"cum_session_time\",\n",
    "    \"cum_session_time\",\n",
    "   \n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"cum_projects\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "RESCALER_COLS = [\n",
    "    'session_30_count',\n",
    "    'session_5_count',\n",
    "    'cum_session_event',\n",
    "    'cum_session_time',\n",
    "    'cum_platform_time',\n",
    "    'cum_platform_event'\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    'seq_40',\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'sim_size',\n",
    "    'session_minutes',\n",
    "    'sim_minutes',\n",
    "    'reward',\n",
    "    'cum_session_time_raw'\n",
    "]\n",
    "\n",
    "TORCH_LOAD_COLS = [\n",
    "    'user_id',\n",
    "    'date_time'\n",
    "] + [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \"expanding_click_average\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "    \"delta_last_event\"\n",
    "] + [\n",
    "    'seq_40',\n",
    "    'session_30_raw',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import cudf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint, pformat\n",
    "import cudf as gpu_pd\n",
    "\n",
    "logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "COLS_FOR_INFLECTION = [\n",
    "    'user_id',\n",
    "    'date_time',\n",
    "] + OUT_FEATURE_COLUMNS + PREDICTION_COLS\n",
    "\n",
    "class SessionCalculate:\n",
    "    logger = logging.getLogger('rl_results_eval')\n",
    "    def __init__(self, df, use_gpu) -> None:\n",
    "        self.df = df\n",
    "        self.use_gpu = use_gpu\n",
    "            \n",
    "        \n",
    "    def calculate_inflections(self):\n",
    "      \n",
    "        self.logger.info('Calculating subsequent date time')\n",
    "        self.df['next_date_time'] = self.df.groupby('user_id')['date_time'].shift(-1)\n",
    "        self.df = self.df.drop_duplicates(subset=['user_id', 'date_time'], keep='last').reset_index()\n",
    "        if self.use_gpu:\n",
    "            self.logger.info('Bringing to CPU for second calculation')\n",
    "            self.df = self.df.to_pandas()\n",
    "           \n",
    "            \n",
    "        self.df['diff_seconds'] = (self.df['next_date_time'] - self.df['date_time']).apply(lambda x: x.total_seconds())\n",
    "        \n",
    "        self.logger.info('Diff seconds calculated')\n",
    "        if self.use_gpu:\n",
    "            self.logger.info('Bringing back to GPU for final calculations')\n",
    "            self.df = cudf.from_pandas(self.df)\n",
    "\n",
    "        self.df['diff_minutes'] = (self.df['diff_seconds'] / 60)\n",
    "        self.df['session_5'] = (self.df['diff_minutes'] < 5)\n",
    "        self.df['session_30'] = self.df['diff_minutes'] < 30\n",
    "        \n",
    "        self.df['session_30'] = self.df['session_30'].fillna(False)\n",
    "        self.df['session_5'] = self.df['session_5'].fillna(False)        \n",
    "        self.logger.info(f'Labels calculated: removing rows with diff seconds > 0')\n",
    "       \n",
    "        \n",
    "\n",
    "        self.logger.info(f'Number of rows following drop: {self.df.shape[0]}')\n",
    "        self.logger.info(f'Sorting rows by date time and applying row count')\n",
    "        self.df = self.df.sort_values(['date_time']).reset_index()\n",
    "        self.df['row_count'] = self.df.index.values\n",
    "        self.logger.info(f'Sorted rows and applied row count on updated index')  \n",
    "        self.logger.info('Calculating inflection points')\n",
    "        self.df['user_id'] = self.df['user_id'].astype('int32')\n",
    "        \n",
    "       \n",
    "        inflections_5_merge = self.df[self.df['session_5'] == False].sort_values(by=['date_time'])\n",
    "        inflections_30_merge = self.df[self.df['session_30'] == False].sort_values(by=['date_time']) \n",
    "     \n",
    "        self.logger.info('Calculating session 5 inflections') \n",
    "        inflections_5_merge['session_5'] = inflections_5_merge.groupby('user_id').cumcount() + 1\n",
    "        inflections_5_merge = inflections_5_merge.rename(columns={'session_5': 'session_5_count'})\n",
    "        \n",
    "        self.logger.info('Calculating session 30 inflections')\n",
    "        inflections_30_merge['session_30'] = inflections_30_merge.groupby('user_id').cumcount() + 1\n",
    "        inflections_30_merge = inflections_30_merge.rename(columns={'session_30': 'session_30_count'})\n",
    "        \n",
    "        inflections_5_merge = inflections_5_merge[['user_id', 'date_time', 'row_count', 'session_5_count']].sort_values(by=['row_count', 'user_id'])\n",
    "        inflections_30_merge = inflections_30_merge[['user_id', 'date_time', 'row_count', 'session_30_count']].sort_values(by=['row_count', 'user_id'])\n",
    "        inflections_5_merge = inflections_5_merge.drop(columns=['date_time'])\n",
    "        \n",
    "        inflections_30_merge = inflections_30_merge.rename(columns={'date_time': 'session_end_time'})\n",
    "\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            self.logger.info('Bringing back to GPU for labelling')\n",
    "            self.df, inflections_5_merge, inflections_30_merge = self.df.to_pandas(), inflections_5_merge.to_pandas(), inflections_30_merge.to_pandas()\n",
    "            self.df = self.df.sort_values(by=['row_count', 'user_id'])\n",
    "            self.df = pd.merge_asof(self.df, inflections_5_merge, on='row_count', by='user_id', direction='forward')\n",
    "            self.df = pd.merge_asof(self.df, inflections_30_merge, on='row_count', by='user_id', direction='forward')\n",
    "            self.df['session_terminates_30_minutes'] = (self.df['session_end_time'] - self.df['date_time']).apply(lambda x: x.total_seconds() / 60) < 30\n",
    "            self.df = cudf.from_pandas(self.df)\n",
    "        else:\n",
    "            self.logger.info('Labelling on CPU')\n",
    "            self.df = pd.merge_asof(self.df.sort_values(by=['row_count', 'user_id']), inflections_5_merge, on='row_count', by='user_id', direction='forward')\n",
    "\n",
    "            self.df = pd.merge_asof(self.df.sort_values(by=['row_count', 'user_id']), inflections_5_merge, on='row_count', by='user_id', direction='forward')\n",
    "            self.df = pd.merge_asof(self.df.sort_values(by=['row_count', 'user_id']), inflections_30_merge, on='row_count', by='user_id', direction='forward') \n",
    "            self.df['session_terminates_30_minutes'] = (self.df['session_end_time'] - self.df['date_time']).apply(lambda x: x.total_seconds() / 60) < 30\n",
    " \n",
    "        self.logger.info('Inflections calculated')\n",
    " \n",
    "        session_end_30_minutes = self.df[self.df['session_terminates_30_minutes'] == False].shape[0]\n",
    "        self.logger.info(f'Percent sessions end in 30 minutes: {session_end_30_minutes / self.df.shape[0]}')\n",
    "        self.logger.info(f'Columns for df') \n",
    "        self.logger.info(pformat(self.df.columns))\n",
    "        \n",
    "        return self.df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "global logger\n",
    "logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('rl_results_eval')\n",
    "from functools import reduce\n",
    "from pprint import pformat\n",
    "import cudf as gpu_pd\n",
    "import cupy as gpu_np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('mode.use_inf_as_na', True)\n",
    "from cuml.preprocessing import MinMaxScaler\n",
    "\n",
    "def convolve_delta_events(df, window, write_path):\n",
    "    \n",
    "    df = df.to_pandas()\n",
    "   \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    before_resample = df.shape\n",
    "    logger.info(f'Convolutional shape before resample: {before_resample}')\n",
    "    logger.info(f'Convolution over delta last event')\n",
    "    \n",
    "    df['delta_last_event'] = (\n",
    "        df.sort_values(by=['session_30_raw', 'cum_session_event_raw']) \\\n",
    "            .set_index('date_time') \\\n",
    "            .groupby(by=['user_id', 'session_30_raw'], group_keys=False) \\\n",
    "            .rolling(f'{window}T', min_periods=1)['delta_last_event'] \\\n",
    "            .mean()\n",
    "            .reset_index(name='convolved_event_delta')['convolved_event_delta'] \\\n",
    "    ) \n",
    "\n",
    "    df = df.drop(columns=['session_30_raw'])\n",
    "    df = df.loc[:,~df.columns.duplicated()].reset_index(drop=True)  \n",
    "    \n",
    "\n",
    "    \n",
    "    df = gpu_pd.from_pandas(df)\n",
    "  \n",
    "    # remove duplicate columns\n",
    "    df['year'] = df['date_time'].dt.year\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['minute'] = df['date_time'].dt.minute\n",
    "    df['second'] = df['date_time'].dt.second\n",
    "    \n",
    "    df['user_id'] = df['user_id'].astype('int32')\n",
    "    \n",
    "   \n",
    "    resampled_df = df.sort_values(by='date_time') \\\n",
    "        .drop_duplicates(subset=['user_id', 'year', 'month', 'day', 'hour', 'minute'], keep='last') \\\n",
    "        .sort_values(by=['date_time']) \\\n",
    "        .reset_index()\n",
    "    \n",
    "    logger.info(f'Convolution complete: {before_resample} -> {resampled_df.shape}')\n",
    "    logger.info(f'Writing intermediate results to {write_path}_convolve.parquet')\n",
    "    \n",
    "    resampled_df.to_parquet(write_path + '_convolve.parquet')\n",
    "    \n",
    "    logger.info(f'Recalculating inflections')\n",
    "    resample_events = SessionCalculate(resampled_df, use_gpu=True)\n",
    "    resampled_event_out = resample_events.calculate_inflections()\n",
    "   \n",
    "    logger.info(f'Events sessionized: writing to {write_path}_session.parquet')\n",
    "    resampled_event_out.to_parquet(write_path + '_session.parquet')\n",
    "    logger.info(f'Events resampled')\n",
    "    \n",
    "    return resampled_event_out\n",
    "     \n",
    "     \n",
    "\n",
    "def generate_metadata_session(dataset):\n",
    "    \n",
    "    logger.info(f'Calculating session size and minutes')\n",
    "    session_size = dataset.groupby(['user_id', 'session_30_count'])['cum_session_event'].max().reset_index(name='session_size')\n",
    "    session_minutes = dataset.groupby(['user_id', 'session_30_count'])['cum_session_time'].max().reset_index(name='session_minutes')\n",
    "    \n",
    "    \n",
    "    logger.info(f'Calculating sim size and minutes')\n",
    "    sim_minutes = dataset.groupby(['user_id', 'session_30_count'])['cum_session_time'].quantile(.7, interpolation='nearest').reset_index(name='sim_minutes')\n",
    "    sim_size = dataset.groupby(['user_id', 'session_30_count'])['cum_session_event'].quantile(.7, interpolation='nearest').reset_index(name='sim_size')\n",
    "    \n",
    "    \n",
    "    sessions = [session_size, session_minutes, sim_minutes, sim_size]\n",
    "    logger.info(f'Merging metadata')\n",
    "    sessions = reduce(lambda left, right: pd.merge(left, right, on=['user_id', 'session_30_count']), sessions)\n",
    "    \n",
    "    logger.info(f'Merging metadata complete')\n",
    "    dataset = pd.merge(dataset, sessions, on=['user_id', 'session_30_count'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def setup_data_at_window(df, window, write_path):\n",
    "    logger.info(f'Convolution over {window} minute window')\n",
    "    df = df.sort_values(by='date_time')\n",
    "    df['cum_session_event_raw'] = df.groupby(['user_id', 'session_30_raw'])['date_time'].cumcount() + 1\n",
    "    df = df.sort_values(by='date_time').reset_index(drop=True)\n",
    "    df  = convolve_delta_events(df, window, write_path)\n",
    "    logger.info(f'Convolving over {window} minute window complete: generating metadata')\n",
    "    logger.info(f'Generating metadata complete')\n",
    "    return df\n",
    "\n",
    "\n",
    "def reset_intra_session(subset_df):\n",
    "   \n",
    "    logger.info(f'Dropping sessions with less than one event') \n",
    "    \n",
    "    \n",
    "    \n",
    "    logger.info(f'Resetting cum_session_event_count')\n",
    "    subset_df['cum_session_event'] = subset_df.groupby(['user_id', 'session_30_count'])['date_time'].cumcount() + 1\n",
    "    logger.info(f'Resetting cum_session_time and setting reward')\n",
    "    subset_df = subset_df.to_pandas()\n",
    "    subset_df['reward'] = subset_df.groupby(['user_id', 'session_30_count'])['date_time'].diff().dt.total_seconds().fillna(0) / 60\n",
    "    subset_df['reward'] = subset_df[['reward', 'cum_session_event']].apply(lambda x: x['reward'] if x['cum_session_event'] > 1 else 0, axis=1)\n",
    "    subset_df['cum_session_time'] = subset_df.groupby(['user_id', 'session_30_count'])['reward'].cumsum()\n",
    "    \n",
    "    logger.info(f'Resetting cum_platform_time and cum_platform_events')\n",
    "    subset_df['cum_platform_time'] = subset_df.groupby(['user_id'])['reward'].cumsum()\n",
    "    subset_df['cum_platform_event'] = subset_df.groupby(['user_id'])['cum_session_event'].cumcount()\n",
    "    \n",
    "    subset_df = gpu_pd.from_pandas(subset_df)\n",
    "    \n",
    "    return subset_df\n",
    "\n",
    "def get_dataset(read_path, conv_path, n_files, window):\n",
    "    \n",
    "    conv_path, read_path = (\n",
    "        os.path.join(conv_path, f'files_used_{n_files}'),\n",
    "        os.path.join(read_path, f'files_used_{n_files}', 'predicted_data.parquet')\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(conv_path):\n",
    "        logger.info(f'Creating directory {conv_path}')\n",
    "        os.makedirs(conv_path)\n",
    "    \n",
    "    logger.info(f'Convolutional dataset not found at {conv_path}: creating')\n",
    "    logger.info(f'Getting dataset from {read_path}')\n",
    "    df = gpu_pd.read_parquet(read_path, columns=TORCH_LOAD_COLS)\n",
    "    \n",
    "    df['date_time'] = gpu_pd.to_datetime(df['date_time'])\n",
    "    logger.info(f'Non nan values: {df.count().min()}: 3ropping na')\n",
    "    logger.info(f'NA values dropped: {df.count().min()}')\n",
    "    \n",
    "    df = df.sort_values(by='date_time')\n",
    "    subsets = [\n",
    "        { \"name\": \"train\", \"start\": 0, \"end\": int(df.shape[0] * .7) },\n",
    "        { \"name\": \"eval\", \"start\": int(df.shape[0] * .7), \"end\": int(df.shape[0]) }\n",
    "    ]\n",
    "\n",
    "   \n",
    "    logger.info(f'Initial shape: {df.shape}: dropping na and inf')\n",
    "    df = df.dropna()\n",
    "    logger.info(f'Final shape: {df.shape}: dropping na and inf')\n",
    "    \n",
    "    for s in subsets:\n",
    "        \n",
    "        base_conv_path = os.path.join(conv_path, f'window_{window}_{s[\"name\"]}')\n",
    "        updated_conv_path = os.path.join(conv_path, f\"window_{window}_{s['name']}_final.parquet\")\n",
    "        logger.info(f'Running conv on {s[\"name\"]} subset')\n",
    "        subset_df = df[s['start']:s['end']].copy()\n",
    "        subset_df = subset_df.sort_values(by='date_time')\n",
    "        logger.info(f'Subset setup complete: {subset_df.shape}')\n",
    "        subset_df = setup_data_at_window(subset_df, window, base_conv_path)\n",
    "        logger.info(f'Subset convolution complete: {subset_df.shape}, resetting stats')\n",
    "        subset_df = subset_df.sort_values(by='date_time')\n",
    "        subset_df = reset_intra_session(subset_df)    \n",
    "        logger.info(f'Intra session stats calculated: {subset_df.shape}, saving intermediate')\n",
    "        subset_df.to_parquet(updated_conv_path + '_intra_session.parquet')\n",
    "        logger.info(f'Intra session reset complete: {subset_df.shape}')\n",
    "        logger.info(f'Stats reset complete, resetting metadata')\n",
    "        subset_df = subset_df.to_pandas()\n",
    "        subset_df = generate_metadata_session(subset_df)\n",
    "        logger.info(f'Metadata reset complete: {subset_df.shape}')\n",
    "        logger.info(f'Saving convolutional dataset to {updated_conv_path}')\n",
    "        \n",
    "        is_monotic_increasing_sess_time = subset_df.round(3).groupby(['user_id', 'session_30_count'])['cum_session_time'].is_monotonic_increasing.reset_index(name='is_monotic_increasing')\n",
    "        \n",
    "        if is_monotic_increasing_sess_time[is_monotic_increasing_sess_time['is_monotic_increasing'] == False].shape[0] > 0:\n",
    "            logger.info(f'Non monotonic increasing reward found: perc {is_monotic_increasing_sess_time[is_monotic_increasing_sess_time[\"is_monotic_increasing\"] == False].shape[0] / is_monotic_increasing_sess_time.shape[0]}')\n",
    "            logger.info(is_monotic_increasing_sess_time[is_monotic_increasing_sess_time[\"is_monotic_increasing\"] == False])\n",
    "        else:\n",
    "            logger.info(f'All rewards are monotonic increasing and no errors')\n",
    "            \n",
    "        \n",
    "        is_monotic_increasing_date_time = subset_df.round(3).groupby(['user_id'])['date_time'].is_monotonic_increasing.reset_index(name='is_monotic_increasing')\n",
    "        \n",
    "        if is_monotic_increasing_date_time[is_monotic_increasing_date_time['is_monotic_increasing'] == False].shape[0] > 0:\n",
    "            logger.info(f'Non monotonic increasing date time found: perc {is_monotic_increasing_date_time[is_monotic_increasing_date_time[\"is_monotic_increasing\"] == False].shape[0] / is_monotic_increasing_date_time.shape[0]}')\n",
    "            logger.info(is_monotic_increasing_date_time[is_monotic_increasing_date_time[\"is_monotic_increasing\"] == False])\n",
    "        else:\n",
    "            logger.info(f'All date times are monotonic increasing and no errors')\n",
    "       \n",
    "        logger.info(f'Rescaling feature cols: {RESCALER_COLS}')\n",
    "        \n",
    "        for col in RESCALER_COLS:\n",
    "            subset_df[f'{col}_raw'] = subset_df[col]\n",
    "        subset_df[RESCALER_COLS] = MinMaxScaler(feature_range=(-1, 1)).fit_transform(subset_df[RESCALER_COLS])\n",
    "       \n",
    "        logger.info(f'Writing to disk: {subset_df.shape}') \n",
    "        subset_df.to_parquet(updated_conv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    read_path = 'rl_ready_data'\n",
    "    conv_path = 'rl_ready_data_conv'\n",
    "    n_files = 30\n",
    "    window = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 10:39:55,145 - rl_results_eval - INFO - Convolutional dataset not found at rl_ready_data_conv/files_used_30: creating\n",
      "2023-06-03 10:39:55,146 - rl_results_eval - INFO - Getting dataset from rl_ready_data/files_used_30/predicted_data.parquet\n",
      "2023-06-03 10:39:58,210 - rl_results_eval - INFO - Non nan values: 36241442: 3ropping na\n",
      "2023-06-03 10:39:58,212 - rl_results_eval - INFO - NA values dropped: 36241442\n",
      "2023-06-03 10:39:58,335 - rl_results_eval - INFO - Initial shape: (36241442, 18): dropping na and inf\n",
      "2023-06-03 10:39:58,431 - rl_results_eval - INFO - Final shape: (36241442, 18): dropping na and inf\n",
      "2023-06-03 10:39:58,431 - rl_results_eval - INFO - Running conv on train subset\n",
      "2023-06-03 10:39:58,550 - rl_results_eval - INFO - Subset setup complete: (25369009, 18)\n",
      "2023-06-03 10:39:58,551 - rl_results_eval - INFO - Convolution over 1 minute window\n",
      "2023-06-03 10:40:05,397 - rl_results_eval - INFO - Convolutional shape before resample: (25369009, 19)\n",
      "2023-06-03 10:40:05,398 - rl_results_eval - INFO - Convolution over delta last event\n",
      "2023-06-03 10:40:42,506 - rl_results_eval - INFO - Convolution complete: (25369009, 19) -> (6200623, 25)\n",
      "2023-06-03 10:40:42,507 - rl_results_eval - INFO - Writing intermediate results to rl_ready_data_conv/files_used_30/window_1_train_convolve.parquet\n",
      "2023-06-03 10:40:43,170 - rl_results_eval - INFO - Recalculating inflections\n",
      "2023-06-03 10:40:43,171 - rl_results_eval - INFO - Calculating subsequent date time\n",
      "2023-06-03 10:40:43,252 - rl_results_eval - INFO - Bringing to CPU for second calculation\n",
      "2023-06-03 10:40:54,942 - rl_results_eval - INFO - Diff seconds calculated\n",
      "2023-06-03 10:40:54,943 - rl_results_eval - INFO - Bringing back to GPU for final calculations\n",
      "2023-06-03 10:40:55,289 - rl_results_eval - INFO - Labels calculated: removing rows with diff seconds > 0\n",
      "2023-06-03 10:40:55,290 - rl_results_eval - INFO - Number of rows following drop: 6200623\n",
      "2023-06-03 10:40:55,290 - rl_results_eval - INFO - Sorting rows by date time and applying row count\n",
      "2023-06-03 10:40:55,338 - rl_results_eval - INFO - Sorted rows and applied row count on updated index\n",
      "2023-06-03 10:40:55,338 - rl_results_eval - INFO - Calculating inflection points\n",
      "2023-06-03 10:40:55,374 - rl_results_eval - INFO - Calculating session 5 inflections\n",
      "2023-06-03 10:40:55,393 - rl_results_eval - INFO - Calculating session 30 inflections\n",
      "2023-06-03 10:40:55,423 - rl_results_eval - INFO - Bringing back to GPU for labelling\n",
      "2023-06-03 10:41:13,879 - rl_results_eval - INFO - Inflections calculated\n",
      "2023-06-03 10:41:13,905 - rl_results_eval - INFO - Percent sessions end in 30 minutes: 0.4231007110092002\n",
      "2023-06-03 10:41:13,906 - rl_results_eval - INFO - Columns for df\n",
      "2023-06-03 10:41:13,907 - rl_results_eval - INFO - Index(['level_0', 'index', 'user_id', 'date_time', 'country_count',\n",
      "       'date_hour_sin', 'date_hour_cos', 'date_minute_sin', 'date_minute_cos',\n",
      "       'expanding_click_average', 'cum_projects', 'average_event_time',\n",
      "       'rolling_session_time', 'rolling_session_events', 'rolling_session_gap',\n",
      "       'previous_session_time', 'previous_session_events', 'delta_last_event',\n",
      "       'seq_40', 'cum_session_event_raw', 'year', 'month', 'day', 'hour',\n",
      "       'minute', 'second', 'next_date_time', 'diff_seconds', 'diff_minutes',\n",
      "       'session_5', 'session_30', 'row_count', 'session_5_count',\n",
      "       'session_end_time', 'session_30_count',\n",
      "       'session_terminates_30_minutes'],\n",
      "      dtype='object')\n",
      "2023-06-03 10:41:13,907 - rl_results_eval - INFO - Events sessionized: writing to rl_ready_data_conv/files_used_30/window_1_train_session.parquet\n",
      "2023-06-03 10:41:14,798 - rl_results_eval - INFO - Events resampled\n",
      "2023-06-03 10:41:14,799 - rl_results_eval - INFO - ['\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", '(', ')', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '0', '0', '0', '0', '0', '3', '3', '3', '4', '5', '5', '=', 'I', '[', ']', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'f', 'f', 'f', 'f', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'h', 'h', 'h', 'h', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'j', 'j', 'k', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'p', 'p', 'p', 'p', 'p', 'p', 'q', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'w', 'w', 'x', 'x', 'x', 'x', 'y', 'y', 'y', 'y']\n",
      "2023-06-03 10:41:14,878 - rl_results_eval - INFO - Convolving over 1 minute window complete: generating metadata\n",
      "2023-06-03 10:41:14,879 - rl_results_eval - INFO - Generating metadata complete\n",
      "2023-06-03 10:41:14,903 - rl_results_eval - INFO - Subset convolution complete: (6200623, 36), resetting stats\n",
      "2023-06-03 10:41:14,992 - rl_results_eval - INFO - Dropping sessions with less than one event\n",
      "2023-06-03 10:41:14,993 - rl_results_eval - INFO - Resetting cum_session_event_count\n",
      "2023-06-03 10:41:15,051 - rl_results_eval - INFO - Resetting cum_session_time and setting reward\n",
      "2023-06-03 10:41:49,942 - rl_results_eval - INFO - Resetting cum_platform_time and cum_platform_events\n",
      "2023-06-03 10:41:50,954 - rl_results_eval - INFO - Intra session stats calculated: (6200623, 41), saving intermediate\n",
      "2023-06-03 10:41:51,840 - rl_results_eval - INFO - Intra session reset complete: (6200623, 41)\n",
      "2023-06-03 10:41:51,841 - rl_results_eval - INFO - Stats reset complete, resetting metadata\n",
      "2023-06-03 10:41:53,739 - rl_results_eval - INFO - Calculating session size and minutes\n",
      "2023-06-03 10:41:54,268 - rl_results_eval - INFO - Calculating sim size and minutes\n",
      "2023-06-03 10:41:58,364 - rl_results_eval - INFO - Merging metadata\n",
      "2023-06-03 10:41:58,585 - rl_results_eval - INFO - Merging metadata complete\n",
      "2023-06-03 10:42:00,191 - rl_results_eval - INFO - Metadata reset complete: (6200623, 45)\n",
      "2023-06-03 10:42:00,192 - rl_results_eval - INFO - Saving convolutional dataset to rl_ready_data_conv/files_used_30/window_1_train_final.parquet\n",
      "2023-06-03 10:42:16,952 - rl_results_eval - INFO - All rewards are monotonic increasing and no errors\n",
      "2023-06-03 10:42:23,722 - rl_results_eval - INFO - All date times are monotonic increasing and no errors\n",
      "2023-06-03 10:42:23,723 - rl_results_eval - INFO - Rescaling feature cols: ['session_30_count', 'session_5_count', 'cum_session_event', 'cum_session_time', 'cum_platform_time', 'cum_platform_event']\n",
      "2023-06-03 10:42:26,557 - rl_results_eval - INFO - Writing to disk: (6200623, 50)\n",
      "2023-06-03 10:42:31,826 - rl_results_eval - INFO - Running conv on eval subset\n",
      "2023-06-03 10:42:31,888 - rl_results_eval - INFO - Subset setup complete: (10872433, 18)\n",
      "2023-06-03 10:42:31,889 - rl_results_eval - INFO - Convolution over 1 minute window\n",
      "2023-06-03 10:42:34,594 - rl_results_eval - INFO - Convolutional shape before resample: (10872433, 19)\n",
      "2023-06-03 10:42:34,595 - rl_results_eval - INFO - Convolution over delta last event\n",
      "2023-06-03 10:42:49,883 - rl_results_eval - INFO - Convolution complete: (10872433, 19) -> (2667843, 25)\n",
      "2023-06-03 10:42:49,884 - rl_results_eval - INFO - Writing intermediate results to rl_ready_data_conv/files_used_30/window_1_eval_convolve.parquet\n",
      "2023-06-03 10:42:50,335 - rl_results_eval - INFO - Recalculating inflections\n",
      "2023-06-03 10:42:50,335 - rl_results_eval - INFO - Calculating subsequent date time\n",
      "2023-06-03 10:42:50,375 - rl_results_eval - INFO - Bringing to CPU for second calculation\n",
      "2023-06-03 10:42:54,944 - rl_results_eval - INFO - Diff seconds calculated\n",
      "2023-06-03 10:42:54,945 - rl_results_eval - INFO - Bringing back to GPU for final calculations\n",
      "2023-06-03 10:42:55,109 - rl_results_eval - INFO - Labels calculated: removing rows with diff seconds > 0\n",
      "2023-06-03 10:42:55,110 - rl_results_eval - INFO - Number of rows following drop: 2667843\n",
      "2023-06-03 10:42:55,110 - rl_results_eval - INFO - Sorting rows by date time and applying row count\n",
      "2023-06-03 10:42:55,136 - rl_results_eval - INFO - Sorted rows and applied row count on updated index\n",
      "2023-06-03 10:42:55,137 - rl_results_eval - INFO - Calculating inflection points\n",
      "2023-06-03 10:42:55,162 - rl_results_eval - INFO - Calculating session 5 inflections\n",
      "2023-06-03 10:42:55,174 - rl_results_eval - INFO - Calculating session 30 inflections\n",
      "2023-06-03 10:42:55,190 - rl_results_eval - INFO - Bringing back to GPU for labelling\n",
      "2023-06-03 10:43:02,856 - rl_results_eval - INFO - Inflections calculated\n",
      "2023-06-03 10:43:02,872 - rl_results_eval - INFO - Percent sessions end in 30 minutes: 0.47488401678809433\n",
      "2023-06-03 10:43:02,873 - rl_results_eval - INFO - Columns for df\n",
      "2023-06-03 10:43:02,874 - rl_results_eval - INFO - Index(['level_0', 'index', 'user_id', 'date_time', 'country_count',\n",
      "       'date_hour_sin', 'date_hour_cos', 'date_minute_sin', 'date_minute_cos',\n",
      "       'expanding_click_average', 'cum_projects', 'average_event_time',\n",
      "       'rolling_session_time', 'rolling_session_events', 'rolling_session_gap',\n",
      "       'previous_session_time', 'previous_session_events', 'delta_last_event',\n",
      "       'seq_40', 'cum_session_event_raw', 'year', 'month', 'day', 'hour',\n",
      "       'minute', 'second', 'next_date_time', 'diff_seconds', 'diff_minutes',\n",
      "       'session_5', 'session_30', 'row_count', 'session_5_count',\n",
      "       'session_end_time', 'session_30_count',\n",
      "       'session_terminates_30_minutes'],\n",
      "      dtype='object')\n",
      "2023-06-03 10:43:02,874 - rl_results_eval - INFO - Events sessionized: writing to rl_ready_data_conv/files_used_30/window_1_eval_session.parquet\n",
      "2023-06-03 10:43:03,366 - rl_results_eval - INFO - Events resampled\n",
      "2023-06-03 10:43:03,368 - rl_results_eval - INFO - ['\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", '(', ')', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '0', '0', '0', '0', '0', '3', '3', '3', '4', '5', '5', '=', 'I', '[', ']', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'f', 'f', 'f', 'f', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'h', 'h', 'h', 'h', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'j', 'j', 'k', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'p', 'p', 'p', 'p', 'p', 'p', 'q', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'v', 'w', 'w', 'x', 'x', 'x', 'x', 'y', 'y', 'y', 'y']\n",
      "2023-06-03 10:43:03,422 - rl_results_eval - INFO - Convolving over 1 minute window complete: generating metadata\n",
      "2023-06-03 10:43:03,423 - rl_results_eval - INFO - Generating metadata complete\n",
      "2023-06-03 10:43:03,444 - rl_results_eval - INFO - Subset convolution complete: (2667843, 36), resetting stats\n",
      "2023-06-03 10:43:03,471 - rl_results_eval - INFO - Dropping sessions with less than one event\n",
      "2023-06-03 10:43:03,472 - rl_results_eval - INFO - Resetting cum_session_event_count\n",
      "2023-06-03 10:43:03,501 - rl_results_eval - INFO - Resetting cum_session_time and setting reward\n",
      "2023-06-03 10:43:18,161 - rl_results_eval - INFO - Resetting cum_platform_time and cum_platform_events\n",
      "2023-06-03 10:43:18,578 - rl_results_eval - INFO - Intra session stats calculated: (2667843, 41), saving intermediate\n",
      "2023-06-03 10:43:19,111 - rl_results_eval - INFO - Intra session reset complete: (2667843, 41)\n",
      "2023-06-03 10:43:19,112 - rl_results_eval - INFO - Stats reset complete, resetting metadata\n",
      "2023-06-03 10:43:19,824 - rl_results_eval - INFO - Calculating session size and minutes\n",
      "2023-06-03 10:43:20,026 - rl_results_eval - INFO - Calculating sim size and minutes\n",
      "2023-06-03 10:43:21,439 - rl_results_eval - INFO - Merging metadata\n",
      "2023-06-03 10:43:21,517 - rl_results_eval - INFO - Merging metadata complete\n",
      "2023-06-03 10:43:22,116 - rl_results_eval - INFO - Metadata reset complete: (2667843, 45)\n",
      "2023-06-03 10:43:22,117 - rl_results_eval - INFO - Saving convolutional dataset to rl_ready_data_conv/files_used_30/window_1_eval_final.parquet\n",
      "2023-06-03 10:43:28,459 - rl_results_eval - INFO - All rewards are monotonic increasing and no errors\n",
      "2023-06-03 10:43:31,023 - rl_results_eval - INFO - All date times are monotonic increasing and no errors\n",
      "2023-06-03 10:43:31,024 - rl_results_eval - INFO - Rescaling feature cols: ['session_30_count', 'session_5_count', 'cum_session_event', 'cum_session_time', 'cum_platform_time', 'cum_platform_event']\n",
      "2023-06-03 10:43:31,696 - rl_results_eval - INFO - Writing to disk: (2667843, 50)\n"
     ]
    }
   ],
   "source": [
    "get_dataset(Arguments.read_path, Arguments.conv_path, Arguments.n_files, Arguments.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r rl_ready_data/files_used_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://dissertation-data-dmiller/rl_ready_data/files_used_10/predicted_data.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync rl_ready_data s3://dissertation-data-dmiller/rl_ready_data --delete\n",
    "!aws s3 sync rl_ready_data_conv s3://dissertation-data-dmiller/rl_ready_data_conv --delete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
