{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 22.10.1+2.gca9a422da9 requires cupy-cuda115<12.0.0a0,>=9.5.0, which is not installed.\n",
      "cudf 22.10.1+2.gca9a422da9 requires cupy-cuda115<12.0.0a0,>=9.5.0, which is not installed.\n",
      "cudf 22.10.1+2.gca9a422da9 requires cuda-python<11.7.1,>=11.5, but you have cuda-python 11.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch --quiet\n",
    "!python -m pip install gym stable-baselines3[extra] python-dotenv fsspec[\"s3\"] boto3 s3fs==2022.11.0 tensorboard --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load npz_extractor.py\n",
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import logging \n",
    "\n",
    "class NPZExtractor:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, input_path, n_files, n_sequences, s3_client, data_partition) -> None:\n",
    "        self.input_path = input_path\n",
    "        self.n_files = n_files\n",
    "        self.n_sequences = n_sequences\n",
    "        self.s3_client = s3_client\n",
    "        self.data_partition = data_partition\n",
    "\n",
    "\n",
    "    def get_dataset_pointer(self):\n",
    "\n",
    "        read_path = os.path.join(self.input_path, f'files_used_{self.n_files}')\n",
    "        if not os.path.exists(read_path):\n",
    "            print(f'Creating directory: {read_path}')\n",
    "            os.makedirs(read_path)\n",
    "\n",
    "\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            key_zip, key_npy = (\n",
    "                os.path.join(read_path, f'sequence_index_{_}.npz'),\n",
    "                os.path.join(read_path, f'sequence_index_{_}')\n",
    "            )\n",
    "\n",
    "            self.logger.info(f'Loading pointer to dataset: {key_npy}: derived from {key_zip}')\n",
    "\n",
    "            if not os.path.exists(key_npy):\n",
    "                self.logger.info(f'Zip file to extract: {key_zip}: npy file to load: {key_npy}')\n",
    "                # self.s3_client.download_file(\n",
    "                #     'dissertation-data-dmiller',\n",
    "                #     key_zip,\n",
    "                #     key_zip\n",
    "                # )\n",
    "                self.logger.info(f'Zip file downloaded: {key_zip}')\n",
    "                self._zip_extract(key_zip, key_npy)\n",
    "\n",
    "        lz_concatenated_results = self._lazy_concatenate()\n",
    "\n",
    "        if self.data_partition:\n",
    "            return [p[:self.data_partition] for p in lz_concatenated_results]\n",
    "        else:\n",
    "            return lz_concatenated_results\n",
    "\n",
    "\n",
    "    def _zip_extract(self, key_zip, key_npy):\n",
    "        self.logger.info(f'Extracting file: {key_zip} -> {key_npy}')\n",
    "\n",
    "        with zipfile.ZipFile(key_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=key_npy, members=['arr_0.npy'])\n",
    "\n",
    "\n",
    "        self.logger.info(f'Zip file exracted: {key_zip} -> {key_npy}/arr_0.npy')\n",
    "\n",
    "    def _lazy_concatenate(self):\n",
    "        lz_concat = []\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            path_to_load = os.path.join(self.input_path, f'files_used_{self.n_files}', f'sequence_index_{_}', f'arr_0.npy')\n",
    "            self.logger.info(f'Loading: {path_to_load}')\n",
    "            lz_concat.append(np.load(path_to_load))\n",
    "        return lz_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load callback\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat\n",
    "import numpy as np\n",
    "\n",
    "class DistributionCallback(BaseCallback):\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self._log_freq = 10000\n",
    "        output_formats = self.logger.output_formats\n",
    "        self.tb_formatter = next(f for f in output_formats if isinstance(f, TensorBoardOutputFormat))\n",
    "\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._log_freq == 0:\n",
    "            dist_list = self.training_env.env_method('dists')\n",
    "            dists = np.concatenate(dist_list, axis=1) \n",
    "            try:\n",
    "                self.tb_formatter.writer.add_histogram('incentive_index', dists[:, 0], int(self.n_calls / self._log_freq))\n",
    "                self.tb_formatter.writer.add_histogram('distance_session_end', dists[:, 1], int(self.n_calls / self._log_freq))\n",
    "                self.tb_formatter.writer.add_histogram('distance_incentive_allocated', dists[:, 2], int(self.n_calls / self._log_freq))\n",
    "                self.tb_formatter.writer.flush()\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "TASK_INDEX = 3\n",
    "\n",
    "N_EVENT_INDEX = -1\n",
    "\n",
    "USER_IN_SESSION_INDEX = 0\n",
    "SESSION_COUNT_INDEX = 1\n",
    "TASK_IN_SESSION_INDEX = 2\n",
    "REWARD_ALLOCATED_INDEX = 3\n",
    "\n",
    "SESSION_FINISHED_INDEX = -1\n",
    "\n",
    "CUM_PLATFORM_TIME_INDEX = 4\n",
    "METADATA_INDEX = 12\n",
    "import logging\n",
    "from scipy.stats import norm \n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    logger = logging.getLogger(__name__) \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, user_sessions, experience_dataset, n_sequences, n_features) -> None:\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.user_sessions = user_sessions\n",
    "        self.experience_dataset = experience_dataset\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(n_sequences + 1, n_features), dtype=np.float32)\n",
    "        self.n_sequences = n_sequences\n",
    "        self.n_features = n_features\n",
    "        self.current_session = None\n",
    "        \n",
    "    def _extract_features(self, feature_array):\n",
    "        \n",
    "        metadata, features = feature_array[:, :METADATA_INDEX], feature_array[:, METADATA_INDEX:]\n",
    "        print(features.shape)\n",
    "        features = features.reshape((features.shape[0], self.n_sequences + 1, self.n_features))\n",
    "        features = np.flip(features, axis=1).squeeze(0)\n",
    "        return metadata.squeeze(0), features\n",
    "\n",
    "    def _state(self, user, session, task_count):\n",
    "        \n",
    "        \"\"\"\n",
    "        get index of current state\n",
    "        \"\"\" \n",
    "        current_state = self.experience_dataset[\n",
    "            (self.experience_dataset[:, USER_INDEX] == user) &\n",
    "            (self.experience_dataset[:, SESSION_INDEX] == session) &\n",
    "            (self.experience_dataset[:, TASK_INDEX] == task_count)\n",
    "        ]\n",
    "\n",
    "        metadata, features = self._extract_features(current_state)\n",
    "        cum_platform_time = metadata[CUM_PLATFORM_TIME_INDEX]\n",
    "        return features, cum_platform_time\n",
    "\n",
    "    \n",
    "    def _seed_user_session(self):\n",
    "        \"\"\"\n",
    "        find all users sessions that have not been completed\n",
    "        select random user session from list\n",
    "        \"\"\"\n",
    "        current_session = self.user_sessions[self.user_sessions['ended'] == 0].sample(1)\n",
    "        current_session['task_index'] = 1\n",
    "        current_session['total_reward'] = 0\n",
    "        self.current_session = current_session\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        self._take_action(action)\n",
    "            \n",
    "        state, rewards, done, meta = self._calculate_next_state() \n",
    "        if not done:\n",
    "            self._update_session_metadata(self.current_session)\n",
    "        \n",
    "        return state, rewards, done, meta\n",
    "\n",
    "    def _update_session_metadata(self, current_session):\n",
    "        self.user_sessions.loc[current_session.index] = current_session \n",
    "        \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        next_state = self.current_session['task_index'] + 1\n",
    "        extending = self._extending()\n",
    "        if not extending:\n",
    "            self.logger.debug(f'User: {self.current_session} has completed their session')\n",
    "            self._user_session_terminate()\n",
    "            if self.user_sessions['ended'].all():\n",
    "                self.logger.debug('All users have completed their sessions')\n",
    "                return None, self.user_sessions['total_reward'].sum().astype(float), True, {}\n",
    "            \n",
    "            self._seed_user_session()\n",
    "            user, session, count = self.current_session[['user_id', 'session_id', 'task_index']].values[0]\n",
    "            return (\n",
    "                self._state(user, session, count)[0], \n",
    "                self.user_sessions['total_reward'].sum().astype(float),\n",
    "                False,\n",
    "                {}\n",
    "            )\n",
    "        self.logger.debug(f'User: {self.current_session} has moving to next state: {next_state}')\n",
    "        self.current_session['task_index'] = next_state\n",
    "        user, session, count = self.current_session[['user_id', 'session_id', 'task_index']].values[0]\n",
    "        state, cum_platform_time = self._state(user, session, count)\n",
    "        self.current_session['total_reward'] = cum_platform_time\n",
    "        return (\n",
    "            state,\n",
    "            self.user_sessions['total_reward'].sum().astype(float),\n",
    "            False,\n",
    "            {}\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def _extending(self):\n",
    "        current_session = self.current_session.to_dict('records')[0]\n",
    "        if current_session['task_index'] == current_session['counts']:\n",
    "            return False\n",
    "    \n",
    "        if current_session['task_index'] <= current_session['sim_counts']:\n",
    "            return True\n",
    "\n",
    "        continue_session = self._probability_extending(current_session)\n",
    "        return all([continue_session >= 0.3, continue_session < 0.9])\n",
    "    \n",
    "    \n",
    "    def _probability_extending(self, current_session):\n",
    "        if current_session['incentive_index'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            continue_session = norm(\n",
    "                loc=current_session['incentive_index'],\n",
    "                scale=5\n",
    "            ).cdf(current_session['task_index']) + self._gaussian_noise()\n",
    "       \n",
    "        return continue_session\n",
    "        \n",
    "    def _gaussian_noise(self):\n",
    "        return np.random.normal(0, 0.1, 100).sum() / 10\n",
    "     \n",
    "    def _user_session_terminate(self):\n",
    "        self.current_session['ended'] = 1\n",
    "        self._update_session_metadata(self.current_session)\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        \n",
    "        current_session = self.current_session.to_dict('records')[0]\n",
    "        \n",
    "        if current_session['incentive_index'] > 0 or action == 0:\n",
    "            self.logger.debug(f'Incentive already allocation for session or no-op: {action}, {current_session}')\n",
    "            return\n",
    "        \n",
    "    \n",
    "        self.logger.debug('Taking action and allocating incentive')\n",
    "        self.current_session['incentive_index'] = self.current_session['task_index']\n",
    "        self.current_session['reward_allocated'] = action\n",
    "        \n",
    "        self.logger.debug('Taking action and allocating incentive: updating user session')\n",
    "        self.logger.debug(f'User session: {self.current_session}')\n",
    "\n",
    "    def reset(self):\n",
    "        self.user_sessions = self.user_sessions.sample(frac=1)\n",
    "        self.user_sessions['incentive_index'] = 0\n",
    "        self.user_sessions['task_index'] = 0\n",
    "        self.user_sessions['ended'] = 0\n",
    "        self.user_sessions['total_reward'] = 0\n",
    "        self.user_sessions['total_reward'] = self.user_sessions['total_reward'].astype(float)\n",
    "        \n",
    "        self._seed_user_session()\n",
    "        self._update_session_metadata(self.current_session)\n",
    "        user, session, count = self.current_session[['user_id', 'session_id', 'task_index']].values[0]\n",
    "        return self._state(user, session, count)[0]\n",
    "        \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        print('rendering')\n",
    "        \n",
    "    def dists(self):\n",
    "        incentive_index = self.user_sessions['incentive_index'].values\n",
    "        distance_end = (self.user_sessions['counts'] - self.user_sessions['incentive_index']).values\n",
    "        distance_reward = (self.user_sessions['total_reward'] - self.user_sessions['incentive_index']).values\n",
    "        return np.array([incentive_index, distance_end, distance_reward])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load incentive_reinforcement_learning_cpu.py\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, StopTrainingOnMaxEpisodes\n",
    "from stable_baselines3 import PPO, A2C\n",
    "import logging\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "import pandas as pd\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "from pprint import pformat\n",
    "import os\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "torch.set_printoptions(precision=2, linewidth=200, sci_mode=False)\n",
    "\n",
    "\n",
    "S3_BASELINE_PATH = 's3://dissertation-data-dmiller'\n",
    "\n",
    "def train_eval_split(dataset):\n",
    "    train_split = int(dataset.shape[0] * TRAIN_SPLIT)\n",
    "    eval_split = int(dataset.shape[0] * EVAL_SPLIT)\n",
    "    test_split = dataset.shape[0] - train_split - eval_split\n",
    "    logger.info(f'Train size: 0:{train_split}, eval size: {train_split}:{train_split+eval_split}: test size: {train_split + eval_split}:{dataset.shape[0]}')\n",
    "    train_dataset, eval_dataset, test_split = dataset[:train_split], dataset[train_split:train_split+eval_split], dataset[train_split+eval_split:]\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'eval': eval_dataset,\n",
    "        'test': test_split\n",
    "    }\n",
    "\n",
    "def generate_metadata(dataset):\n",
    "     \n",
    "    logger.info('Generating metadata tasks per session')\n",
    "    sessions = pd.DataFrame(\n",
    "        dataset[:, [USER_INDEX, SESSION_INDEX]],\n",
    "        columns=['user_id', 'session_id']\n",
    "    )\n",
    "    \n",
    "    sessions = sessions.groupby(['user_id', 'session_id']).size().reset_index(name='counts')\n",
    "    sessions['sim_counts'] = (sessions['counts'] * 0.8).astype(int)\n",
    "    sessions['sim_counts'] = sessions['sim_counts'].apply(lambda x: 1 if x == 0 else x)\n",
    "    sessions['incentive_index'] = 0\n",
    "    \n",
    "    sessions['task_index'] = 0\n",
    "    sessions['total_reward'] = 0\n",
    "    sessions['total_reward'] = sessions['total_reward'].astype(float)\n",
    "    sessions['ended'] = 0\n",
    "    return sessions\n",
    "\n",
    "\n",
    "def run_reinforcement_learning_incentives(environment, logger, n_episodes=1):\n",
    "    for epoch in range(n_episodes):\n",
    "        environment_comp = False\n",
    "        state = environment.reset()\n",
    "        i = 0\n",
    "        while not environment_comp:\n",
    "            next_action = (\n",
    "                1 if np.random.uniform(low=0, high=1) > 0.8 else 0\n",
    "            )\n",
    "            state, rewards, environment_comp, meta = environment.step(next_action)\n",
    "            i +=1\n",
    "            if i % 100 == 0:\n",
    "                logger.info(f'Step: {i} - Reward: {rewards}')\n",
    "                \n",
    "        logger.info(f'Epoch: {epoch} - Reward: {rewards}')\n",
    "        print(environment.user_sessions.head(10))\n",
    "\n",
    "    \n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    exec_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "    global logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    \n",
    "    read_path, n_files, n_sequences, n_features, n_episodes, device = (\n",
    "        args.read_path, \n",
    "        args.n_files, \n",
    "        args.n_sequences, \n",
    "        args.n_features, \n",
    "        args.n_episodes, \n",
    "        args.device\n",
    "    )\n",
    "    \n",
    "    npz_extractor = NPZExtractor(\n",
    "        read_path,\n",
    "        n_files,\n",
    "        n_sequences,\n",
    "        None,\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    cpu_count = os.cpu_count()\n",
    "   \n",
    "    logger.info(f'Starting experiment at {exec_time}') \n",
    "    logger.info(f'Extracting dataset from npz files to tensor' )\n",
    "    dataset = np.concatenate(npz_extractor.get_dataset_pointer(), axis=1)\n",
    "    datasets = train_eval_split(dataset)\n",
    "    train_data = datasets['train']\n",
    " \n",
    "    logger.info(f'Dataset shape: {dataset.shape}: generating metadata tensor')\n",
    "    sessions_train = generate_metadata(train_data)\n",
    "    logger.info(f'Metadata train: {sessions_train.shape}')\n",
    "    logger.info(f'Creating vectorized training environment: num envs: {cpu_count}')\n",
    "    \n",
    "\n",
    "    citizen_science_vec = SubprocVecEnv([lambda: CitizenScienceEnv(sessions_train, train_data, n_sequences, n_features) for _ in range(cpu_count)])\n",
    "    \"\"\"\n",
    "    Eval environment is not used in training and is used after training to evaluate the agent\n",
    "    \"\"\"\n",
    "    # citizen_science_vec_eval = DummyVecEnv([lambda: CitizenScienceEnv(sessions_train, train_data, n_sequences, n_features) for _ in range(2)])\n",
    "    \n",
    "    logger.info(f'Vectorized environments created, wrapping with monitor')\n",
    "    \n",
    "    monitor_train, monitor_eval = VecMonitor(citizen_science_vec), VecMonitor(citizen_science_vec)\n",
    "    base_path = os.path.join(\n",
    "        S3_BASELINE_PATH,\n",
    "        'reinforcement_learning_incentives',\n",
    "        f'n_files_{n_files}',\n",
    "        'results',\n",
    "        exec_time,\n",
    "    ) \n",
    "    \n",
    "    tensorboard_dir, checkpoint_dir = (\n",
    "        os.path.join(base_path, 'training_metrics'),\n",
    "        os.path.join(base_path, 'checkpoints')\n",
    "    )\n",
    " \n",
    "    agent = A2C(\n",
    "        'MlpPolicy',\n",
    "        monitor_train,\n",
    "        verbose=0,\n",
    "        device='cpu',\n",
    "        tensorboard_log=tensorboard_dir,\n",
    "    )\n",
    "    \n",
    "    eval_callback = EvalCallback(\n",
    "        monitor_eval,\n",
    "        best_model_save_path=checkpoint_dir,\n",
    "        eval_freq=1000,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "    \n",
    "    callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=n_episodes, verbose=0)\n",
    "    \n",
    "    dist_callback = DistributionCallback()\n",
    "    callback_list = CallbackList([dist_callback, callback_max_episodes, eval_callback])\n",
    "\n",
    "    logger.info(pformat([\n",
    "        'n_episodes: {}'.format(n_episodes),\n",
    "        'read_path: {}'.format(read_path),\n",
    "        'n_files: {}'.format(n_files),\n",
    "        'n_sequences: {}'.format(n_sequences),\n",
    "        'n_features: {}'.format(n_features),\n",
    "        'total_timesteps: {}'.format(dataset.shape[0] -1),\n",
    "        'device: {}'.format(device),\n",
    "        'tensorboard_dir: {}'.format(tensorboard_dir),\n",
    "        'checkpoint_dir: {}'.format(checkpoint_dir)\n",
    "    ]))\n",
    "\n",
    "    agent.learn(\n",
    "        total_timesteps=int(10e7),\n",
    "        log_interval=100, \n",
    "        progress_bar=True,\n",
    "        callback=callback_list\n",
    "    )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument:\n",
    "    read_path = 'torch_ready_data'\n",
    "    n_files = 2\n",
    "    n_sequences = 10\n",
    "    n_features = 18\n",
    "    n_episodes = 10\n",
    "    device = 'gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/14/2023 10:34:06 AM Starting experiment at 2023-04-14-10-34\n",
      "04/14/2023 10:34:06 AM Extracting dataset from npz files to tensor\n",
      "04/14/2023 10:34:06 AM Loading pointer to dataset: torch_ready_data/files_used_2/sequence_index_0: derived from torch_ready_data/files_used_2/sequence_index_0.npz\n",
      "04/14/2023 10:34:06 AM Loading pointer to dataset: torch_ready_data/files_used_2/sequence_index_10: derived from torch_ready_data/files_used_2/sequence_index_10.npz\n",
      "04/14/2023 10:34:06 AM Loading: torch_ready_data/files_used_2/sequence_index_0/arr_0.npy\n",
      "04/14/2023 10:34:06 AM Loading: torch_ready_data/files_used_2/sequence_index_10/arr_0.npy\n",
      "04/14/2023 10:34:09 AM Train size: 0:1796713, eval size: 1796713:2181723: test size: 2181723:2566734\n",
      "04/14/2023 10:34:09 AM Dataset shape: (2566734, 210): generating metadata tensor\n",
      "04/14/2023 10:34:09 AM Generating metadata tasks per session\n",
      "04/14/2023 10:34:10 AM Metadata train: (27963, 8)\n",
      "04/14/2023 10:34:10 AM Creating vectorized training environment: num envs: 8\n",
      "04/14/2023 10:34:57 AM Vectorized environments created, wrapping with monitor\n",
      "04/14/2023 10:34:59 AM ['n_episodes: 10',\n",
      " 'read_path: torch_ready_data',\n",
      " 'n_files: 2',\n",
      " 'n_sequences: 10',\n",
      " 'n_features: 18',\n",
      " 'total_timesteps: 2566733',\n",
      " 'device: gpu',\n",
      " 'tensorboard_dir: '\n",
      " 's3://dissertation-data-dmiller/reinforcement_learning_incentives/n_files_2/results/2023-04-14-10-34/training_metrics',\n",
      " 'checkpoint_dir: '\n",
      " 's3://dissertation-data-dmiller/reinforcement_learning_incentives/n_files_2/results/2023-04-14-10-34/checkpoints']\n",
      "04/14/2023 10:34:59 AM Found credentials in environment variables.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7f4fed8827413c903a9cff79702e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "main(Argument)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
