{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7968016923715895\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "import torch.distributions as tdist\n",
    "import numpy as np\n",
    "\n",
    "sample_norm = norm(\n",
    "    loc=13,\n",
    "    scale=13 ** .7\n",
    ").cdf(18)\n",
    "\n",
    "print(sample_norm)\n",
    "random_noise = np.random.uniform(-0.06, 0.01, 1).round(3)\n",
    "print(random_noise.mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.1373410196329804e-05"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_noise.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.random.normal(27, 15, 10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.966019494179836"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv --quiet\n",
    "!pip install gym stable-baselines3[extra] awscli boto3 pqdm awscli --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dissertation-data-dmiller/rl_ready_data_conv rl_ready_data_conv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant.py\n",
    "# %load rl_constant.py\n",
    "FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event\",\n",
    "    \"cum_session_time\",\n",
    "    \"expanding_click_average\",\n",
    "   \n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_event\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \"delta_last_event\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_count_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"cum_session_event_raw\",\n",
    "    \"date_time\"\n",
    "]\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'session_minutes',\n",
    "    'sim_size',\n",
    "    'sim_minutes',\n",
    "    'reward'\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    \"seq_40\",\n",
    "    \"session_terminates_30_minutes\"\n",
    "    # \"label\"\n",
    "]\n",
    "\n",
    "LOAD_COLS = list(set(FEATURE_COLUMNS + METADATA + RL_STAT_COLS + PREDICTION_COLS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment\n",
    "import gym\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "import gym\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, dataset, out_features, n_sequences, evaluation=False):\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.n_sequences = n_sequences\n",
    "        self.current_session = None\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        self.n_sequences = n_sequences\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(len(out_features), n_sequences + 1), dtype=np.float32)\n",
    "        self.evalution = evaluation\n",
    "        self.episode_bins = []\n",
    "        self.exp_runs = 0\n",
    "\n",
    "    def reset(self):\n",
    "        user_to_run, session_to_run = self.dataset.sample(1)[['user_id', 'session_30_count_raw']].values[0]\n",
    "        self.current_session = self._get_events(user_to_run, session_to_run)\n",
    "        self.metadata = self._metadata()\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        return self._state()\n",
    "    \n",
    "    def _row_to_dict(self, metadata):\n",
    "        \"\"\"\n",
    "        Convert a row of metadata to a dictionary.\n",
    "        \"\"\"\n",
    "        return metadata.to_dict()\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        self._take_action(action)\n",
    "        next_state, done, meta = self._calculate_next_state()\n",
    "        \n",
    "        if done:\n",
    "            current_session_index = self.current_session_index if \\\n",
    "                self.current_session_index != self.current_session.shape[0] else self.current_session.shape[0] - 1\n",
    "            \n",
    "            self.exp_runs += 1\n",
    "        \n",
    "            self.metadata['ended'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "            self.metadata['reward'] = self.current_session.iloc[current_session_index]['reward']\n",
    "            self.metadata['session_exp_time'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            self.metadata['exp_runs'] = self.exp_runs\n",
    "            self.episode_bins.append(self._row_to_dict(self.metadata))\n",
    "            \n",
    "            return next_state, float(self.reward), done, {}\n",
    "        else:\n",
    "            self.reward = self.current_session.iloc[self.current_session_index]['reward'] \n",
    "            self.current_session_index += 1        \n",
    "            return next_state, float(self.reward), done, meta\n",
    "    \n",
    "    def _metadata(self):\n",
    "        session_metadata = self.current_session.iloc[0][RL_STAT_COLS]\n",
    "        session_metadata['ended'] = 0\n",
    "        session_metadata['incentive_index'] = 0\n",
    "        return session_metadata\n",
    "    \n",
    "    \n",
    "    def flush_episode_bins(self):\n",
    "        episode_bins = self.episode_bins.copy()\n",
    "        self.episode_bins = []\n",
    "        return episode_bins\n",
    "    \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        if (self.current_session_index == self.current_session.shape[0]):\n",
    "            return None, True, {}\n",
    "\n",
    "        if self._continuing_in_session():\n",
    "            return self._state(), False, {}\n",
    "    \n",
    "        return None, True, {}\n",
    "         \n",
    "    def _continuing_in_session(self):\n",
    "        sim_minutes = self.current_session.iloc[self.current_session_index]['sim_minutes']\n",
    "        current_session_minutes = self.current_session.iloc[self.current_session_index]['cum_session_time_raw']\n",
    "        if current_session_minutes < sim_minutes:\n",
    "            return True\n",
    "        \n",
    "        extending_session = self._probability_extending_session(current_session_minutes)\n",
    "        \n",
    "        return all([extending_session >= .3, extending_session <= .7])\n",
    "        \n",
    "    \n",
    "    def _probability_extending_session(self, current_session_count):\n",
    "        if self.metadata['incentive_index'] == 0:\n",
    "            return 0\n",
    "        \n",
    "        scale = max(5, int(self.metadata['session_minutes'] / 5))\n",
    "        continue_session = norm(\n",
    "            loc=self.metadata['incentive_time'],\n",
    "            scale=scale\n",
    "        ).cdf(current_session_count)\n",
    "        \n",
    "        return continue_session\n",
    "        \n",
    "\n",
    "    def _get_events(self, user_id, session):\n",
    "        subset = self.dataset[\n",
    "            (self.dataset['user_id'] == user_id) &\n",
    "            (self.dataset['session_30_count_raw'] == session).copy()\n",
    "        ]\n",
    "\n",
    "        subset = subset.sort_values(by=['date_time'])\n",
    "        return subset\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        if action == 0 or self.metadata['incentive_index'] > 0:\n",
    "            return\n",
    "        \n",
    "        current_session_index = self.current_session_index if \\\n",
    "            self.current_session_index != self.current_session.shape[0] else self.current_session.shape[0] - 1\n",
    "        \n",
    "        self.metadata['incentive_index'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "        self.metadata['incentive_time'] = self.current_session.iloc[current_session_index]['cum_session_time_raw']\n",
    "        \n",
    "    def _state(self):\n",
    "\n",
    "        if self.current_session_index > self.n_sequences:\n",
    "            events = self.current_session.iloc[self.current_session_index - (self.n_sequences + 1):self.current_session_index][self.out_features].values\n",
    "            \n",
    "        else:\n",
    "            delta = min((self.n_sequences + 1)- self.current_session_index, self.n_sequences)\n",
    "            zero_cat = np.zeros((delta, len(self.out_features)))\n",
    "            events = self.current_session.iloc[:max(self.current_session_index, 1)][self.out_features].values\n",
    "            events = np.concatenate((zero_cat, events), axis=0)\n",
    "            \n",
    "\n",
    "        return events.astype(np.float32).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load callback\n",
    "# %load callback\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat\n",
    "from datetime import datetime\n",
    "\n",
    "class DistributionCallback(BaseCallback):\n",
    "    \n",
    "    @classmethod\n",
    "    def tensorboard_setup(cls, log_dir, log_freq):\n",
    "        cls._log_dir = log_dir\n",
    "        cls._log_freq = log_freq\n",
    "\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._log_freq == 0:\n",
    "            dist_list = self.training_env.env_method('flush_episode_bins')\n",
    "            values_to_log = [item for sublist in dist_list for item in sublist if len(sublist) > 0]\n",
    "\n",
    "            values_df = pd.DataFrame(\n",
    "                values_to_log\n",
    "            )\n",
    "            \n",
    "            \n",
    "            session_size, sim_size, session_minutes, sim_minutes, ended, reward, inc_time, inc_index = (\n",
    "                values_df['session_size'].mean(),\n",
    "                values_df['sim_size'].mean(),\n",
    "                values_df['session_minutes'].mean(),\n",
    "                values_df['sim_minutes'].mean(),\n",
    "                values_df['ended'].mean(),\n",
    "                values_df['reward'].mean(),\n",
    "                values_df['incentive_time'].mean(),\n",
    "                values_df['incentive_index'].mean()\n",
    "            )\n",
    "            \n",
    "            size_stats = {\n",
    "                'session_size': session_size,\n",
    "                'sim_size': sim_size,\n",
    "                'ended': ended,\n",
    "                'inc_index': inc_index\n",
    "            }\n",
    "            \n",
    "            \n",
    "            time_stats = {\n",
    "                'session_minutes': session_minutes,\n",
    "                'sim_minutes': sim_minutes,\n",
    "                'reward': reward,\n",
    "                'inc_time': inc_time   \n",
    "            }\n",
    "            \n",
    "            for key, value in size_stats.items():\n",
    "                self.logger.record(f'size/{key}', value)\n",
    "            \n",
    "            for key, value in time_stats.items():\n",
    "                self.logger.record(f'time/{key}', value)\n",
    "\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load policies/cnn_policy\n",
    "# %load policies/cnn_policy\n",
    "from typing import Dict, List, Type, Union\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CustomConv1dFeatures(BaseFeaturesExtractor):\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_sequences_features(cls, n_sequences, n_features):\n",
    "        cls.n_sequences = n_sequences\n",
    "        cls.n_features = n_features\n",
    "        \n",
    "    \n",
    "    def __init__(self, observation_space: spaces.Box, features_dim=20):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        self.cnn_1 = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            \n",
    "            nn.AvgPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.cnn_2 = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features*2, self.n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features, self.n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.act = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out_shape = self.act(self.cnn_2(self.cnn_1(torch.zeros((1, self.n_features, self.n_sequences))))).shape[1]\n",
    "            self.linear = nn.Linear(out_shape, features_dim)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        out = self.cnn_1(obs)\n",
    "        out = self.cnn_2(out)\n",
    "        out = self.act(out)\n",
    "        return self.linear(out)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_util\n",
    "import os\n",
    "import logging\n",
    "global logger\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "import numpy as np\n",
    "from pqdm.threads import pqdm\n",
    "import pdb\n",
    "\n",
    "def download_dataset_from_s3(client, base_read_path, full_read_path):\n",
    "    logger.info(f'Downloading data from {base_read_path}')\n",
    "    os.makedirs(base_read_path, exist_ok=True)\n",
    "    \n",
    "    logger.info(f'Downloading data from dissertation-data-dmiller/{full_read_path}')\n",
    "    client.download_file(\n",
    "        'dissertation-data-dmiller',\n",
    "        full_read_path,\n",
    "        full_read_path\n",
    "    )\n",
    "    logger.info(f'Downloaded data from dissertation-data-dmiller/{full_read_path}')\n",
    "    \n",
    "\n",
    "def _parralel_partition_users(unique_sessions, df, index, vec_df_path):\n",
    "    subset_session = df.merge(unique_sessions, on=['user_id', 'session_30_count_raw'], how='inner')\n",
    "    subset_session.to_parquet(f'{vec_df_path}/batch_{index}.parquet')\n",
    "    return subset_session.copy().reset_index(drop=True)\n",
    "    \n",
    "\n",
    "def batch_environments_for_vectorization(df, n_envs, vec_df_path):\n",
    "   \n",
    "    df[['user_id', 'session_30_count_raw']] = df[['user_id', 'session_30_count_raw']].astype(int)\n",
    "   \n",
    "    unique_sessions = df[['user_id', 'session_30_count_raw']].drop_duplicates().sample(frac=1).reset_index(drop=True)\n",
    "    logger.info(f'Unique sessions shape: {unique_sessions.shape}. Splitting into {n_envs} environments')\n",
    "    unique_session_split = np.array_split(unique_sessions, n_envs)\n",
    "    \n",
    "    unique_session_args = [{\n",
    "        'unique_sessions': sess,\n",
    "        'df': df,\n",
    "        'index': i,\n",
    "        'vec_df_path': vec_df_path,\n",
    "    } for i, sess in enumerate(unique_session_split)]\n",
    "\n",
    "    logger.info(f'Environments split: running parralel partitioning')\n",
    "    result = pqdm(unique_session_args, _parralel_partition_users, n_jobs=os.cpu_count() * 2, argument_type='kwargs')\n",
    "    logger.info(f'Environments split: finished parralel partitioning')\n",
    "    return result\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load incentive_reinforcement_learning_cpu.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from pprint import pformat\n",
    "from typing import Callable\n",
    "import boto3\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.common.callbacks import (CallbackList,\n",
    "                                                CheckpointCallback,\n",
    "                                                StopTrainingOnMaxEpisodes)\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "torch.set_printoptions(precision=2, linewidth=200, sci_mode=False)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "global logger\n",
    "logger = logging.getLogger('rl_exp_train')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "S3_BASELINE_PATH = 's3://dissertation-data-dmiller/'\n",
    "N_SEQUENCES = 40\n",
    "CHECKPOINT_FREQ = 250_000\n",
    "TB_LOG = 10_000\n",
    "WINDOW = 1\n",
    "\n",
    "def parse_args():\n",
    "    parse = argparse.ArgumentParser()\n",
    "    parse.add_argument('--read_path', type=str, default='rl_ready_data_conv')\n",
    "    parse.add_argument('--n_files', type=int, default=2)\n",
    "    parse.add_argument('--n_episodes', type=int, default=50)\n",
    "    parse.add_argument('--n_envs', type=int, default=100)\n",
    "    parse.add_argument('--lstm', type=str, default='label')\n",
    "    parse.add_argument('--part', type=str, default='train')\n",
    "    parse.add_argument('--feature_extractor', type=str, default='cnn') \n",
    "    args = parse.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_and_dedupe(read_path, cols):\n",
    "    \n",
    "    df = pd.read_parquet(read_path, columns=cols)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    \n",
    "    logger.info('Starting Incentive Reinforcement Learning')\n",
    "    logger.info(pformat(args.__dict__))\n",
    "    exec_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    \n",
    "    read_path, n_files, n_episodes, n_envs, lstm, part, feature_ext = (\n",
    "        args.read_path, \n",
    "        args.n_files, \n",
    "        args.n_episodes, \n",
    "        args.n_envs,\n",
    "        args.lstm,\n",
    "        args.part,\n",
    "        args.feature_extractor,\n",
    "    )\n",
    "\n",
    "    base_read_path = os.path.join('rl_ready_data_conv', f'files_used_{n_files}')\n",
    "    full_read_path = os.path.join(base_read_path, f'window_{WINDOW}_{part}_final.parquet')\n",
    "    vec_df_path = os.path.join(base_read_path, f'window_{WINDOW}_{part}_batched')\n",
    "    load_cols = LOAD_COLS\n",
    "\n",
    "    base_exp_path = '/'.join(\n",
    "        [\n",
    "            'rl_experiments',\n",
    "            f'n_files_{n_files}',\n",
    "            f'{feature_ext}_{lstm}',\n",
    "            exec_time,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(full_read_path):\n",
    "        client = boto3.client('s3')\n",
    "        download_dataset_from_s3(client,  base_read_path, full_read_path)\n",
    "        \n",
    "    if not os.path.exists(vec_df_path):\n",
    "        df = load_and_dedupe(full_read_path, cols=load_cols)\n",
    "        df = df.sort_values(['date_time', 'user_id'])\n",
    "        logger.info(f'Loaded data with shape {df.shape}')\n",
    "        os.makedirs(vec_df_path, exist_ok=True)\n",
    "        logger.info(f'Writing vectorized data to {vec_df_path}')\n",
    "        batch_environments_for_vectorization(df, n_envs, vec_df_path)\n",
    "        logger.info(f'Vectorized environments created')\n",
    "        del df\n",
    "    \n",
    "\n",
    "    logger.info(f'Loading vectorized data from {vec_df_path}')\n",
    "    vectorized_df = [\n",
    "        pd.read_parquet(os.path.join(vec_df_path, f'batch_{i}.parquet'), columns=LOAD_COLS)\n",
    "        for i in range(n_envs)\n",
    "    ]\n",
    "\n",
    "    out_features = FEATURE_COLUMNS + [lstm] if lstm else FEATURE_COLUMNS\n",
    "\n",
    "\n",
    "    citizen_science_vec =DummyVecEnv([lambda: CitizenScienceEnv(vec_df, out_features, N_SEQUENCES) for vec_df in vectorized_df])\n",
    "    monitor_train = VecMonitor(citizen_science_vec)\n",
    "    \n",
    "    logger.info(f'Vectorized environments created')\n",
    "\n",
    "\n",
    "\n",
    "    tensorboard_dir, checkpoint_dir = (\n",
    "        os.path.join(base_exp_path, 'training_metrics'),\n",
    "        os.path.join(base_exp_path, 'checkpoints')\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        logger.info(f'Creating directory {tensorboard_dir} for tensorboard logs')\n",
    "        os.makedirs(tensorboard_dir)\n",
    "   \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        logger.info(f'Creating directory {checkpoint_dir} for checkpoints')\n",
    "        os.makedirs(checkpoint_dir) \n",
    "\n",
    "    callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=n_episodes, verbose=1)\n",
    "    checkpoint_freq = int(CHECKPOINT_FREQ // n_envs)\n",
    "    log_freq = int(TB_LOG // n_envs)\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=checkpoint_freq,\n",
    "        save_path=checkpoint_dir, \n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    DistributionCallback.tensorboard_setup(tensorboard_dir, log_freq)\n",
    "    logger_callback = DistributionCallback()\n",
    "    \n",
    "    callback_list = CallbackList([checkpoint_callback, logger_callback, callback_max_episodes])\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if feature_ext == 'cnn':\n",
    "        CustomConv1dFeatures.setup_sequences_features(N_SEQUENCES + 1, len(out_features))\n",
    "        logger.info('Using custom 1 dimensional CNN feature extractor')\n",
    "        policy_kwargs = dict(\n",
    "            features_extractor_class=CustomConv1dFeatures,\n",
    "            net_arch=[10]\n",
    "        )\n",
    "        model = DQN(policy='CnnPolicy', env=monitor_train, verbose=1, tensorboard_log=tensorboard_dir, policy_kwargs=policy_kwargs, device=device, stats_window_size=1000)\n",
    "    else:\n",
    "        logger.info('Using default MLP feature extractor')\n",
    "        model = DQN(policy='MlpPolicy', env=monitor_train, verbose=1, tensorboard_log=tensorboard_dir, device=device, stats_window_size=1000)\n",
    "        \n",
    "    logger.info(f'Model created: policy')\n",
    "    \n",
    "    logger.info(pformat(model.policy))\n",
    "        \n",
    "    logger.info(f'Beginning training') \n",
    "    \n",
    "            \n",
    "    logger.info(pformat([\n",
    "        'n_episodes: {}'.format(n_episodes),\n",
    "        'read_path: {}'.format(read_path),\n",
    "        'n_files: {}'.format(n_files),\n",
    "        'n_sequences: {}'.format(N_SEQUENCES),\n",
    "        'n_envs: {}'.format(n_envs),\n",
    "        'device: {}'.format(device),\n",
    "        'lstm: {}'.format(lstm),\n",
    "        'part: {}'.format(part),\n",
    "        'feature_extractor: {}'.format(feature_ext),\n",
    "        'tensorboard_dir: {}'.format(tensorboard_dir),\n",
    "        'checkpoint_dir: {}'.format(checkpoint_dir),\n",
    "        'checkpoint_freq: {}'.format(checkpoint_freq),\n",
    "        'tb_freq: {}'.format(log_freq),\n",
    "    ]))\n",
    "    \n",
    "    model.learn(total_timesteps=25_000_000, progress_bar=True, log_interval=log_freq, callback=callback_list)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument:\n",
    "    read_path = 'rl_ready_data_conv'\n",
    "    n_files = 30\n",
    "    n_episodes = 500_000\n",
    "    n_envs = 100\n",
    "    lstm = None\n",
    "    part = 'train'\n",
    "    feature_extractor = 'cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/03/2023 07:33:59 PM Starting Incentive Reinforcement Learning\n",
      "06/03/2023 07:33:59 PM mappingproxy({'__dict__': <attribute '__dict__' of 'Argument' objects>,\n",
      "              '__doc__': None,\n",
      "              '__module__': '__main__',\n",
      "              '__weakref__': <attribute '__weakref__' of 'Argument' objects>,\n",
      "              'feature_extractor': 'cnn',\n",
      "              'lstm': None,\n",
      "              'n_envs': 100,\n",
      "              'n_episodes': 500000,\n",
      "              'n_files': 30,\n",
      "              'part': 'train',\n",
      "              'read_path': 'rl_ready_data_conv'})\n",
      "06/03/2023 07:33:59 PM Loading vectorized data from rl_ready_data_conv/files_used_30/window_1_train_batched\n",
      "06/03/2023 07:34:00 PM Vectorized environments created\n",
      "06/03/2023 07:34:00 PM Creating directory rl_experiments/n_files_30/cnn_None/2023-06-03_19-33-59/training_metrics for tensorboard logs\n",
      "06/03/2023 07:34:00 PM Creating directory rl_experiments/n_files_30/cnn_None/2023-06-03_19-33-59/checkpoints for checkpoints\n",
      "06/03/2023 07:34:00 PM Using custom 1 dimensional CNN feature extractor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/03/2023 07:34:01 PM Model created: policy\n",
      "06/03/2023 07:34:01 PM CnnPolicy(\n",
      "  (q_net): QNetwork(\n",
      "    (features_extractor): CustomConv1dFeatures(\n",
      "      (cnn_1): Sequential(\n",
      "        (0): Conv1d(20, 40, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv1d(40, 40, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (4): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "        (6): Conv1d(40, 40, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (7): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (8): Conv1d(40, 40, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (9): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "      )\n",
      "      (cnn_2): Sequential(\n",
      "        (0): Conv1d(40, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (4): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "      (act): Sequential(\n",
      "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      )\n",
      "      (linear): Linear(in_features=200, out_features=20, bias=True)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=10, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (q_net_target): QNetwork(\n",
      "    (features_extractor): CustomConv1dFeatures(\n",
      "      (cnn_1): Sequential(\n",
      "        (0): Conv1d(20, 40, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv1d(40, 40, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (4): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "        (6): Conv1d(40, 40, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (7): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (8): Conv1d(40, 40, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (9): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "      )\n",
      "      (cnn_2): Sequential(\n",
      "        (0): Conv1d(40, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "        (4): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "      (act): Sequential(\n",
      "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      )\n",
      "      (linear): Linear(in_features=200, out_features=20, bias=True)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=10, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "06/03/2023 07:34:01 PM Beginning training\n",
      "06/03/2023 07:34:01 PM ['n_episodes: 500000',\n",
      " 'read_path: rl_ready_data_conv',\n",
      " 'n_files: 30',\n",
      " 'n_sequences: 40',\n",
      " 'n_envs: 100',\n",
      " 'device: cuda',\n",
      " 'lstm: None',\n",
      " 'part: train',\n",
      " 'feature_extractor: cnn',\n",
      " 'tensorboard_dir: '\n",
      " 'rl_experiments/n_files_30/cnn_None/2023-06-03_19-33-59/training_metrics',\n",
      " 'checkpoint_dir: '\n",
      " 'rl_experiments/n_files_30/cnn_None/2023-06-03_19-33-59/checkpoints',\n",
      " 'checkpoint_freq: 2500',\n",
      " 'tb_freq: 100']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to rl_experiments/n_files_30/cnn_None/2023-06-03_19-33-59/training_metrics/DQN_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ef376832c8417fa527f384ca983839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 13.9      |\n",
      "|    ep_rew_mean      | 27.743235 |\n",
      "|    exploration_rate | 0.999     |\n",
      "| time/               |           |\n",
      "|    episodes         | 100       |\n",
      "|    fps              | 614       |\n",
      "|    time_elapsed     | 5         |\n",
      "|    total_timesteps  | 3600      |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 21.7      |\n",
      "|    ep_rew_mean      | 38.780884 |\n",
      "|    exploration_rate | 0.997     |\n",
      "| time/               |           |\n",
      "|    episodes         | 200       |\n",
      "|    fps              | 735       |\n",
      "|    time_elapsed     | 10        |\n",
      "|    total_timesteps  | 7400      |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 40.47379 |\n",
      "|    exploration_rate | 0.996    |\n",
      "| size/               |          |\n",
      "|    ended            | 23.2     |\n",
      "|    inc_index        | 2.04     |\n",
      "|    session_size     | 32.7     |\n",
      "|    sim_size         | 23.2     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 770      |\n",
      "|    inc_time         | 2.29     |\n",
      "|    reward           | 2.62     |\n",
      "|    session_minutes  | 57.9     |\n",
      "|    sim_minutes      | 39.9     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 11200    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 27.5      |\n",
      "|    ep_rew_mean      | 44.626587 |\n",
      "|    exploration_rate | 0.994     |\n",
      "| time/               |           |\n",
      "|    episodes         | 400       |\n",
      "|    fps              | 811       |\n",
      "|    time_elapsed     | 18        |\n",
      "|    total_timesteps  | 14900     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 30.3      |\n",
      "|    ep_rew_mean      | 48.101593 |\n",
      "|    exploration_rate | 0.993     |\n",
      "| time/               |           |\n",
      "|    episodes         | 500       |\n",
      "|    fps              | 841       |\n",
      "|    time_elapsed     | 22        |\n",
      "|    total_timesteps  | 19000     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 31.4      |\n",
      "|    ep_rew_mean      | 49.834805 |\n",
      "|    exploration_rate | 0.991     |\n",
      "| size/               |           |\n",
      "|    ended            | 38.1      |\n",
      "|    inc_index        | 1.97      |\n",
      "|    session_size     | 54        |\n",
      "|    sim_size         | 38.1      |\n",
      "| time/               |           |\n",
      "|    episodes         | 600       |\n",
      "|    fps              | 860       |\n",
      "|    inc_time         | 2.11      |\n",
      "|    reward           | 1.94      |\n",
      "|    session_minutes  | 81.2      |\n",
      "|    sim_minutes      | 57.8      |\n",
      "|    time_elapsed     | 26        |\n",
      "|    total_timesteps  | 23000     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 32.4     |\n",
      "|    ep_rew_mean      | 50.60825 |\n",
      "|    exploration_rate | 0.99     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 869      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 26300    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 33.2      |\n",
      "|    ep_rew_mean      | 51.732357 |\n",
      "|    exploration_rate | 0.989     |\n",
      "| size/               |           |\n",
      "|    ended            | 38.1      |\n",
      "|    inc_index        | 1.85      |\n",
      "|    session_size     | 53.9      |\n",
      "|    sim_size         | 38.1      |\n",
      "| time/               |           |\n",
      "|    episodes         | 800       |\n",
      "|    fps              | 882       |\n",
      "|    inc_time         | 1.85      |\n",
      "|    reward           | 2.11      |\n",
      "|    session_minutes  | 82.3      |\n",
      "|    sim_minutes      | 58.1      |\n",
      "|    time_elapsed     | 34        |\n",
      "|    total_timesteps  | 30200     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 33.1      |\n",
      "|    ep_rew_mean      | 51.886257 |\n",
      "|    exploration_rate | 0.987     |\n",
      "| time/               |           |\n",
      "|    episodes         | 900       |\n",
      "|    fps              | 889       |\n",
      "|    time_elapsed     | 38        |\n",
      "|    total_timesteps  | 33900     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 33.5      |\n",
      "|    ep_rew_mean      | 52.237904 |\n",
      "|    exploration_rate | 0.986     |\n",
      "| time/               |           |\n",
      "|    episodes         | 1000      |\n",
      "|    fps              | 897       |\n",
      "|    time_elapsed     | 42        |\n",
      "|    total_timesteps  | 37900     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 35.8      |\n",
      "|    ep_rew_mean      | 55.223915 |\n",
      "|    exploration_rate | 0.984     |\n",
      "| size/               |           |\n",
      "|    ended            | 34.7      |\n",
      "|    inc_index        | 1.99      |\n",
      "|    session_size     | 49.1      |\n",
      "|    sim_size         | 34.7      |\n",
      "| time/               |           |\n",
      "|    episodes         | 1100      |\n",
      "|    fps              | 904       |\n",
      "|    inc_time         | 2.22      |\n",
      "|    reward           | 2.14      |\n",
      "|    session_minutes  | 80.1      |\n",
      "|    sim_minutes      | 54.9      |\n",
      "|    time_elapsed     | 45        |\n",
      "|    total_timesteps  | 41400     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 36.7     |\n",
      "|    ep_rew_mean      | 55.95528 |\n",
      "|    exploration_rate | 0.983    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1200     |\n",
      "|    fps              | 911      |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 44700    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 37.5      |\n",
      "|    ep_rew_mean      | 57.271664 |\n",
      "|    exploration_rate | 0.982     |\n",
      "| time/               |           |\n",
      "|    episodes         | 1300      |\n",
      "|    fps              | 920       |\n",
      "|    time_elapsed     | 52        |\n",
      "|    total_timesteps  | 48600     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 36.8      |\n",
      "|    ep_rew_mean      | 56.713383 |\n",
      "|    exploration_rate | 0.98      |\n",
      "| size/               |           |\n",
      "|    ended            | 37.5      |\n",
      "|    inc_index        | 2.07      |\n",
      "|    session_size     | 53.1      |\n",
      "|    sim_size         | 37.5      |\n",
      "| time/               |           |\n",
      "|    episodes         | 1400      |\n",
      "|    fps              | 880       |\n",
      "|    inc_time         | 2.54      |\n",
      "|    reward           | 1.87      |\n",
      "|    session_minutes  | 81.9      |\n",
      "|    sim_minutes      | 57.4      |\n",
      "|    time_elapsed     | 58        |\n",
      "|    total_timesteps  | 51900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.957     |\n",
      "|    n_updates        | 4         |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 37.1      |\n",
      "|    ep_rew_mean      | 57.767685 |\n",
      "|    exploration_rate | 0.979     |\n",
      "| time/               |           |\n",
      "|    episodes         | 1500      |\n",
      "|    fps              | 885       |\n",
      "|    time_elapsed     | 62        |\n",
      "|    total_timesteps  | 55500     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.717     |\n",
      "|    n_updates        | 13        |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 36.8     |\n",
      "|    ep_rew_mean      | 56.92637 |\n",
      "|    exploration_rate | 0.977    |\n",
      "| time/               |          |\n",
      "|    episodes         | 1600     |\n",
      "|    fps              | 890      |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 59900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.09     |\n",
      "|    n_updates        | 24       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 37.2      |\n",
      "|    ep_rew_mean      | 57.834183 |\n",
      "|    exploration_rate | 0.976     |\n",
      "| size/               |           |\n",
      "|    ended            | 36.9      |\n",
      "|    inc_index        | 1.97      |\n",
      "|    session_size     | 52.2      |\n",
      "|    sim_size         | 36.9      |\n",
      "| time/               |           |\n",
      "|    episodes         | 1700      |\n",
      "|    fps              | 892       |\n",
      "|    inc_time         | 2.5       |\n",
      "|    reward           | 1.86      |\n",
      "|    session_minutes  | 83.2      |\n",
      "|    sim_minutes      | 58.1      |\n",
      "|    time_elapsed     | 71        |\n",
      "|    total_timesteps  | 64000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.485     |\n",
      "|    n_updates        | 34        |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 37.9      |\n",
      "|    ep_rew_mean      | 58.378834 |\n",
      "|    exploration_rate | 0.974     |\n",
      "| time/               |           |\n",
      "|    episodes         | 1800      |\n",
      "|    fps              | 897       |\n",
      "|    time_elapsed     | 76        |\n",
      "|    total_timesteps  | 68700     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.46      |\n",
      "|    n_updates        | 46        |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 38.7     |\n",
      "|    ep_rew_mean      | 59.0444  |\n",
      "|    exploration_rate | 0.972    |\n",
      "| size/               |          |\n",
      "|    ended            | 43.6     |\n",
      "|    inc_index        | 1.94     |\n",
      "|    session_size     | 61.7     |\n",
      "|    sim_size         | 43.5     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1900     |\n",
      "|    fps              | 900      |\n",
      "|    inc_time         | 1.71     |\n",
      "|    reward           | 1.78     |\n",
      "|    session_minutes  | 91.6     |\n",
      "|    sim_minutes      | 63.8     |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 72800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.69     |\n",
      "|    n_updates        | 56       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 38.6     |\n",
      "|    ep_rew_mean      | 59.06325 |\n",
      "|    exploration_rate | 0.971    |\n",
      "| time/               |          |\n",
      "|    episodes         | 2000     |\n",
      "|    fps              | 903      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 76900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.696    |\n",
      "|    n_updates        | 67       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 39.8      |\n",
      "|    ep_rew_mean      | 60.312485 |\n",
      "|    exploration_rate | 0.969     |\n",
      "| size/               |           |\n",
      "|    ended            | 41.1      |\n",
      "|    inc_index        | 1.98      |\n",
      "|    session_size     | 58.2      |\n",
      "|    sim_size         | 41.1      |\n",
      "| time/               |           |\n",
      "|    episodes         | 2100      |\n",
      "|    fps              | 906       |\n",
      "|    inc_time         | 1.84      |\n",
      "|    reward           | 2.03      |\n",
      "|    session_minutes  | 87.9      |\n",
      "|    sim_minutes      | 62        |\n",
      "|    time_elapsed     | 90        |\n",
      "|    total_timesteps  | 81900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.41      |\n",
      "|    n_updates        | 79        |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 40.8      |\n",
      "|    ep_rew_mean      | 61.900784 |\n",
      "|    exploration_rate | 0.967     |\n",
      "| time/               |           |\n",
      "|    episodes         | 2200      |\n",
      "|    fps              | 908       |\n",
      "|    time_elapsed     | 94        |\n",
      "|    total_timesteps  | 86100     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.31      |\n",
      "|    n_updates        | 90        |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 42        |\n",
      "|    ep_rew_mean      | 63.043247 |\n",
      "|    exploration_rate | 0.966     |\n",
      "| size/               |           |\n",
      "|    ended            | 46.8      |\n",
      "|    inc_index        | 2         |\n",
      "|    session_size     | 66.4      |\n",
      "|    sim_size         | 46.8      |\n",
      "| time/               |           |\n",
      "|    episodes         | 2300      |\n",
      "|    fps              | 909       |\n",
      "|    inc_time         | 2.14      |\n",
      "|    reward           | 1.76      |\n",
      "|    session_minutes  | 98.3      |\n",
      "|    sim_minutes      | 68.9      |\n",
      "|    time_elapsed     | 99        |\n",
      "|    total_timesteps  | 90200     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.19      |\n",
      "|    n_updates        | 100       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 41.7      |\n",
      "|    ep_rew_mean      | 62.469112 |\n",
      "|    exploration_rate | 0.964     |\n",
      "| time/               |           |\n",
      "|    episodes         | 2400      |\n",
      "|    fps              | 910       |\n",
      "|    time_elapsed     | 102       |\n",
      "|    total_timesteps  | 93800     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.32      |\n",
      "|    n_updates        | 109       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 40.8      |\n",
      "|    ep_rew_mean      | 61.047516 |\n",
      "|    exploration_rate | 0.963     |\n",
      "| time/               |           |\n",
      "|    episodes         | 2500      |\n",
      "|    fps              | 911       |\n",
      "|    time_elapsed     | 107       |\n",
      "|    total_timesteps  | 97800     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.31      |\n",
      "|    n_updates        | 119       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 41.3      |\n",
      "|    ep_rew_mean      | 61.673733 |\n",
      "|    exploration_rate | 0.961     |\n",
      "| size/               |           |\n",
      "|    ended            | 35.2      |\n",
      "|    inc_index        | 1.9       |\n",
      "|    session_size     | 49.8      |\n",
      "|    sim_size         | 35.2      |\n",
      "| time/               |           |\n",
      "|    episodes         | 2600      |\n",
      "|    fps              | 911       |\n",
      "|    inc_time         | 1.81      |\n",
      "|    reward           | 1.78      |\n",
      "|    session_minutes  | 76.9      |\n",
      "|    sim_minutes      | 53.7      |\n",
      "|    time_elapsed     | 111       |\n",
      "|    total_timesteps  | 101500    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.02      |\n",
      "|    n_updates        | 128       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.9     |\n",
      "|    ep_rew_mean      | 61.02838 |\n",
      "|    exploration_rate | 0.96     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2700     |\n",
      "|    fps              | 911      |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 105300   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.683    |\n",
      "|    n_updates        | 138      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40       |\n",
      "|    ep_rew_mean      | 60.19645 |\n",
      "|    exploration_rate | 0.959    |\n",
      "| time/               |          |\n",
      "|    episodes         | 2800     |\n",
      "|    fps              | 912      |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 108500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.806    |\n",
      "|    n_updates        | 146      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 39.6     |\n",
      "|    ep_rew_mean      | 59.82985 |\n",
      "|    exploration_rate | 0.957    |\n",
      "| size/               |          |\n",
      "|    ended            | 36.4     |\n",
      "|    inc_index        | 2.08     |\n",
      "|    session_size     | 51.4     |\n",
      "|    sim_size         | 36.4     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2900     |\n",
      "|    fps              | 913      |\n",
      "|    inc_time         | 2.61     |\n",
      "|    reward           | 2.07     |\n",
      "|    session_minutes  | 81.7     |\n",
      "|    sim_minutes      | 57       |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 112200   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.8      |\n",
      "|    n_updates        | 155      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 40.3     |\n",
      "|    ep_rew_mean      | 60.28913 |\n",
      "|    exploration_rate | 0.956    |\n",
      "| time/               |          |\n",
      "|    episodes         | 3000     |\n",
      "|    fps              | 916      |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 117000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.975    |\n",
      "|    n_updates        | 167      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 39.7     |\n",
      "|    ep_rew_mean      | 59.65618 |\n",
      "|    exploration_rate | 0.954    |\n",
      "| size/               |          |\n",
      "|    ended            | 43.1     |\n",
      "|    inc_index        | 1.96     |\n",
      "|    session_size     | 61       |\n",
      "|    sim_size         | 43.1     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3100     |\n",
      "|    fps              | 917      |\n",
      "|    inc_time         | 1.92     |\n",
      "|    reward           | 1.9      |\n",
      "|    session_minutes  | 88.7     |\n",
      "|    sim_minutes      | 61.9     |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 121000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.14     |\n",
      "|    n_updates        | 177      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 39       |\n",
      "|    ep_rew_mean      | 58.93857 |\n",
      "|    exploration_rate | 0.953    |\n",
      "| time/               |          |\n",
      "|    episodes         | 3200     |\n",
      "|    fps              | 918      |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total_timesteps  | 124700   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.64     |\n",
      "|    n_updates        | 186      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 37.7     |\n",
      "|    ep_rew_mean      | 57.09935 |\n",
      "|    exploration_rate | 0.951    |\n",
      "| time/               |          |\n",
      "|    episodes         | 3300     |\n",
      "|    fps              | 920      |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total_timesteps  | 128700   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.29     |\n",
      "|    n_updates        | 196      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 39.3     |\n",
      "|    ep_rew_mean      | 59.46882 |\n",
      "|    exploration_rate | 0.949    |\n",
      "| size/               |          |\n",
      "|    ended            | 38.8     |\n",
      "|    inc_index        | 2.05     |\n",
      "|    session_size     | 54.9     |\n",
      "|    sim_size         | 38.8     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3400     |\n",
      "|    fps              | 922      |\n",
      "|    inc_time         | 2.34     |\n",
      "|    reward           | 1.95     |\n",
      "|    session_minutes  | 83.7     |\n",
      "|    sim_minutes      | 58.3     |\n",
      "|    time_elapsed     | 144      |\n",
      "|    total_timesteps  | 132900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.54     |\n",
      "|    n_updates        | 207      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 39.3      |\n",
      "|    ep_rew_mean      | 59.197853 |\n",
      "|    exploration_rate | 0.948     |\n",
      "| time/               |           |\n",
      "|    episodes         | 3500      |\n",
      "|    fps              | 921       |\n",
      "|    time_elapsed     | 148       |\n",
      "|    total_timesteps  | 136500    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 2.76      |\n",
      "|    n_updates        | 216       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 39        |\n",
      "|    ep_rew_mean      | 58.937786 |\n",
      "|    exploration_rate | 0.947     |\n",
      "| time/               |           |\n",
      "|    episodes         | 3600      |\n",
      "|    fps              | 922       |\n",
      "|    time_elapsed     | 151       |\n",
      "|    total_timesteps  | 139900    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.865     |\n",
      "|    n_updates        | 224       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 38.8     |\n",
      "|    ep_rew_mean      | 58.77157 |\n",
      "|    exploration_rate | 0.945    |\n",
      "| size/               |          |\n",
      "|    ended            | 38.2     |\n",
      "|    inc_index        | 1.95     |\n",
      "|    session_size     | 54       |\n",
      "|    sim_size         | 38.2     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3700     |\n",
      "|    fps              | 922      |\n",
      "|    inc_time         | 1.94     |\n",
      "|    reward           | 2.11     |\n",
      "|    session_minutes  | 85.2     |\n",
      "|    sim_minutes      | 59.5     |\n",
      "|    time_elapsed     | 155      |\n",
      "|    total_timesteps  | 143700   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.68     |\n",
      "|    n_updates        | 234      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 38.7     |\n",
      "|    ep_rew_mean      | 58.90962 |\n",
      "|    exploration_rate | 0.944    |\n",
      "| time/               |          |\n",
      "|    episodes         | 3800     |\n",
      "|    fps              | 921      |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 147000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.56     |\n",
      "|    n_updates        | 242      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 38.8      |\n",
      "|    ep_rew_mean      | 58.846134 |\n",
      "|    exploration_rate | 0.942     |\n",
      "| size/               |           |\n",
      "|    ended            | 36.2      |\n",
      "|    inc_index        | 2.07      |\n",
      "|    session_size     | 51.3      |\n",
      "|    sim_size         | 36.2      |\n",
      "| time/               |           |\n",
      "|    episodes         | 3900      |\n",
      "|    fps              | 923       |\n",
      "|    inc_time         | 2.61      |\n",
      "|    reward           | 2.19      |\n",
      "|    session_minutes  | 80.4      |\n",
      "|    sim_minutes      | 56.9      |\n",
      "|    time_elapsed     | 164       |\n",
      "|    total_timesteps  | 151500    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.84      |\n",
      "|    n_updates        | 253       |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 38.3     |\n",
      "|    ep_rew_mean      | 58.46743 |\n",
      "|    exploration_rate | 0.941    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4000     |\n",
      "|    fps              | 923      |\n",
      "|    time_elapsed     | 167      |\n",
      "|    total_timesteps  | 155000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.17     |\n",
      "|    n_updates        | 262      |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main(Argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
