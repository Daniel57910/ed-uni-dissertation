{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install python-dotenv --quiet\n",
    "!python -m pip install gym stable-baselines3[extra] awscli boto3 --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync reinforcement_learning_incentives s3://dissertation-data-dmiller/reinforcement_learning_incentives  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! if ![ -d \"rl_ready_data\" ]; then aws s3 cp s3://dissertation-data-dmiller/rl_ready_data ./rl_ready_data --recursive; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load rl_constant.py\n",
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time\",\n",
    "    \n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "PREDICTION_COLS = [\n",
    "    'seq_10',\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "RL_STAT_COLS = [\n",
    "    'session_size',\n",
    "    'sim_size',\n",
    "    'session_minutes',\n",
    "    'sim_minutes',\n",
    "    'reward',\n",
    "    'session_30_raw',\n",
    "    'cum_platform_time_raw',\n",
    "    'global_session_time',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load callback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import TensorBoardOutputFormat\n",
    "from datetime import datetime\n",
    "\n",
    "class DistributionCallback(BaseCallback):\n",
    "    \n",
    "    @classmethod\n",
    "    def tensorboard_setup(cls, log_dir, log_freq):\n",
    "        cls._log_dir = log_dir\n",
    "        cls._log_freq = log_freq\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        output_formats = self.logger.output_formats\n",
    "        self.tb_formatter = next(f for f in output_formats if isinstance(f, TensorBoardOutputFormat))\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._log_freq == 0:\n",
    "            dist_list = self.training_env.env_method('dists')\n",
    "            values_to_log = np.concatenate([d for d in dist_list if d.shape[0] > 0])\n",
    "\n",
    "            values_df = pd.DataFrame(\n",
    "                values_to_log, \n",
    "                columns=RL_STAT_COLS + ['ended', 'incentive_index', 'n_episodes'] + ['date_time']\n",
    "            )\n",
    "            \n",
    "            dist_session_time = (values_df['session_minutes'] - values_df['reward']).mean()\n",
    "            dist_session_end = (values_df['session_size'] - values_df['ended']).mean()\n",
    "            \n",
    "            dist_sim_time = (values_df['reward'] - values_df['sim_size']).mean()\n",
    "            dist_sim_end = (values_df['ended'] - values_df['sim_size']).mean()\n",
    "\n",
    "\n",
    "            dist_inc_session = (values_df['session_size'] - values_df['incentive_index']).mean()\n",
    "            dist_inc_end = (values_df['ended'] - values_df['incentive_index']).mean()\n",
    "            dist_inc_sim_index = (values_df['sim_size'] - values_df['incentive_index']).mean()\n",
    "\n",
    "            n_call = self.n_calls // self._log_freq\n",
    "            \n",
    "            self.tb_formatter.writer.add_scalar('distance/session/max_reward::decrease', dist_session_time, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/session/max_ended::decrease', dist_session_end, n_call)\n",
    "           \n",
    "            self.tb_formatter.writer.add_scalar('distance/session/sim_reward::increase', dist_sim_time, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/session/sim_ended::increase', dist_sim_end, n_call)\n",
    "            \n",
    "            \n",
    "            self.tb_formatter.writer.add_scalar('distance/incentive/max_incentive::decrease', dist_inc_session, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/incentive/ended_incentive', dist_inc_end, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/incentive/sim_inc_placement::decrease', dist_inc_sim_index, n_call)\n",
    "            self.tb_formatter.writer.add_scalar('distance/ended_sim_size::increase', dist_inc_sim_index, n_call)\n",
    "            \n",
    "            self.tb_formatter.writer.flush()\n",
    "            \n",
    "            current_time = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "            values_df.to_csv(f'{self._log_dir}/dist_{n_call}_{current_time}.csv', index=False)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load environment\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm \n",
    "import gym\n",
    "from datetime import datetime\n",
    "\n",
    "class CitizenScienceEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, dataset, unique_episodes, unique_sessions, out_features, n_sequences):\n",
    "        \"\"\"\n",
    "        trajectories: dictionary of user_id to their respective trajectories.\n",
    "        n_sequences: number of sequences used for preprocessing.\n",
    "        n_features: number of features used for preprocessing.\n",
    "        \"\"\"\n",
    "        super(CitizenScienceEnv, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.unique_episodes = unique_episodes\n",
    "        self.n_episodes = 0\n",
    "        self.n_sequences = n_sequences\n",
    "        self.unique_sessions = unique_sessions\n",
    "        self.current_session = None\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        self.metadata_container = []\n",
    "        self.n_sequences = n_sequences\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(len(out_features), n_sequences + 1), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_episodes += 1\n",
    "        session_to_run = self.unique_sessions.sample(1)['session_30_raw'].values[0]\n",
    "        user_to_run = self.unique_episodes[self.unique_episodes['session_30_raw'] == session_to_run].sample(1)['user_id'].values[0]\n",
    "        self.current_session = self._get_events(user_to_run, session_to_run)\n",
    "        self.metadata = self._metadata()\n",
    "        self.current_session_index = 0\n",
    "        self.reward = 0\n",
    "        return self._state()\n",
    "\n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        next_state, done, meta = self._calculate_next_state()\n",
    "        \n",
    "        if done:\n",
    "            curent_session_index = min(self.current_session_index, self.current_session.shape[0] - 1)\n",
    "            self.metadata['ended'] = self.current_session.iloc[curent_session_index]['cum_session_event_raw']\n",
    "            self.metadata['reward'] = self.reward\n",
    "            self.metadata['date_time'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            self.metadata_container.append(self.metadata.values)\n",
    "            return next_state, float(self.reward), done, meta\n",
    "        else:\n",
    "            self.reward = self.current_session.iloc[self.current_session_index]['reward'] \n",
    "            self.current_session_index += 1        \n",
    "        return next_state, float(self.reward), done, meta\n",
    "    \n",
    "    def _metadata(self):\n",
    "        session_metadata = self.current_session.iloc[0][RL_STAT_COLS]\n",
    "        session_metadata['ended'] = 0\n",
    "        session_metadata['incentive_index'] = 0\n",
    "        session_metadata['n_episodes'] = self.n_episodes\n",
    "        return session_metadata\n",
    "    \n",
    "    \n",
    "    def _calculate_next_state(self):\n",
    "        \n",
    "        if (self.current_session_index == self.current_session.shape[0]):\n",
    "            return None, True, {}\n",
    "\n",
    "        if self._continuing_in_session():\n",
    "            return self._state(), False, {}\n",
    "    \n",
    "        return None, True, {}\n",
    "        \n",
    "      \n",
    "  \n",
    "    def _continuing_in_session(self):\n",
    "        sim_counts = self.metadata['sim_size']\n",
    "        current_session_count = self.current_session.iloc[self.current_session_index]['cum_session_event_raw']\n",
    "        if current_session_count < sim_counts:\n",
    "            return True\n",
    "        \n",
    "        extending_session = self._probability_extending_session(current_session_count)\n",
    "        \n",
    "        return all([extending_session >= .3, extending_session <= .7])\n",
    "        \n",
    "    \n",
    "    def _probability_extending_session(self, current_session_count):\n",
    "        if self.metadata['incentive_index'] == 0:\n",
    "            return 0\n",
    "        \n",
    "        scale = max(5, int(self.metadata['session_size'] / 4))\n",
    "        continue_session = norm(\n",
    "            loc=self.metadata['incentive_index'],\n",
    "            scale=scale\n",
    "        ).cdf(current_session_count)\n",
    "        \n",
    "        return continue_session\n",
    "        \n",
    "\n",
    "    def _get_events(self, user_id, session):\n",
    "        subset = self.dataset[\n",
    "            (self.dataset['user_id'] == user_id) &\n",
    "            (self.dataset['session_30_raw'] == session)\n",
    "        ]\n",
    "   \n",
    "        return subset.sort_values(by=['date_time']).reset_index(drop=True)\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        if action == 0 or self.metadata['incentive_index'] > 0:\n",
    "            return\n",
    "        \n",
    "        current_session_index = min(self.current_session_index, self.current_session.shape[0] - 1)\n",
    "        self.metadata['incentive_index'] = self.current_session.iloc[current_session_index]['cum_session_event_raw']\n",
    "        \n",
    "    def _state(self):\n",
    "\n",
    "        if self.current_session_index > self.n_sequences:\n",
    "            events = self.current_session.iloc[self.current_session_index - (self.n_sequences + 1):self.current_session_index][self.out_features].values\n",
    "            \n",
    "        else:\n",
    "            delta = min((self.n_sequences + 1)- self.current_session_index, self.n_sequences)\n",
    "            zero_cat = np.zeros((delta, len(self.out_features)))\n",
    "            events = self.current_session.iloc[:max(self.current_session_index, 1)][self.out_features].values\n",
    "            events = np.concatenate((zero_cat, events), axis=0)\n",
    "            \n",
    "\n",
    "        return events.astype(np.float32).T\n",
    "  \n",
    "    \n",
    "    def dists(self):\n",
    "        metadata_container = self.metadata_container.copy()\n",
    "        self.metadata_container = []\n",
    "        return np.array(metadata_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load policies/cnn_policy\n",
    "# %load policies/cnn_policy\n",
    "from typing import Dict, List, Type, Union\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CustomConv1dFeatures(BaseFeaturesExtractor):\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_sequences_features(cls, n_sequences, n_features):\n",
    "        cls.n_sequences = n_sequences\n",
    "        cls.n_features = n_features\n",
    "        \n",
    "    \n",
    "    def __init__(self, observation_space: spaces.Box, features_dim=20):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        \n",
    "        self.cnn_1 = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features*2),\n",
    "            nn.Conv1d(self.n_features*2, self.n_features*2, kernel_size=3, padding=1),\n",
    "            \n",
    "            nn.AvgPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.cnn_2 = nn.Sequential(\n",
    "            nn.Conv1d(self.n_features*2, self.n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(self.n_features, self.n_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(self.n_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.act = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out_shape = self.act(self.cnn_2(self.cnn_1(torch.zeros((1, self.n_features, self.n_sequences))))).shape[1]\n",
    "            self.linear = nn.Linear(out_shape, features_dim)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        out = self.cnn_1(obs)\n",
    "        out = self.cnn_2(out)\n",
    "        out = self.act(out)\n",
    "        return self.linear(out)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load incentive_reinforcement_learning_cpu.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from pprint import pformat\n",
    "from typing import Callable\n",
    "import boto3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from callback import DistributionCallback\n",
    "# from environment import CitizenScienceEnv\n",
    "# from policies.cnn_policy import CustomConv1dFeatures\n",
    "# from rl_constant import LABEL, METADATA, OUT_FEATURE_COLUMNS, PREDICTION_COLS\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.common.callbacks import (CallbackList,\n",
    "                                                CheckpointCallback,\n",
    "                                                StopTrainingOnMaxEpisodes)\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.dqn.policies import DQNPolicy\n",
    "\n",
    "ALL_COLS = LABEL + METADATA + OUT_FEATURE_COLUMNS \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=200, suppress=True)\n",
    "torch.set_printoptions(precision=2, linewidth=200, sci_mode=False)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "S3_BASELINE_PATH = 's3://dissertation-data-dmiller'\n",
    "USER_INDEX = 1\n",
    "SESSION_INDEX = 2\n",
    "CUM_SESSION_EVENT_RAW = 3\n",
    "TIMESTAMP_INDEX = 11\n",
    "TRAIN_SPLIT = 0.7\n",
    "EVAL_SPLIT = 0.15\n",
    "\n",
    "\n",
    "def train_eval_split(dataset, logger):\n",
    "    train_split = int(dataset.shape[0] * TRAIN_SPLIT)\n",
    "    eval_split = int(dataset.shape[0] * EVAL_SPLIT)\n",
    "    test_split = dataset.shape[0] - train_split - eval_split\n",
    "    logger.info(f'Train size: 0:{train_split}, eval size: {train_split}:{train_split+eval_split}: test size: {train_split + eval_split}:{dataset.shape[0]}')\n",
    "    train_dataset, eval_dataset, test_split = dataset[:train_split], dataset[train_split:train_split+eval_split], dataset[train_split+eval_split:]\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'eval': eval_dataset,\n",
    "        'test': test_split\n",
    "    }\n",
    "\n",
    "def generate_metadata(dataset):\n",
    "    \n",
    "    session_size = dataset.groupby(['user_id', 'session_30_raw'])['size_of_session'].max().reset_index(name='session_size')\n",
    "    session_minutes = dataset.groupby(['user_id', 'session_30_raw'])['cum_session_time_raw'].max().reset_index(name='session_minutes')\n",
    "    \n",
    "    sim_minutes = dataset.groupby(['user_id', 'session_30_raw'])['cum_session_time_raw'].quantile(.7, interpolation='nearest').reset_index(name='sim_minutes')\n",
    "    sim_size = dataset.groupby(['user_id', 'session_30_raw'])['cum_session_event_raw'].quantile(.7, interpolation='nearest').reset_index(name='sim_size')\n",
    "    \n",
    "    \n",
    "    sessions = [session_size, session_minutes, sim_minutes, sim_size]\n",
    "    sessions = reduce(lambda left, right: pd.merge(left, right, on=['user_id', 'session_30_raw']), sessions)\n",
    "    dataset = pd.merge(dataset, sessions, on=['user_id', 'session_30_raw'])\n",
    "    dataset['reward'] = dataset['cum_session_time_raw']\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parse = argparse.ArgumentParser()\n",
    "    parse.add_argument('--read_path', type=str, default='datasets/rl_ready_data')\n",
    "    parse.add_argument('--n_files', type=int, default=2)\n",
    "    parse.add_argument('--n_episodes', type=int, default=50)\n",
    "    parse.add_argument('--n_sequences', type=int, default=40)\n",
    "    parse.add_argument('--n_envs', type=int, default=100)\n",
    "    parse.add_argument('--lstm', type=str, default='seq_10')\n",
    "    parse.add_argument('--device', type=str, default='cpu')\n",
    "    parse.add_argument('--checkpoint_freq', type=int, default=1000)\n",
    "    parse.add_argument('--tb_log', type=int, default=100)\n",
    "    parse.add_argument('--feature_extractor', type=str, default='cnn') \n",
    "    args = parse.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def run_reinforcement_learning_incentives(environment, logger, n_episodes=1):\n",
    "    for epoch in range(n_episodes):\n",
    "        environment_comp = False\n",
    "        state = environment.reset()\n",
    "        i = 0\n",
    "        while not environment_comp:\n",
    "            next_action = (\n",
    "                1 if np.random.uniform(low=0, high=1) > 0.8 else 0\n",
    "            )\n",
    "            state, rewards, environment_comp, meta = environment.step(next_action)\n",
    "            i +=1\n",
    "            if i % 100 == 0:\n",
    "                logger.info(f'Step: {i} - Reward: {rewards}')\n",
    "                \n",
    "        logger.info(f'Epoch: {epoch} - Reward: {rewards}')\n",
    "        print(environment.user_sessions.head(10))\n",
    "\n",
    "\n",
    "def remove_events_in_minute_window(df):\n",
    "    df['second_window'] = df['second'] // 10\n",
    "    df = df.drop_duplicates(\n",
    "        subset=['user_id', 'session_30_raw', 'year', 'month', 'day', 'hour', 'minute'],\n",
    "        keep='last'\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def convolve_delta_events(df, window):\n",
    "    df['convolved_delta_event'] = (\n",
    "        df.set_index('date_time').groupby(by=['user_id', 'session_30_raw'], group_keys=False) \\\n",
    "            .rolling(f'{window}T', min_periods=1)['delta_last_event'] \\\n",
    "            .mean()\n",
    "            .reset_index(name='convolved_event_delta')['convolved_event_delta']\n",
    "    )\n",
    "\n",
    "    df['delta_last_event'] = df['convolved_delta_event']\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "def main(args):\n",
    "    \n",
    "    exec_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    logger.info('Starting Incentive Reinforcement Learning')\n",
    "    \n",
    "    read_path, n_files, n_sequences, n_episodes, device, n_envs, lstm, tb_log, check_freq, feature_ext, window = (\n",
    "        args.read_path, \n",
    "        args.n_files, \n",
    "        args.n_sequences, \n",
    "        args.n_episodes, \n",
    "        args.device,\n",
    "        args.n_envs,\n",
    "        args.lstm,\n",
    "        args.tb_log,\n",
    "        args.checkpoint_freq,\n",
    "        args.feature_extractor,\n",
    "        args.window\n",
    "    )\n",
    "    \n",
    "    read_path = os.path.join(\n",
    "        read_path,\n",
    "        f'files_used_{n_files}',\n",
    "        f'predicted_data.parquet'\n",
    "    )\n",
    "   \n",
    "    if not os.path.exists(read_path):\n",
    "        logger.info(f'Downloading data from {S3_BASELINE_PATH}/{read_path}') \n",
    "        s3 = boto3.client('s3')\n",
    "        s3.download_file(\n",
    "            S3_BASELINE_PATH,     \n",
    "            read_path,\n",
    "            read_path\n",
    "        )\n",
    "             \n",
    "            \n",
    "    logger.info(f'Reading data from {read_path}')\n",
    "    \n",
    "    logger.info(f'Setting up model with prediction {lstm}')\n",
    "    df = pd.read_parquet(read_path, columns=ALL_COLS + [args.lstm] if args.lstm else ALL_COLS)\n",
    "    df['date_time'] = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute', 'second']], errors='coerce')\n",
    "\n",
    "\n",
    "    df = df.sort_values(by=['date_time'])    \n",
    "    \n",
    "    logger.info(f'N events:: {df.shape[0]} creating training partitions')\n",
    "    df = df.head(int(df.shape[0] * .7))\n",
    "    logger.info(f'N events after 70% split: {df.shape[0]}')\n",
    "    size_of_session = df.groupby(['user_id', 'session_30_raw']).size().reset_index(name='size_of_session')\n",
    "    df = pd.merge(df, size_of_session, on=['user_id', 'session_30_raw'])\n",
    "    df['cum_session_event_raw'] = df.groupby(['user_id', 'session_30_raw'])['date_time'].cumcount() + 1\n",
    "    \n",
    "    logger.info(f'Convolution over {window} minute window')\n",
    "    df = convolve_delta_events(df, window)\n",
    "    logger.info(f'Convolving over 2 minute window complete: generating metadata')\n",
    "    df = generate_metadata(df) \n",
    "    logger.info(f'Metadata generated: selecting events only at {window} minute intervals')\n",
    "    df = df[df['minute'] % window == 0]\n",
    "    logger.info(f'Data read: {df.shape[0]} rows, {df.shape[1]} columns, dropping events within 2 minute window')\n",
    "    df = remove_events_in_minute_window(df)\n",
    "    return df\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    logger.info(f'Number of events after dropping events within 2 minute window: {df.shape[0]}')\n",
    "    \n",
    "    unique_episodes = df[['user_id', 'session_30_raw']].drop_duplicates()\n",
    "    unique_sessions = df[['session_30_raw']].drop_duplicates()\n",
    "    df = df.drop(columns=['year', 'month', 'day', 'hour', 'minute', 'second', 'second_window'])\n",
    "    out_features = OUT_FEATURE_COLUMNS + [lstm] if lstm else OUT_FEATURE_COLUMNS\n",
    "    \n",
    "    citizen_science_vec =DummyVecEnv([lambda: CitizenScienceEnv(df, unique_episodes, unique_sessions, out_features, 10) for i in range(n_envs)])\n",
    "    logger.info(f'Vectorized environments created')\n",
    "    \n",
    "    base_path = os.path.join(\n",
    "        S3_BASELINE_PATH,\n",
    "        'reinforcement_learning_incentives',\n",
    "        f'n_files_{n_files}',\n",
    "        feature_ext + '_' + 'label' if lstm.startswith('continue') else lstm,\n",
    "        'results',\n",
    "        exec_time,\n",
    "    ) \n",
    "    \n",
    "    \n",
    "    tensorboard_dir, checkpoint_dir = (\n",
    "        os.path.join(base_path, 'training_metrics'),\n",
    "        os.path.join(base_path, 'checkpoints')\n",
    "    )\n",
    "\n",
    "    logger.info(f'Creating callbacks, monitors and loggerss')\n",
    "    callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=n_episodes, verbose=1)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=check_freq // (n_envs // 2), save_path=checkpoint_dir, name_prefix='rl_model')\n",
    "    dist_callback = DistributionCallback()\n",
    "    DistributionCallback.tensorboard_setup(tensorboard_dir, tb_log)\n",
    "    callback_list = CallbackList([callback_max_episodes, dist_callback, checkpoint_callback])\n",
    "    monitor_train = VecMonitor(citizen_science_vec)\n",
    "    \n",
    "    \n",
    "    if feature_ext == 'cnn':\n",
    "        CustomConv1dFeatures.setup_sequences_features(n_sequences + 1, 21)\n",
    "        logger.info('Using custom 1 dimensional CNN feature extractor')\n",
    "        policy_kwargs = dict(\n",
    "            features_extractor_class=CustomConv1dFeatures,\n",
    "            net_arch=[10]\n",
    "        )\n",
    "        model = DQN(policy='CnnPolicy', env=monitor_train, verbose=1, tensorboard_log=tensorboard_dir, policy_kwargs=policy_kwargs, device=device, stats_window_size=1000)\n",
    "    else:\n",
    "        logger.info('Using default MLP feature extractor')\n",
    "        model = DQN(policy='MlpPolicy', env=monitor_train, verbose=1, tensorboard_log=tensorboard_dir, device=device, stats_window_size=1000)\n",
    "    \n",
    "    logger.info(f'Model created: policy')\n",
    "    \n",
    "    logger.info(pformat(model.policy))\n",
    "        \n",
    "    logger.info(f'Beginning training') \n",
    "    \n",
    "    # retutrn\n",
    "            \n",
    "    logger.info(pformat([\n",
    "        'n_episodes: {}'.format(n_episodes),\n",
    "        'read_path: {}'.format(read_path),\n",
    "        'n_files: {}'.format(n_files),\n",
    "        'n_sequences: {}'.format(n_sequences),\n",
    "        'n_envs: {}'.format(n_envs),\n",
    "        'total_timesteps: {}'.format(df.shape),\n",
    "        f'unique_episodes: {unique_episodes.shape[0]}',\n",
    "        'device: {}'.format(device),\n",
    "        'tensorboard_dir: {}'.format(tensorboard_dir),\n",
    "        'checkpoint_dir: {}'.format(checkpoint_dir)\n",
    "    ]))\n",
    "    \n",
    "    model.learn(total_timesteps=100_000_000, progress_bar=True, log_interval=1000, callback=callback_list)\n",
    "    \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     args = parse_args()\n",
    "#     main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument:\n",
    "    read_path = 'rl_ready_data'\n",
    "    n_files = 2\n",
    "    n_sequences = 40\n",
    "    n_episodes = 500_000\n",
    "    \n",
    "    n_envs = 1000\n",
    "    lstm = None\n",
    "    device = 'cuda'\n",
    "    checkpoint_freq = 250_000\n",
    "    tb_log = 50\n",
    "    feature_extractor = 'cnn'\n",
    "    \n",
    "    window = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/28/2023 10:27:41 AM Starting Incentive Reinforcement Learning\n",
      "05/28/2023 10:27:41 AM Reading data from rl_ready_data/files_used_2/predicted_data.parquet\n",
      "05/28/2023 10:27:41 AM Setting up model with prediction None\n",
      "05/28/2023 10:27:42 AM N events:: 2566734 creating training partitions\n",
      "05/28/2023 10:27:42 AM N events after 70% split: 1796713\n",
      "05/28/2023 10:27:43 AM Convolution over 4 minute window\n",
      "05/28/2023 10:27:46 AM Convolving over 2 minute window complete: generating metadata\n",
      "05/28/2023 10:27:48 AM Metadata generated: selecting events only at 4 minute intervals\n",
      "05/28/2023 10:27:48 AM Data read: 449369 rows, 43 columns, dropping events within 2 minute window\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df= main(Argument)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>session_30_raw</th>\n",
       "      <th>cum_platform_time_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6632</th>\n",
       "      <td>2021-10-20 06:20:58</td>\n",
       "      <td>5.0</td>\n",
       "      <td>607.049988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6633</th>\n",
       "      <td>2021-10-20 06:24:59</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1057.283325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6634</th>\n",
       "      <td>2021-10-20 06:28:32</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1664.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8604</th>\n",
       "      <td>2021-10-20 08:40:55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13613</th>\n",
       "      <td>2021-10-20 11:32:49</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10920.633789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26589</th>\n",
       "      <td>2021-10-20 16:36:59</td>\n",
       "      <td>3.0</td>\n",
       "      <td>507.883331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29785</th>\n",
       "      <td>2021-10-20 17:36:19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>536.916687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33272</th>\n",
       "      <td>2021-10-20 18:48:53</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12610.633789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33611</th>\n",
       "      <td>2021-10-20 18:56:52</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1753.616699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33273</th>\n",
       "      <td>2021-10-20 18:56:59</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13241.400391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33274</th>\n",
       "      <td>2021-10-20 19:00:55</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14282.950195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33275</th>\n",
       "      <td>2021-10-20 19:04:41</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15679.933594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33612</th>\n",
       "      <td>2021-10-20 19:04:58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1874.716675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33613</th>\n",
       "      <td>2021-10-20 19:08:57</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3464.850098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33614</th>\n",
       "      <td>2021-10-20 19:12:59</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4914.033203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33615</th>\n",
       "      <td>2021-10-20 19:16:55</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6057.799805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33276</th>\n",
       "      <td>2021-10-20 19:20:39</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16722.900391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33616</th>\n",
       "      <td>2021-10-20 19:28:58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8333.766602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33277</th>\n",
       "      <td>2021-10-20 19:28:59</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18177.433594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33278</th>\n",
       "      <td>2021-10-20 19:32:58</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22828.300781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33617</th>\n",
       "      <td>2021-10-20 19:32:59</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10690.033203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33279</th>\n",
       "      <td>2021-10-20 19:36:58</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28224.833984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33280</th>\n",
       "      <td>2021-10-20 19:44:58</td>\n",
       "      <td>9.0</td>\n",
       "      <td>37760.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33281</th>\n",
       "      <td>2021-10-20 19:48:59</td>\n",
       "      <td>9.0</td>\n",
       "      <td>44846.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33282</th>\n",
       "      <td>2021-10-20 19:52:54</td>\n",
       "      <td>9.0</td>\n",
       "      <td>45985.285156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33283</th>\n",
       "      <td>2021-10-20 19:56:59</td>\n",
       "      <td>9.0</td>\n",
       "      <td>53335.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33284</th>\n",
       "      <td>2021-10-20 20:00:55</td>\n",
       "      <td>9.0</td>\n",
       "      <td>62006.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33285</th>\n",
       "      <td>2021-10-20 20:16:59</td>\n",
       "      <td>9.0</td>\n",
       "      <td>79867.164062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33286</th>\n",
       "      <td>2021-10-20 20:20:59</td>\n",
       "      <td>9.0</td>\n",
       "      <td>90330.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33287</th>\n",
       "      <td>2021-10-20 20:24:59</td>\n",
       "      <td>9.0</td>\n",
       "      <td>93680.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33288</th>\n",
       "      <td>2021-10-20 20:28:48</td>\n",
       "      <td>9.0</td>\n",
       "      <td>103521.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33289</th>\n",
       "      <td>2021-10-20 20:32:53</td>\n",
       "      <td>9.0</td>\n",
       "      <td>116689.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33290</th>\n",
       "      <td>2021-10-20 20:36:26</td>\n",
       "      <td>9.0</td>\n",
       "      <td>126762.101562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date_time  session_30_raw  cum_platform_time_raw\n",
       "6632  2021-10-20 06:20:58             5.0             607.049988\n",
       "6633  2021-10-20 06:24:59             5.0            1057.283325\n",
       "6634  2021-10-20 06:28:32             5.0            1664.750000\n",
       "8604  2021-10-20 08:40:55             1.0               1.633333\n",
       "13613 2021-10-20 11:32:49             7.0           10920.633789\n",
       "26589 2021-10-20 16:36:59             3.0             507.883331\n",
       "29785 2021-10-20 17:36:19             4.0             536.916687\n",
       "33272 2021-10-20 18:48:53             9.0           12610.633789\n",
       "33611 2021-10-20 18:56:52             6.0            1753.616699\n",
       "33273 2021-10-20 18:56:59             9.0           13241.400391\n",
       "33274 2021-10-20 19:00:55             9.0           14282.950195\n",
       "33275 2021-10-20 19:04:41             9.0           15679.933594\n",
       "33612 2021-10-20 19:04:58             6.0            1874.716675\n",
       "33613 2021-10-20 19:08:57             6.0            3464.850098\n",
       "33614 2021-10-20 19:12:59             6.0            4914.033203\n",
       "33615 2021-10-20 19:16:55             6.0            6057.799805\n",
       "33276 2021-10-20 19:20:39             9.0           16722.900391\n",
       "33616 2021-10-20 19:28:58             6.0            8333.766602\n",
       "33277 2021-10-20 19:28:59             9.0           18177.433594\n",
       "33278 2021-10-20 19:32:58             9.0           22828.300781\n",
       "33617 2021-10-20 19:32:59             6.0           10690.033203\n",
       "33279 2021-10-20 19:36:58             9.0           28224.833984\n",
       "33280 2021-10-20 19:44:58             9.0           37760.898438\n",
       "33281 2021-10-20 19:48:59             9.0           44846.265625\n",
       "33282 2021-10-20 19:52:54             9.0           45985.285156\n",
       "33283 2021-10-20 19:56:59             9.0           53335.765625\n",
       "33284 2021-10-20 20:00:55             9.0           62006.035156\n",
       "33285 2021-10-20 20:16:59             9.0           79867.164062\n",
       "33286 2021-10-20 20:20:59             9.0           90330.031250\n",
       "33287 2021-10-20 20:24:59             9.0           93680.234375\n",
       "33288 2021-10-20 20:28:48             9.0          103521.968750\n",
       "33289 2021-10-20 20:32:53             9.0          116689.734375\n",
       "33290 2021-10-20 20:36:26             9.0          126762.101562"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['user_id'] == 0.0][['date_time', 'session_30_raw', 'cum_platform_time_raw']].sort_values(by=['date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_180/1934561964.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moriginal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'session_30_raw'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cum_platform_time_raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'original' is not defined"
     ]
    }
   ],
   "source": [
    "original[original['user_id'] == 0.0][['date_time', 'session_30_raw', 'cum_platform_time_raw']].sort_values(by=['date_time'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
