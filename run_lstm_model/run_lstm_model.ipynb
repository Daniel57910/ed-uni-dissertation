{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    }
   ],
   "source": [
    "!python --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113 --quiet \n",
    "!python -m pip install pytorch-lightning==1.8.6 python-dotenv fsspec[\"s3\"] boto3 s3fs==2022.11.0 tensorboard tensorflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET = 's3://dissertation-data-dmiller'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time\",\n",
    "    \n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load npz_extractor.py\n",
    "\n",
    "class NPZExtractor:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, input_path, n_files, n_sequences, s3_client, data_partition) -> None:\n",
    "        self.input_path = input_path\n",
    "        self.n_files = n_files\n",
    "        self.n_sequences = n_sequences\n",
    "        self.s3_client = s3_client\n",
    "        self.data_partition = data_partition\n",
    "\n",
    "\n",
    "    def get_dataset_pointer(self):\n",
    "\n",
    "        read_path = os.path.join(self.input_path, f'files_used_{self.n_files}')\n",
    "        if not os.path.exists(read_path):\n",
    "            self.logger.info(f'Creating directory: {read_path}')\n",
    "            os.makedirs(read_path)\n",
    "\n",
    "\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            key_zip, key_npy = (\n",
    "                os.path.join(read_path, f'sequence_index_{_}.npz'),\n",
    "                os.path.join(read_path, f'sequence_index_{_}')\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f'Loading pointer to dataset: {key_npy}: derived from {key_zip}')\n",
    "\n",
    "\n",
    "            if not os.path.exists(key_zip):\n",
    "                self.logger.info(f'Zip file to extract: {key_zip}: npy file to load: {key_npy}')\n",
    "                self.s3_client.download_file(\n",
    "                    'dissertation-data-dmiller',\n",
    "                    key_zip,\n",
    "                    key_zip\n",
    "                )\n",
    "            if not os.path.exists(key_npy):\n",
    "                self.logger.info(f'Zip file downloaded: {key_zip}: npy file to load: {key_npy}')\n",
    "\n",
    "                self._zip_extract(key_zip, key_npy)\n",
    "\n",
    "        lz_concatenated_results = self._lazy_concatenate()\n",
    "\n",
    "        if self.data_partition:\n",
    "            return [p[:self.data_partition] for p in lz_concatenated_results]\n",
    "        else:\n",
    "            return lz_concatenated_results\n",
    "\n",
    "\n",
    "    def _zip_extract(self, key_zip, key_npy):\n",
    "        self.logger.info(f'Extracting file: {key_zip} -> {key_npy}')\n",
    "\n",
    "        with zipfile.ZipFile(key_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=key_npy, members=['arr_0.npy'])\n",
    "\n",
    "        self.logger.info(f'Zip file exracted: {key_zip} -> {key_npy}/arr_0.npy')\n",
    "\n",
    "    def _lazy_concatenate(self):\n",
    "        lz_concat = []\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            path_to_load = os.path.join(self.input_path, f'files_used_{self.n_files}', f'sequence_index_{_}', f'arr_0.npy')\n",
    "            self.logger.info(f'Loading: {path_to_load}')\n",
    "            lz_concat.append(np.load(path_to_load, mmap_mode='r'))\n",
    "        return lz_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data_module.py\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "LABEL_INDEX = 1\n",
    "TOTAL_EVENTS_INDEX = 2\n",
    "BATCHES = 1000000\n",
    "\n",
    "class ClickstreamDataModule(LightningDataModule):\n",
    "    def __init__(self, pointer_list, batch_size, n_sequences) -> None:\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        train_index = int(pointer_list[0].shape[0] * 0.7)\n",
    "        val_index = int(pointer_list[0].shape[0] * 0.85)\n",
    "        self.training_data = [p[:train_index] for p in pointer_list]\n",
    "        self.validation_data = [p[train_index:val_index] for p in pointer_list]\n",
    "        self.test_data = [p[val_index:] for p in pointer_list]\n",
    "        self.n_sequences = n_sequences\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = ClickstreamDataset(self.training_data, self.n_sequences)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = ClickstreamDataset(self.validation_data, self.n_sequences)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataset = ClickstreamDataset(self.test_data, self.n_sequences)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    \n",
    "class ClickstreamDataset(Dataset):\n",
    "    def __init__(self, dataset_pointer_list, n_sequences) -> None:\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Yield data in batches of BATCHES\n",
    "        \"\"\"\n",
    "        self.pointer_list = dataset_pointer_list\n",
    "        self.n_sequences = n_sequences\n",
    "        self.total_events = self.pointer_list[0].shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        result = [np.array(i[idx]) for i in self.pointer_list]\n",
    "        return np.concatenate(result)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model_base.py\n",
    "\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import Precision\n",
    "from torchmetrics import Recall\n",
    "METADATA_INDEX = 14\n",
    "\n",
    "class ModelBase(LightningModule):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.runtime_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.train_accuracy = Accuracy(task='binary', threshold=0.5)\n",
    "        self.valid_accuracy = Accuracy(task='binary', threshold=0.5)\n",
    "\n",
    "        self.train_precision = Precision(task='binary', threshold=0.5)\n",
    "        self.valid_precision = Precision(task='binary', threshold=0.5)\n",
    "\n",
    "        self.train_recall = Recall(task='binary', threshold=0.5)\n",
    "        self.valid_recall = Recall(task='binary', threshold=0.5)\n",
    "\n",
    "        self = self.to(self.runtime_device)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc, prec, rec = self._run_step(batch, 'train')\n",
    "\n",
    "        self.log(\n",
    "            'loss/train',\n",
    "            loss,\n",
    "            logger=True,\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "        )\n",
    "\n",
    "        self.log(\n",
    "            'loss_train',\n",
    "            loss,\n",
    "            logger=False,\n",
    "            prog_bar=False,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            sync_dist=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"acc\": acc,\n",
    "            \"prec\": prec,\n",
    "            \"rec\": rec\n",
    "        }\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc, prec, rec = self._run_step(batch, 'valid')\n",
    "\n",
    "        self.log(\n",
    "            'loss/valid',\n",
    "            loss,\n",
    "            logger=True,\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "        )\n",
    "\n",
    "        self.log(\n",
    "            'loss_valid',\n",
    "            loss,\n",
    "            logger=False,\n",
    "            prog_bar=False,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            sync_dist=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"acc\": acc,\n",
    "            \"prec\": prec,\n",
    "            \"rec\": rec\n",
    "        }\n",
    "\n",
    "\n",
    "    def _run_step(self, batch, type):\n",
    "        \n",
    "    \n",
    "\n",
    "        metadata, features  = self._extract_features(batch)\n",
    "        y = metadata[:, 0].unsqueeze(1)\n",
    "\n",
    "        if 'ordinal' in self.model_name:\n",
    "            y_hat = self(features)\n",
    "        \n",
    "        # if 'embedded' in self.model_name:\n",
    "        #     concatenated = torch.cat((ordinal_features, categorical_features), dim=2)\n",
    "        #     assert concatenated.shape == (batch.shape[0], self.n_sequences, 19), 'concatenated shape is wrong'\n",
    "        #     y_hat = self(torch.cat((ordinal_features, categorical_features), dim=2))\n",
    "\n",
    "        # if self.zero_heuristic:\n",
    "        #     y_hat = torch.where(total_events <= ZERO_HEURISTIC_RATE, torch.zeros_like(y_hat), y_hat)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        if 'train' in type:\n",
    "            acc = self.train_accuracy(y_hat, y)\n",
    "            prec = self.train_precision(y_hat, y)\n",
    "            rec = self.train_recall(y_hat, y)\n",
    "\n",
    "        else:\n",
    "            acc = self.valid_accuracy(y_hat, y)\n",
    "            prec = self.valid_precision(y_hat, y)\n",
    "            rec = self.valid_recall(y_hat, y)\n",
    "\n",
    "        return loss, acc, prec, rec\n",
    "\n",
    "    def _extract_features(self, tensor):\n",
    "        \n",
    "        metadata, features = tensor[:, :METADATA_INDEX], tensor[:, METADATA_INDEX:] \n",
    "                \n",
    "        features = torch.flip(\n",
    "            torch.reshape(features, (features.shape[0], self.n_sequences, self.n_features)),\n",
    "            dims=[1]\n",
    "        )\n",
    "        \n",
    "        return metadata, features[:, -1, :]\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        acc, prec, rec, loss = (\n",
    "            torch.stack([out['acc'] for out in outputs]),\n",
    "            torch.stack([out['prec'] for out in outputs]),\n",
    "            torch.stack([out['rec'] for out in outputs]),\n",
    "            torch.stack([out['loss'] for out in outputs])\n",
    "        )\n",
    "\n",
    "        acc, prec, rec, loss = (\n",
    "            torch.mean(acc),\n",
    "            torch.mean(prec),\n",
    "            torch.mean(rec),\n",
    "            torch.mean(loss)\n",
    "        )\n",
    "\n",
    "        self.logger.experiment.add_scalar('acc/train', acc, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('prec/train', prec, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('rec/train', rec, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('loss_e/train', loss, self.current_epoch)\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        acc, prec, rec, loss = (\n",
    "            torch.stack([out['acc'] for out in outputs]),\n",
    "            torch.stack([out['prec'] for out in outputs]),\n",
    "            torch.stack([out['rec'] for out in outputs]),\n",
    "            torch.stack([out['loss'] for out in outputs])\n",
    "        )\n",
    "\n",
    "        acc, prec, rec, loss = (\n",
    "            torch.mean(acc),\n",
    "            torch.mean(prec),\n",
    "            torch.mean(rec),\n",
    "            torch.mean(loss)\n",
    "        )\n",
    "\n",
    "        self.logger.experiment.add_scalar('acc/valid', acc, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('prec/valid', prec, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('rec/valid', rec, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('loss_e/valid', loss, self.current_epoch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # equation for adam optimizer\n",
    "        \"\"\"\n",
    "        m_t = beta_1 * m_{t-1} + (1 - beta_1) * g_t\n",
    "        v_t = beta_2 * v_{t-1} + (1 - beta_2) * g_t^2\n",
    "        m_cap = m_t / (1 - beta_1^t)\n",
    "        v_cap = v_t / (1 - beta_2^t)\n",
    "        w_t = w_{t-1} - lr * m_cap / (sqrt(v_cap) + eps)\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model_protos.py\n",
    "# %load model_protos.py\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "ORDINAL_FEATURE_INDEX = 17\n",
    "\n",
    "\n",
    "class LSTMOrdinal(ModelBase):\n",
    "    def __init__(self, n_features, n_seqeuences, hidden_size=32, dropout=0.2, lr=0.01, batch_size=256, zero_heuristic=False) -> None:\n",
    "        self.n_features = n_features\n",
    "        self.n_sequences = n_seqeuences\n",
    "        self.zero_heuristic = zero_heuristic\n",
    "        self.learning_rate = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = 'ordinal'\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.output = nn.Linear(\n",
    "            hidden_size,\n",
    "            1\n",
    "        )\n",
    "\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1]\n",
    "        return self.output(x)\n",
    "\n",
    "class LSTMEmbedUserProject(ModelBase):\n",
    "    def __init__(self, n_features, n_sequences, embedding_matrix, hidden_size=32, dropout=0.2, lr=0.01, batch_size=256, zero_heuristic=False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ordinal = nn.LSTM(\n",
    "            input_size=n_features - 3,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.user_embedding = nn.Embedding(\n",
    "            num_embeddings=embedding_matrix['user_id'][0] + 1,\n",
    "            embedding_dim=embedding_matrix['user_id'][1],\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.project_embedding = nn.Embedding(\n",
    "            num_embeddings=embedding_matrix['project_id'][0] + 1,\n",
    "            embedding_dim=embedding_matrix['project_id'][1],\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.lstm_embedding = nn.LSTM(\n",
    "            input_size=embedding_matrix['user_id'][1] + embedding_matrix['project_id'][1],\n",
    "            hidden_size=(embedding_matrix['user_id'][1] + embedding_matrix['project_id'][1]) // 2,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.learning_rate = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = 'embedded'\n",
    "\n",
    "        trunk_linear = hidden_size + ((embedding_matrix['user_id'][1] + embedding_matrix['project_id'][1]) // 3)\n",
    "\n",
    "        self.out_trunk = nn.Sequential(\n",
    "            nn.Linear(trunk_linear, 20),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 1)\n",
    "        )\n",
    "\n",
    "        self.n_sequences = n_sequences\n",
    "        self.zero_heuristic = zero_heuristic\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ordinal_features, categorical_features = x[:, :, :ORDINAL_FEATURE_INDEX], x[:, :, ORDINAL_FEATURE_INDEX:].int()\n",
    "\n",
    "\n",
    "        user, project = categorical_features[:, :, 0], categorical_features[:, :, 1]\n",
    "\n",
    "        \"\"\"\n",
    "        assert no more than 2 unique users: padded and value\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        user, project = self.user_embedding(user), self.project_embedding(project)\n",
    "        categorical_features = torch.cat((user, project), dim=2)\n",
    "        ordinal_out = self.lstm_ordinal(ordinal_features)\n",
    "        categorical_out = self.lstm_embedding(categorical_features)\n",
    "        return self.out_trunk(torch.cat((ordinal_out[0][:, -1], categorical_out[0][:, -1]), dim=1))\n",
    "    \n",
    "\n",
    "class LSTMEmbedUser(ModelBase):\n",
    "    def __init__(self, n_features, n_sequences, embedding_matrix, hidden_size=32, dropout=0.2, lr=0.01, batch_size=256, zero_heuristic=False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ordinal = nn.LSTM(\n",
    "            input_size=n_features - 3,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.user_embedding = nn.Embedding(\n",
    "            num_embeddings=embedding_matrix['user_id'][0] + 1,\n",
    "            embedding_dim=embedding_matrix['user_id'][1],\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.lstm_embedding = nn.LSTM(\n",
    "            input_size=embedding_matrix['user_id'][1],\n",
    "            hidden_size = max(3, (embedding_matrix['user_id'][1]) // 3),\n",
    "            # hidden_size=(embedding_matrix['user_id'][1]) // 3,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "\n",
    "        self.learning_rate = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = 'embedded_user'\n",
    "\n",
    "        trunk_linear = hidden_size + max(((embedding_matrix['user_id'][1]) // 3), 3)\n",
    "\n",
    "        self.out_trunk = nn.Sequential(\n",
    "            nn.Linear(trunk_linear, 20),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 1)\n",
    "        )\n",
    "\n",
    "        self.n_sequences = n_sequences\n",
    "        self.zero_heuristic = zero_heuristic\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ordinal_features, categorical_features = x[:, :, :ORDINAL_FEATURE_INDEX], x[:, :, ORDINAL_FEATURE_INDEX:].int()\n",
    "        user = categorical_features[:, :, 0]\n",
    "        user = self.user_embedding(user)\n",
    "        ordinal_out = self.lstm_ordinal(ordinal_features)\n",
    "        user_out = self.lstm_embedding(user)\n",
    "        return self.out_trunk(torch.cat((ordinal_out[0][:, -1], user_out[0][:, -1]), dim=1))\n",
    "\n",
    "\n",
    "class LSTMEmbedOneLSTM(ModelBase):\n",
    "    def __init__(self, n_features, n_sequences, embedding_matrix, hidden_size=32, dropout=0.2, lr=0.01, batch_size=256, zero_heuristic=False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_combined = nn.LSTM(\n",
    "            input_size=(n_features - 3) + embedding_matrix['user_id'][1],\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.user_embedding = nn.Embedding(\n",
    "            num_embeddings=embedding_matrix['user_id'][0] + 1,\n",
    "            embedding_dim=embedding_matrix['user_id'][1],\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.learning_rate = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = 'embedded_one_lstm'\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.n_sequences = n_sequences\n",
    "        self.zero_heuristic = zero_heuristic\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ordinal_features, categorical_features = x[:, :, :ORDINAL_FEATURE_INDEX], x[:, :, ORDINAL_FEATURE_INDEX:].int()\n",
    "        user = categorical_features[:, :, 0]\n",
    "        user = self.user_embedding(user)\n",
    "        features = torch.cat((ordinal_features, user), dim=2)\n",
    "        out = self.lstm_combined(features)\n",
    "        return self.out(out[0][:, -1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load run_lstm_model.py\n",
    "# %load run_lstm_model.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import json\n",
    "import pdb\n",
    "S3_BUCKET = 's3://dissertation-data-dmiller'\n",
    "SNS_TOPIC = 'arn:aws:sns:eu-west-1:774141665752:gradient-task'\n",
    "\n",
    "USER_INDEX = 9\n",
    "PROJECT_INDEX = 10\n",
    "COUNTRY_INDEX = 11\n",
    "\n",
    "torch.set_printoptions(precision=3, linewidth=400, sci_mode=False)\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=400)\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\"\"\"\n",
    "Embedding dim based on cube root of number of unique values\n",
    "\"\"\"\n",
    "\n",
    "EMBEDDING_DIMS = {\n",
    "    '5': {\n",
    "        'user_id': (17891, 2),\n",
    "        'project_id': (328, 1),\n",
    "    },\n",
    "    '30': {\n",
    "        'user_id': (60459 ,int(60459**0.25)),\n",
    "        'project_id': (617. , int(617**0.25)),\n",
    "    },\n",
    "    '45': {\n",
    "        'user_id': (85663, int(85663**0.25)),\n",
    "        'project_id': (757, int(757**0.25)),\n",
    "    },\n",
    "    '61': {\n",
    "        'user_id': (104744, 18),\n",
    "        'project_id': (846, 6),\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def setup_logging():\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def _device_count():\n",
    "    if 'ipykernel' in sys.modules: return 1\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.device_count()\n",
    "\n",
    "    return 1\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_type', type=str, default='ordinal')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001)\n",
    "\n",
    "    parser.add_argument('--n_workers', type=int, default=8)\n",
    "    parser.add_argument('--n_epochs', type=int, default=1)\n",
    "\n",
    "    parser.add_argument('--hidden_size', type=int, default=32)\n",
    "    parser.add_argument('--dropout', type=float, default=0.2)\n",
    "    parser.add_argument('--n_sequences', type=int, default=10)\n",
    "    parser.add_argument('--n_features', type=int, default=18)\n",
    "\n",
    "    parser.add_argument('--data_input_path', type=str, default='datasets/torch_ready_data_5')\n",
    "    parser.add_argument('--data_partition', type=int, default=1000)\n",
    "\n",
    "    parser.add_argument('--n_files', type=str, default='5')\n",
    "\n",
    "    parser.add_argument('--progress_bar', type=bool, default=True)\n",
    "    parser.add_argument('--checkpoint', type=str, default=None)\n",
    "\n",
    "    parser.add_argument('--find_hparams', type=bool, default=False)\n",
    "\n",
    "    parser.add_argument('--zero_heuristic', type=bool, default=False)\n",
    "    parser.add_argument('--validate_only', type=bool, default=False)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    logger,\n",
    "    model_type,\n",
    "    n_features,\n",
    "    n_sequences,\n",
    "    hidden_size,\n",
    "    dropout,\n",
    "    learning_rate,\n",
    "    batch_size,\n",
    "    zero_heuristic,\n",
    "    n_files\n",
    "):\n",
    "    if model_type == 'ordinal':\n",
    "        logger.info('Creating LSTMOrdinal model')\n",
    "        return LSTMOrdinal(\n",
    "            n_features,\n",
    "            n_sequences,\n",
    "            hidden_size,\n",
    "            dropout,\n",
    "            learning_rate,\n",
    "            batch_size,\n",
    "            zero_heuristic\n",
    "        )\n",
    "    elif model_type == 'embed_user_project':\n",
    "        return LSTMEmbedUserProject(\n",
    "            n_features,\n",
    "            n_sequences,\n",
    "            EMBEDDING_DIMS[n_files],\n",
    "            hidden_size,\n",
    "            dropout,\n",
    "            learning_rate,\n",
    "            batch_size,\n",
    "        )\n",
    "    elif model_type == 'embed_user':\n",
    "        return LSTMEmbedUser(\n",
    "            n_features,\n",
    "            n_sequences,\n",
    "            EMBEDDING_DIMS[n_files],\n",
    "            hidden_size,\n",
    "            dropout,\n",
    "            learning_rate,\n",
    "            batch_size,\n",
    "        )\n",
    "    else:\n",
    "        return LSTMEmbedOneLSTM(\n",
    "            n_features,\n",
    "            n_sequences,\n",
    "            EMBEDDING_DIMS[n_files],\n",
    "            hidden_size,\n",
    "            dropout,\n",
    "            learning_rate,\n",
    "            batch_size,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    date_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "\n",
    "    logger = setup_logging()\n",
    "    date_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "    logger.info(f'Running experiment at {date_time}')\n",
    "\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "        aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "    )\n",
    "    \n",
    "    npz_extractor = NPZExtractor(\n",
    "        args.data_input_path,\n",
    "        args.n_files,\n",
    "        args.n_sequences,\n",
    "        s3_client,\n",
    "        args.data_partition)\n",
    "    \n",
    "    clickstream_data_loader = ClickstreamDataModule(npz_extractor.get_dataset_pointer(), args.batch_size, args.n_sequences + 1)\n",
    "    model = get_model(\n",
    "        logger,\n",
    "        args.model_type,\n",
    "        args.n_features,\n",
    "        args.n_sequences + 1,\n",
    "        args.hidden_size,\n",
    "        args.dropout,\n",
    "        args.learning_rate,\n",
    "        args.batch_size,\n",
    "        args.zero_heuristic,\n",
    "        args.n_files\n",
    "    )\n",
    "    \n",
    "    data_version = \"1\"\n",
    "    checkpoint_path = os.path.join(\n",
    "        S3_BUCKET,\n",
    "        'lstm_experiments',\n",
    "        'checkpoints',\n",
    "        f'data_v{data_version}',\n",
    "        f'n_files_{str(args.n_files)}',\n",
    "        args.model_type,\n",
    "        f'sequence_length_1',\n",
    "        f'data_partition_{str(args.data_partition)}',\n",
    "        date_time)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        monitor='loss_valid',\n",
    "        dirpath=checkpoint_path,\n",
    "        filename='clickstream-{epoch:02d}-{loss_valid:.2f}',\n",
    "        every_n_epochs=2,\n",
    "        save_top_k=3\n",
    "    )\n",
    "\n",
    "    callbacks = [checkpoint]\n",
    "    if args.progress_bar:\n",
    "        progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "        callbacks += [progress_bar]\n",
    "\n",
    "    metric_logger = TensorBoardLogger(\n",
    "        save_dir=f's3://dissertation-data-dmiller/lstm_experiments/results/data_v{data_version}/n_files_{args.n_files}/{args.model_type}',\n",
    "        name=f'sequence_length_1/data_partition_{args.data_partition}/{date_time}',\n",
    "        flush_secs=60,\n",
    "        log_graph=True,\n",
    "    )\n",
    "\n",
    "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    device_count = _device_count()\n",
    "    strategy = 'ddp' if 'ipykernel' not in sys.modules else None\n",
    "\n",
    "    config = \"\\n\".join([\n",
    "        f'data input path: {args.data_input_path}',\n",
    "        f'data partition: {args.data_partition}',\n",
    "        f'batch_size: {args.batch_size}',\n",
    "        f'n_epoch: {args.n_epochs}',\n",
    "        f'n_workers: 8',\n",
    "        f'device: {model.runtime_device}',\n",
    "        f'train_samples: {clickstream_data_loader.training_data[0].shape[0]}',\n",
    "        f'val_samples: {clickstream_data_loader.validation_data[0].shape[0]}',\n",
    "        f'hidden size: {args.hidden_size}',\n",
    "        f'dropout: {args.dropout}',\n",
    "        f'n_sequences: {args.n_sequences}',\n",
    "        f'n_features: {args.n_features}',\n",
    "        f'learning_rate: {args.learning_rate}',\n",
    "        f'accelerator: {accelerator}',\n",
    "        f'device_count: {device_count}',\n",
    "        f'strategy: {strategy}',\n",
    "        f'model_type: {args.model_type}',\n",
    "        f'zero_heuristic: {args.zero_heuristic}',\n",
    "    ])\n",
    "\n",
    "\n",
    "    logger.info(f'Beginning validation:\\n {config}')\n",
    "    logger.info(f'log_path=\\n tensorboard --logdir {metric_logger.save_dir}/{metric_logger.name}/version_0')\n",
    "    logger.info(f'checkpoint_path=\\n {checkpoint_path}')\n",
    "    trainer = Trainer(\n",
    "        precision=16,\n",
    "        check_val_every_n_epoch=1,\n",
    "        accelerator=accelerator,\n",
    "        strategy=None,\n",
    "        devices=device_count,\n",
    "        max_epochs=args.n_epochs,\n",
    "        callbacks=callbacks,\n",
    "        logger=metric_logger,\n",
    "        enable_progress_bar=args.progress_bar,\n",
    "        log_every_n_steps=500\n",
    "        )\n",
    "    \n",
    "    if args.checkpoint:\n",
    "        checkpoint_s3_path = os.path.join('s3://dissertation-data-dmiller', args.checkpoint)\n",
    "        logger.info(f'Running model from checkpoint: {checkpoint_s3_path}')\n",
    "        trainer.fit(model, ckpt_path=checkpoint_s3_path, datamodule=clickstream_data_loader)\n",
    "    else:\n",
    "        logger.info('Running model from scratch')\n",
    "        trainer.fit(model, datamodule=clickstream_data_loader) \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "\n",
    "    model_type = 'ordinal'\n",
    "    batch_size = 4096\n",
    "    learning_rate = 0.01\n",
    "    n_workers = 8\n",
    "    n_epochs = 100\n",
    "    hidden_size = 32\n",
    "    dropout = .2\n",
    "    n_sequences = 10\n",
    "    n_features = 20\n",
    "    data_input_path = 'torch_ready_data'\n",
    "    data_partition = None\n",
    "    n_files = 30\n",
    "    progress_bar = True\n",
    "    checkpoint = None\n",
    "    find_hparams = False\n",
    "    zero_heuristic = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 16:09:11,625 - __main__ - INFO - Running experiment at 2023_05_02_16_09\n",
      "2023-05-02 16:09:11,673 - __main__ - INFO - Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_0: derived from torch_ready_data/files_used_30/sequence_index_0.npz\n",
      "2023-05-02 16:09:11,674 - __main__ - INFO - Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_10: derived from torch_ready_data/files_used_30/sequence_index_10.npz\n",
      "2023-05-02 16:09:11,674 - __main__ - INFO - Loading: torch_ready_data/files_used_30/sequence_index_0/arr_0.npy\n",
      "2023-05-02 16:09:11,680 - __main__ - INFO - Loading: torch_ready_data/files_used_30/sequence_index_10/arr_0.npy\n",
      "2023-05-02 16:09:11,685 - __main__ - INFO - Creating LSTMOrdinal model\n",
      "2023-05-02 16:09:11,690 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpt78m_9wl\n",
      "2023-05-02 16:09:11,691 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpt78m_9wl/_remote_module_non_sriptable.py\n",
      "2023-05-02 16:09:16,321 - __main__ - INFO - Beginning validation:\n",
      " data input path: torch_ready_data\n",
      "data partition: None\n",
      "batch_size: 4096\n",
      "n_epoch: 100\n",
      "n_workers: 8\n",
      "device: cuda\n",
      "train_samples: 26950693\n",
      "val_samples: 5775148\n",
      "hidden size: 32\n",
      "dropout: 0.2\n",
      "n_sequences: 10\n",
      "n_features: 20\n",
      "learning_rate: 0.01\n",
      "accelerator: gpu\n",
      "device_count: 1\n",
      "strategy: None\n",
      "model_type: ordinal\n",
      "zero_heuristic: False\n",
      "2023-05-02 16:09:16,322 - __main__ - INFO - log_path=\n",
      " tensorboard --logdir s3://dissertation-data-dmiller/lstm_experiments/results/data_v1/n_files_30/ordinal/sequence_length_1/data_partition_None/2023_05_02_16_09/version_0\n",
      "2023-05-02 16:09:16,323 - __main__ - INFO - checkpoint_path=\n",
      " s3://dissertation-data-dmiller/lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_1/data_partition_None/2023_05_02_16_09\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-05-02 16:09:16,482 - __main__ - INFO - Running model from scratch\n",
      "2023-05-02 16:09:16,503 - aiobotocore.credentials - INFO - Found credentials in environment variables.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | loss            | BCEWithLogitsLoss | 0     \n",
      "1 | train_accuracy  | BinaryAccuracy    | 0     \n",
      "2 | valid_accuracy  | BinaryAccuracy    | 0     \n",
      "3 | train_precision | BinaryPrecision   | 0     \n",
      "4 | valid_precision | BinaryPrecision   | 0     \n",
      "5 | train_recall    | BinaryRecall      | 0     \n",
      "6 | valid_recall    | BinaryRecall      | 0     \n",
      "7 | lstm            | LSTM              | 15.4 K\n",
      "8 | output          | Linear            | 33    \n",
      "------------------------------------------------------\n",
      "15.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.4 K    Total params\n",
      "0.031     Total estimated model params size (MB)\n",
      "/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/loggers/tensorboard.py:261: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4767dff890904c2a9984fef49b532194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 16:09:18,233 - botocore.credentials - INFO - Found credentials in environment variables.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4096 and 32x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_199/1279081131.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_199/2824259917.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running model from scratch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclickstream_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"`Trainer.fit()` requires a `LightningModule`, got: {model.__class__.__qualname__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         )\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;31m# enable train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1260\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m                 \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataloader_idx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mdl_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# store batch level output per dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[1;32m    233\u001b[0m         \u001b[0mhook_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_step\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidationStep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_199/2766968509.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         self.log(\n",
      "\u001b[0;32m/tmp/ipykernel_199/2766968509.py\u001b[0m in \u001b[0;36m_run_step\u001b[0;34m(self, batch, type)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'ordinal'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# if 'embedded' in self.model_name:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_199/3079595009.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLSTMEmbedUserProject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4096 and 32x1)"
     ]
    }
   ],
   "source": [
    "main(Arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1da7ff4cdcccf44e7e228c52b231f7d5c5854d5618af555ed3871fd5cba609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
