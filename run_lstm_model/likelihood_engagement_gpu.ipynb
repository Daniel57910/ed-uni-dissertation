{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch python-dotenv boto3 --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import boto3\n",
    "from pprint import pformat\n",
    "from torch import nn\n",
    "import io\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "LABEL = [\n",
    "    \"continue_work_session_30_minutes\"\n",
    "]\n",
    "\n",
    "METADATA = [\n",
    "    \"user_id\",\n",
    "    \"session_30_raw\",\n",
    "    \"cum_platform_event_raw\",\n",
    "    \"cum_platform_time_raw\",\n",
    "    \"cum_session_time_raw\",\n",
    "    \"global_events_user\",\n",
    "    \"global_session_time\",\n",
    "    \n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\"\n",
    "]\n",
    "\n",
    "OUT_FEATURE_COLUMNS = [\n",
    "    \"country_count\", \n",
    "    \"date_hour_sin\", \n",
    "    \"date_hour_cos\",\n",
    "    \"date_minute_sin\",\n",
    "    \"date_minute_cos\",\n",
    "    \n",
    "    \"session_30_count\",\n",
    "    \"session_5_count\",\n",
    "    \"cum_session_event_count\",\n",
    "    \"delta_last_event\",\n",
    "    \"cum_session_time\",\n",
    "    \n",
    "    \"expanding_click_average\",\n",
    "    \"cum_platform_time\",\n",
    "    \"cum_platform_events\",\n",
    "    \"cum_projects\",\n",
    "    \"average_event_time\",\n",
    "    \n",
    "    \"rolling_session_time\",\n",
    "    \"rolling_session_events\",\n",
    "    \"rolling_session_gap\",\n",
    "    \"previous_session_time\",\n",
    "    \"previous_session_events\",\n",
    "]\n",
    "\n",
    "\n",
    "GROUPBY_COLS = ['user_id']\n",
    "\n",
    "LOAD_COLS = LABEL + METADATA + OUT_FEATURE_COLUMNS\n",
    "\n",
    "S3_BUCKET = 'dissertation-data-dmiller'\n",
    "BASE_CHECK_PATH = 'lstm_experiments/checkpoints/data_v1/n_files_30/ordinal'\n",
    "\n",
    "\n",
    "LSTM_CHECKPOINTS = {\n",
    "    'seq_10': os.path.join(BASE_CHECK_PATH, 'sequence_length_10', 'data_partition_None', '2023_04_28_20_16/clickstream-epoch=63-loss_valid=0.59.ckpt'),\n",
    "    'seq_20': os.path.join(BASE_CHECK_PATH, 'sequence_length_20', 'data_partition_None', '2023_04_29_11_43/clickstream-epoch=85-loss_valid=0.59.ckpt'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data_module\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "LABEL_INDEX = 1\n",
    "TOTAL_EVENTS_INDEX = 2\n",
    "BATCHES = 1000000\n",
    "\n",
    "    \n",
    "class ClickstreamDataset(Dataset):\n",
    "    def __init__(self, dataset_pointer_list) -> None:\n",
    "        \"\"\"\n",
    "        Yield data in batches of BATCHES\n",
    "        \"\"\"\n",
    "        self.events = dataset_pointer_list\n",
    "        self.size = self.events[0].shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        events = [np.array([event[idx]]) for event in self.events]\n",
    "        return np.concatenate(events, axis=1)\n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load torch_model_bases\n",
    "import torch \n",
    "from torch import nn\n",
    "N_FEATURES = 20\n",
    "class LSTMOrdinal(nn.Module):\n",
    "    def __init__(self,  hidden_size=32, dropout=0.2) -> None:\n",
    "        super(LSTMOrdinal, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=N_FEATURES,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.output = nn.Linear(\n",
    "            hidden_size,\n",
    "            1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1]\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load npz_extractor\n",
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "class NPZExtractor:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def __init__(self, input_path, n_files, n_sequences, s3_client, data_partition) -> None:\n",
    "        self.input_path = input_path\n",
    "        self.n_files = n_files\n",
    "        self.n_sequences = n_sequences\n",
    "        self.s3_client = s3_client\n",
    "        self.data_partition = data_partition\n",
    "\n",
    "\n",
    "    def get_dataset_pointer(self):\n",
    "\n",
    "        read_path = os.path.join(self.input_path, f'files_used_{self.n_files}')\n",
    "        if not os.path.exists(read_path):\n",
    "            self.logger.info(f'Creating directory: {read_path}')\n",
    "            os.makedirs(read_path)\n",
    "\n",
    "\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            key_zip, key_npy = (\n",
    "                os.path.join(read_path, f'sequence_index_{_}.npz'),\n",
    "                os.path.join(read_path, f'sequence_index_{_}')\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f'Loading pointer to dataset: {key_npy}: derived from {key_zip}')\n",
    "\n",
    "\n",
    "            if not os.path.exists(key_zip):\n",
    "                self.logger.info(f'Zip file to extract: {key_zip}: npy file to load: {key_npy}')\n",
    "                self.s3_client.download_file(\n",
    "                    'dissertation-data-dmiller',\n",
    "                    key_zip,\n",
    "                    key_zip\n",
    "                )\n",
    "            if not os.path.exists(key_npy):\n",
    "                self.logger.info(f'Zip file downloaded: {key_zip}: npy file to load: {key_npy}')\n",
    "\n",
    "                self._zip_extract(key_zip, key_npy)\n",
    "\n",
    "        lz_concatenated_results = self._lazy_concatenate()\n",
    "\n",
    "        if self.data_partition:\n",
    "            return [p[:self.data_partition] for p in lz_concatenated_results]\n",
    "        else:\n",
    "            return lz_concatenated_results\n",
    "\n",
    "\n",
    "    def _zip_extract(self, key_zip, key_npy):\n",
    "        self.logger.info(f'Extracting file: {key_zip} -> {key_npy}')\n",
    "\n",
    "        with zipfile.ZipFile(key_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=key_npy, members=['arr_0.npy'])\n",
    "\n",
    "        self.logger.info(f'Zip file exracted: {key_zip} -> {key_npy}/arr_0.npy')\n",
    "\n",
    "    def _lazy_concatenate(self):\n",
    "        lz_concat = []\n",
    "        for _ in range(0, self.n_sequences +1, 10):\n",
    "            path_to_load = os.path.join(self.input_path, f'files_used_{self.n_files}', f'sequence_index_{_}', f'arr_0.npy')\n",
    "            self.logger.info(f'Loading: {path_to_load}')\n",
    "            lz_concat.append(np.load(path_to_load, mmap_mode='r'))\n",
    "        return lz_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load likelihood_engagement_cpu\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import logging\n",
    "import boto3\n",
    "from pprint import pformat\n",
    "from torch import nn\n",
    "import io\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)    \n",
    "torch.set_printoptions(sci_mode=False, linewidth=400, precision=2)\n",
    "np.set_printoptions(suppress=True, precision=4, linewidth=200)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "CHECKPOINT_DIR='s3://dissertation-data-dmiller/lstm_experiments/checkpoints/data_v1/n_files_30/ordinal/sequence_length_10/data_partition_None/2023_03_30_07_54'\n",
    "METADATA_INDEX = 14\n",
    "logger = logging.getLogger('likelihood_engagement')\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--n_files', type=int, default=2)\n",
    "    parser.add_argument('--n_sequences', type=int, default=20)\n",
    "    parser.add_argument('--file_path', type=str, default='datasets/torch_ready_data')\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default=CHECKPOINT_DIR)\n",
    "    parser.add_argument('--write_path', type=str, default='datasets/rl_ready_data')\n",
    "    parser.add_argument('--model_type', type=str, default='ordinal')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def _extract_features(tensor, n_sequences, n_features):\n",
    "    \n",
    "    features_dict = {}\n",
    "    tensor = tensor.squeeze()\n",
    "       \n",
    "    metadata, features = tensor[:, :METADATA_INDEX], tensor[:, METADATA_INDEX:] \n",
    "                \n",
    "    features = torch.flip(\n",
    "        torch.reshape(features, (features.shape[0], 21, 20)),\n",
    "        dims=[1]\n",
    "    )\n",
    "    \n",
    "    features_dict['features_20'] = features\n",
    "    features_dict['features_10'] = features[:, 10:, :]\n",
    "    features_dict['last_sequence'] = features[:, -1, :]\n",
    "     \n",
    "    return metadata, features_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_models(checkpoints: dict, s3_client, device):\n",
    "    \"\"\"_summary_\n",
    "    Downloads models from s3 and loads them into memory.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for name, checkpoint in checkpoints.items():\n",
    "        logger.info(f'Downloading model: {name}')\n",
    "        response = s3_client.get_object(\n",
    "            Bucket=S3_BUCKET,\n",
    "            Key=checkpoint\n",
    "        )\n",
    "        buffer = io.BytesIO(response['Body'].read())\n",
    "        state = torch.load(buffer, map_location=torch.device(device))\n",
    "        model = LSTMOrdinal()\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        model.to(device)\n",
    "        models[name] = model\n",
    "    return models\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_static_predictions(args):\n",
    "    \n",
    "    user_metadata_container = []\n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    logger.info('Generating static prediction likelihoods for experiment')\n",
    "    npz_extractor = NPZExtractor(\n",
    "        args.file_path,\n",
    "        args.n_files,\n",
    "        args.n_sequences,\n",
    "        client,\n",
    "        None\n",
    "           \n",
    "    )\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'   \n",
    "    logger.info(f'Setting device to {device}')\n",
    "    \n",
    "    logger.info('generating dataset pointer')\n",
    "    dataset = npz_extractor.get_dataset_pointer()\n",
    "    \n",
    "    logger.info('Downloading model checkpoint')\n",
    "    \n",
    "    write_path = os.path.join(args.write_path, f'files_used_{args.n_files}')\n",
    "    if not os.path.exists(write_path):\n",
    "        logger.info(f'Creating directory: {write_path}')\n",
    "        os.makedirs(write_path)\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    logger.info(f'Downloading models from checkpoints {LSTM_CHECKPOINTS.keys()}')\n",
    "    \n",
    "    models = get_models(LSTM_CHECKPOINTS, client, device)\n",
    "    \n",
    "    dataset = ClickstreamDataset(dataset)\n",
    "    loader = DataLoader(dataset, batch_size=2048*64, shuffle=False)\n",
    "    activation = nn.Sigmoid()\n",
    "    \n",
    "\n",
    "    p_bar = tqdm.tqdm(loader, total=len(loader))\n",
    "    \n",
    "    for indx, data in enumerate(p_bar):\n",
    "        p_bar.set_description(f'Processing batch: {indx}')\n",
    "        data = data.to(device)\n",
    "        metadata, features_dict = _extract_features(data, args.n_sequences + 1, 20)\n",
    "        \n",
    "        preds_10 = activation(models['seq_10'](features_dict['features_10']))\n",
    "        preds_20 = activation(models['seq_20'](features_dict['features_20']))\n",
    "        last_event = features_dict['last_sequence']\n",
    "        \n",
    "        user_metadata = torch.cat([metadata, last_event, preds_10, preds_20], dim=1)\n",
    "        user_metadata_container.append(user_metadata.squeeze())\n",
    "   \n",
    "    predicted_data = torch.cat(user_metadata_container, dim=0).cpu().numpy()\n",
    "    logger.info(f'Predicted data shape: {predicted_data.shape}: generating df')\n",
    "    predicted_data = pd.DataFrame(predicted_data, columns=LABEL + METADATA + OUT_FEATURE_COLUMNS + ['seq_10', 'seq_20'])\n",
    "\n",
    "    logger.info('Decoding date time and sorting')\n",
    "\n",
    "    logger.info(f'Writing to parquet: {os.path.join(write_path, \"predicted_data.parquet\")}')\n",
    "    predicted_data.to_parquet(os.path.join(write_path, 'predicted_data.parquet'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    n_files = 30\n",
    "    n_sequences = 20\n",
    "    file_path = 'torch_ready_data'\n",
    "    write_path = 'rl_ready_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 12:14:52,907 Found credentials in environment variables.\n",
      "2023-04-30 12:14:52,957 Generating static prediction likelihoods for experiment\n",
      "2023-04-30 12:14:52,982 Setting device to cuda\n",
      "2023-04-30 12:14:52,982 generating dataset pointer\n",
      "2023-04-30 12:14:52,983 Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_0: derived from torch_ready_data/files_used_30/sequence_index_0.npz\n",
      "2023-04-30 12:14:52,985 Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_10: derived from torch_ready_data/files_used_30/sequence_index_10.npz\n",
      "2023-04-30 12:14:52,986 Loading pointer to dataset: torch_ready_data/files_used_30/sequence_index_20: derived from torch_ready_data/files_used_30/sequence_index_20.npz\n",
      "2023-04-30 12:14:52,988 Loading: torch_ready_data/files_used_30/sequence_index_0/arr_0.npy\n",
      "2023-04-30 12:14:52,995 Loading: torch_ready_data/files_used_30/sequence_index_10/arr_0.npy\n",
      "2023-04-30 12:14:53,004 Loading: torch_ready_data/files_used_30/sequence_index_20/arr_0.npy\n",
      "2023-04-30 12:14:53,011 Downloading model checkpoint\n",
      "2023-04-30 12:14:53,016 Downloading models from checkpoints dict_keys(['seq_10', 'seq_20'])\n",
      "2023-04-30 12:14:53,018 Downloading model: seq_10\n",
      "2023-04-30 12:14:56,420 Downloading model: seq_20\n",
      "Processing batch: 293: 100%|██████████| 294/294 [15:50<00:00,  3.23s/it]\n",
      "2023-04-30 12:30:52,307 Predicted data shape: (38500990, 36): generating df\n",
      "2023-04-30 12:30:52,321 Decoding date time and sorting\n",
      "2023-04-30 12:30:52,323 Writing to parquet: rl_ready_data/files_used_30/predicted_data.parquet\n"
     ]
    }
   ],
   "source": [
    "generate_static_predictions(Arguments())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
